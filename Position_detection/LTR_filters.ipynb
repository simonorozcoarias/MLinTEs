{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0U0issomqi0"
   },
   "source": [
    "# **1. IMPORTACIÓN DE LIBRERIAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0xw_yPCHmfa3",
    "outputId": "284cb491-a100-49a2-9425-57d0fececb7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/AUTOENCODER\n",
      "Collecting ipython-autotime\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/c9/b413a24f759641bc27ef98c144b590023c8038dfb8a3f09e713e9dff12c1/ipython_autotime-0.3.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (54.1.2)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.0.5)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ipython-autotime) (0.7.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->ipython-autotime) (0.2.0)\n",
      "Installing collected packages: ipython-autotime\n",
      "Successfully installed ipython-autotime-0.3.1\n",
      "time: 1.2 ms (started: 2021-03-20 14:07:49 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from scipy import misc, ndimage, signal                                         # \"scipy\" is one of the core packages that make up the SciPy stack. It provides many user-friendly and efficient numerical routines, such as routines for numerical integration, interpolation, optimization, linear algebra, and statistics.\n",
    "                                                                                # \"misc\" Various utilities that don’t have another home: ascent(), central_diff_weights(), derivative(), face(), electrocardiogram()\n",
    "                                                                                # \"ndimage\" This package contains various functions for multidimensional image processing\n",
    "                                                                                # \"signal\" This package contains various functions for signal processing: convolution, B-splines, filtering, filter design, Matlab-style IIR filter design, Continuous-time linear systems, Discrete-time linear systems, LTI representations, Waveforms, Window functions¶, Wavelets, Peak finding, Spectral analysis.\n",
    "import numpy as np                                                              # \"numpy\" is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more\n",
    "import random                                                                   # \"random\" This module implements pseudo-random number generators for various distributions\n",
    "#import ntpath                                                                   # ----------------------------------¿Para qué sirve?\n",
    "import os                                                                       # \"os\" This module provides a portable way of using operating system dependent functionality. If you just want to read or write a file see open(), if you want to manipulate paths, see the os.path module, and if you want to read all the lines in all the files on the command line see the fileinput module. \n",
    "import pandas as pd                                                             # \"pandas\" is a Python package that provides fast, flexible, and expressive data structures designed to make working with structured (tabular, multidimensional, potentially heterogeneous) and time series data both easy and intuitive\n",
    "import matplotlib as mpl                                                        # \"matplotlib\" is a comprehensive library for creating static, animated, and interactive visualizations in Python\n",
    "import matplotlib.pyplot as plt                                                 # \"pyplot\" is a collection of functions that make matplotlib work like MATLAB\n",
    "import matplotlib.colors as colors                                              # \"colors\" A module for converting numbers or color arguments to RGB or RGBA\n",
    "import tensorflow as tf                                                         # \"tensorflow\" TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications\n",
    "                                                                                # \"keras\" Keras is a deep learning API (application programming interface) written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result as fast as possible is key to doing good research.\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import optimizers                                         # \"optimizers\" is a container of functions -----------¿De qué trata?\n",
    "from tensorflow.keras import regularizers                                       # \"regularizers\" is a container of functions -----------¿De qué trata?\n",
    "from tensorflow.keras import backend as K                                       # \"backend\" is a container of functions -----------¿De qué trata?\n",
    "from tensorflow.keras import datasets,layers,models,Input,Model\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, Adagrad, SGD, Adadelta, Adamax, Nadam\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, AveragePooling2D, Cropping2D\n",
    "from tensorflow.keras.layers import Dropout, Activation, Flatten, Concatenate, Dense, Reshape, Add, PReLU, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.activations import relu\n",
    "import time as tm                                                               # \"time\" This module provides various time-related functions\n",
    "from time import time                                                           # \"time.time\" Return the time in seconds since the epoch as a floating point number. On Windows and most Unix systems, the epoch is January 1, 1970, 00:00:00 (UTC).\n",
    "import datetime                                                                 # \"datetime\" supplies classes for manipulating dates and times\n",
    "from operator import itemgetter                                                 # \"operator\" exports a set of efficient functions that fall into categories that perform object comparisons, logical operations, mathematical operations and sequence operations.\n",
    "                                                                                # \"itemgetter\" is a container of functions -----------¿De qué trata?\n",
    "import glob                                                                     # \"glob\" is used to retrieve files/pathnames matching a specified pattern.\n",
    "import tensorflow.keras.utils                                                   # \"utils\" is a container of functions -----------¿De qué trata?\n",
    "from tensorflow.keras.utils import to_categorical                               # \"to_categorical\" Converts a class vector (integers) to binary class matrix.\n",
    "from numpy import argmax                                                        # \"argmax\" Returns the indices of the maximum values along an axis.\n",
    "import seaborn as sn                                                            # \"seaborn\" Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "from sklearn.model_selection  import train_test_split                           # \"sklearn\" features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n",
    "                                                                                # \"train_test_split\" splits arrays or matrices into random train and test subsets\n",
    "from sklearn.metrics import confusion_matrix                                    # \"confusion_matrix\" Compute confusion matrix to evaluate the accuracy of a classification.\n",
    "from sklearn.metrics import accuracy_score                                      # \"accuracy_score\" In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\n",
    "from sklearn.metrics import f1_score                                            # \"f1_score\" Compute the F1 score, also known as balanced F-score or F-measure\n",
    "from sklearn.metrics import recall_score                                        # \"recall_score\" The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The best value is 1 and the worst value is 0.\n",
    "from sklearn.metrics import precision_score                                     # \"precision_Score\" The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1 and the worst value is 0.\n",
    "from sklearn.metrics import classification_report                               # \"classification_report\" Build a text report showing the main classification metrics\n",
    "from tensorflow.keras import layers                                             # \"layers\" is a container of functions -----------¿De qué trata?\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import cv2\n",
    "\n",
    "\"\"\"from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "% cd /content/drive/MyDrive/AUTOENCODER/\n",
    "\n",
    "\n",
    "!pip install ipython-autotime\n",
    "%load_ext autotime\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zwyPjvfm2qA"
   },
   "source": [
    "# **2. CONTEO DE KMERS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXDGcG64MA9z"
   },
   "source": [
    "## **2.1 CARGA DE DATOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "nIQERW1OtA4b",
    "outputId": "bbcc6d36-7664-4e28-8005-7e3b6efd9f41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2785, 5, 22576)\n",
      "(2785, 5460)\n",
      "(2228, 5, 22576)\n",
      "(2228, 5460)\n",
      "(278, 5, 22576)\n",
      "(278, 5460)\n",
      "(279, 5, 22576)\n",
      "(279, 5460)\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\none_hot_labels_train = tf.keras.utils.to_categorical(Y_train[:,0], num_classes=21)\\none_hot_labels_validation = tf.keras.utils.to_categorical(Y_dev[:,0], num_classes=21)\\none_hot_labels_test = tf.keras.utils.to_categorical(Y_test[:,0], num_classes=21)\\n\\nprint(one_hot_labels_train.shape)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.23 s (started: 2021-03-12 19:25:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "X = np.load('InpactorDB_Repbase_format.fa.filtered_center.npy')\n",
    "Y = open('InpactorDB_Repbase_format.fa.kmers','r')\n",
    "Texto=Y.read()\n",
    "Y.close()\n",
    "Texto=Texto.splitlines()\n",
    "Texto=Texto[1:]\n",
    "label=[]\n",
    "for i in range(len(Texto)):\n",
    "    A=[int(x) for x in Texto[i].split(',')]\n",
    "    label.append(A)\n",
    "\n",
    "label=np.array(label)\n",
    "label=label[:,1:5461]\n",
    "\n",
    "print(X.shape)\n",
    "print(label.shape)\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "X_train, X_test_dev, Y_train, Y_test_dev = train_test_split(X, label, test_size=validation_size, random_state=seed)\n",
    "\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_test_dev, Y_test_dev, test_size=0.5, random_state=seed)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(Y_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "X = None\n",
    "Y = None\n",
    "\n",
    "X_test_dev = None\n",
    "Y_test_dev = None\n",
    "\n",
    "\"\"\"\n",
    "one_hot_labels_train = tf.keras.utils.to_categorical(Y_train[:,0], num_classes=21)\n",
    "one_hot_labels_validation = tf.keras.utils.to_categorical(Y_dev[:,0], num_classes=21)\n",
    "one_hot_labels_test = tf.keras.utils.to_categorical(Y_test[:,0], num_classes=21)\n",
    "\n",
    "print(one_hot_labels_train.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mR-E_baYMF26"
   },
   "source": [
    "## **2.2 FILTROS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFlD27EA1gGU",
    "outputId": "ecddefd0-7d0e-4574-f258-7aed403e5d90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 5, 22576, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Mental_01 (Conv2D)              (None, 1, 22576, 4)  24          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_02 (Conv2D)              (None, 1, 22575, 16) 176         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_03 (Conv2D)              (None, 1, 22574, 64) 1024        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_04 (Conv2D)              (None, 1, 22573, 256 5376        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_05_A (Conv2D)            (None, 1, 22572, 512 13312       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_05_B (Conv2D)            (None, 1, 22572, 512 13312       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_06_A (Conv2D)            (None, 1, 22571, 512 15872       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_06_B (Conv2D)            (None, 1, 22571, 512 15872       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_06_C (Conv2D)            (None, 1, 22571, 512 15872       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_06_D (Conv2D)            (None, 1, 22571, 512 15872       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_06_E (Conv2D)            (None, 1, 22571, 512 15872       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_06_F (Conv2D)            (None, 1, 22571, 512 15872       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_06_G (Conv2D)            (None, 1, 22571, 512 15872       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_06_H (Conv2D)            (None, 1, 22571, 512 15872       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum (TFOpLambda) (None, 1, 4)         0           Mental_01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_1 (TFOpLambd (None, 1, 16)        0           Mental_02[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_2 (TFOpLambd (None, 1, 64)        0           Mental_03[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_3 (TFOpLambd (None, 1, 256)       0           Mental_04[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_4 (TFOpLambd (None, 1, 512)       0           Mental_05_A[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_5 (TFOpLambd (None, 1, 512)       0           Mental_05_B[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_6 (TFOpLambd (None, 1, 512)       0           Mental_06_A[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_7 (TFOpLambd (None, 1, 512)       0           Mental_06_B[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_8 (TFOpLambd (None, 1, 512)       0           Mental_06_C[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_9 (TFOpLambd (None, 1, 512)       0           Mental_06_D[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_10 (TFOpLamb (None, 1, 512)       0           Mental_06_E[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_11 (TFOpLamb (None, 1, 512)       0           Mental_06_F[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_12 (TFOpLamb (None, 1, 512)       0           Mental_06_G[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_13 (TFOpLamb (None, 1, 512)       0           Mental_06_H[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 1, 5460)      0           tf.math.reduce_sum[0][0]         \n",
      "                                                                 tf.math.reduce_sum_1[0][0]       \n",
      "                                                                 tf.math.reduce_sum_2[0][0]       \n",
      "                                                                 tf.math.reduce_sum_3[0][0]       \n",
      "                                                                 tf.math.reduce_sum_4[0][0]       \n",
      "                                                                 tf.math.reduce_sum_5[0][0]       \n",
      "                                                                 tf.math.reduce_sum_6[0][0]       \n",
      "                                                                 tf.math.reduce_sum_7[0][0]       \n",
      "                                                                 tf.math.reduce_sum_8[0][0]       \n",
      "                                                                 tf.math.reduce_sum_9[0][0]       \n",
      "                                                                 tf.math.reduce_sum_10[0][0]      \n",
      "                                                                 tf.math.reduce_sum_11[0][0]      \n",
      "                                                                 tf.math.reduce_sum_12[0][0]      \n",
      "                                                                 tf.math.reduce_sum_13[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 5460)         0           tf.concat[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 160,200\n",
      "Trainable params: 160,200\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "dim A es (2228, 5460)\n",
      "Diferencia primer ejemplo =  6.0\n",
      "La cantidad de A en labels es de  1752\n",
      "La cantidad real de A es  1753\n",
      "Diferencia total =  13363.0\n",
      "El vector de diferencia en sus primeros 50 elementos es  [1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "time: 21.2 s (started: 2021-03-12 19:25:47 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def CONTEO_KMER(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2], 1), name=\"input_1\")\n",
    "    # layer 1\n",
    "    \n",
    "\n",
    "    k=0\n",
    "    n_1=4**1\n",
    "    W_1=np.zeros((5,1,1,n_1))\n",
    "    b_1=np.zeros((1,n_1)).reshape(n_1,)\n",
    "    for i in range(4):\n",
    "      W_1[i,0,0,i]=1\n",
    "    \n",
    "    k=0\n",
    "    n_2=4**2\n",
    "    W_2=np.zeros((5,2,1,n_2))\n",
    "    b_2=-np.ones((1,n_2)).reshape(n_2,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        W_2[i,0,0,k]=1\n",
    "        W_2[j,1,0,k]=1\n",
    "        k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_3=4**3\n",
    "    W_3=np.zeros((5,3,1,n_3))\n",
    "    b_3=-2*np.ones((1,n_3)).reshape(n_3,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          W_3[i,0,0,k]=1\n",
    "          W_3[j,1,0,k]=1\n",
    "          W_3[l,2,0,k]=1\n",
    "          k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_4=4**4\n",
    "    W_4=np.zeros((5,4,1,n_4))\n",
    "    b_4=-3*np.ones((1,n_4)).reshape(n_4,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          for m in range(4):\n",
    "            W_4[i,0,0,k]=1\n",
    "            W_4[j,1,0,k]=1\n",
    "            W_4[l,2,0,k]=1\n",
    "            W_4[m,3,0,k]=1\n",
    "            k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_5=4**5\n",
    "    W_5=np.zeros((5,5,1,n_5))\n",
    "    b_5=-4*np.ones((1,n_5)).reshape(n_5,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          for m in range(4):\n",
    "            for n in range(4):\n",
    "              W_5[i,0,0,k]=1\n",
    "              W_5[j,1,0,k]=1\n",
    "              W_5[l,2,0,k]=1\n",
    "              W_5[m,3,0,k]=1\n",
    "              W_5[n,4,0,k]=1\n",
    "              k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_6=4**6\n",
    "    W_6=np.zeros((5,6,1,n_6))\n",
    "    b_6=-5*np.ones((1,n_6)).reshape(n_6,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          for m in range(4):\n",
    "            for n in range(4):\n",
    "              for p in range(4):\n",
    "                W_6[i,0,0,k]=1\n",
    "                W_6[j,1,0,k]=1\n",
    "                W_6[l,2,0,k]=1\n",
    "                W_6[m,3,0,k]=1\n",
    "                W_6[n,4,0,k]=1\n",
    "                W_6[p,5,0,k]=1\n",
    "                k=k+1\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv2D(n_1, (5, 1), strides=(1,1), weights=[W_1,b_1],activation=fun_act, use_bias=True, name='Mental_01')(inputs)\n",
    "    layers_1=tf.keras.backend.sum(layers_1,axis=-2)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Conv2D(n_2, (5, 2), strides=(1,1), weights=[W_2,b_2],activation=fun_act, use_bias=True, name='Mental_02')(inputs)\n",
    "    layers_2=tf.keras.backend.sum(layers_2,axis=-2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Conv2D(n_3, (5, 3), strides=(1,1), weights=[W_3,b_3],activation=fun_act, use_bias=True, name='Mental_03')(inputs)\n",
    "    layers_3 = tf.keras.backend.sum(layers_3,axis=-2)\n",
    "\n",
    "    layers_4 = tf.keras.layers.Conv2D(n_4, (5, 4), strides=(1,1), weights=[W_4,b_4],activation=fun_act, use_bias=True, name='Mental_04')(inputs)\n",
    "    layers_4 = tf.keras.backend.sum(layers_4,axis=-2)\n",
    "\n",
    "    t=int(n_5/2)\n",
    "    layers_5_A = tf.keras.layers.Conv2D(t, (5, 5), strides=(1,1), weights=[W_5[:,:,:,0:t],b_5[0:t]],activation=fun_act, use_bias=True, name='Mental_05_A')(inputs)\n",
    "    layers_5_A = tf.keras.backend.sum(layers_5_A,axis=-2)\n",
    "\n",
    "    layers_5_B = tf.keras.layers.Conv2D(t, (5, 5), strides=(1,1), weights=[W_5[:,:,:,t:t*2],b_5[t:t*2]],activation=fun_act, use_bias=True, name='Mental_05_B')(inputs)\n",
    "    layers_5_B = tf.keras.backend.sum(layers_5_B,axis=-2)\n",
    "\n",
    "    t=int(n_6/8)\n",
    "    layers_6_A = tf.keras.layers.Conv2D(t, (5, 6), strides=(1,1), weights=[W_6[:,:,:,0:t],b_6[0:t]],activation=fun_act, use_bias=True, name='Mental_06_A')(inputs)\n",
    "    layers_6_A = tf.keras.backend.sum(layers_6_A,axis=-2)\n",
    "\n",
    "    layers_6_B = tf.keras.layers.Conv2D(t, (5, 6), strides=(1,1), weights=[W_6[:,:,:,t:t*2],b_6[t:t*2]],activation=fun_act, use_bias=True, name='Mental_06_B')(inputs)\n",
    "    layers_6_B = tf.keras.backend.sum(layers_6_B,axis=-2)\n",
    "\n",
    "    layers_6_C = tf.keras.layers.Conv2D(t, (5, 6), strides=(1,1), weights=[W_6[:,:,:,t*2:t*3],b_6[t*2:t*3]],activation=fun_act, use_bias=True, name='Mental_06_C')(inputs)\n",
    "    layers_6_C = tf.keras.backend.sum(layers_6_C,axis=-2)\n",
    "\n",
    "    layers_6_D = tf.keras.layers.Conv2D(t, (5, 6), strides=(1,1), weights=[W_6[:,:,:,t*3:t*4],b_6[t*3:t*4]],activation=fun_act, use_bias=True, name='Mental_06_D')(inputs)\n",
    "    layers_6_D = tf.keras.backend.sum(layers_6_D,axis=-2)\n",
    "\n",
    "    layers_6_E = tf.keras.layers.Conv2D(t, (5, 6), strides=(1,1), weights=[W_6[:,:,:,t*4:t*5],b_6[t*4:t*5]],activation=fun_act, use_bias=True, name='Mental_06_E')(inputs)\n",
    "    layers_6_E = tf.keras.backend.sum(layers_6_E,axis=-2)\n",
    "\n",
    "    layers_6_F = tf.keras.layers.Conv2D(t, (5, 6), strides=(1,1), weights=[W_6[:,:,:,t*5:t*6],b_6[t*5:t*6]],activation=fun_act, use_bias=True, name='Mental_06_F')(inputs)\n",
    "    layers_6_F = tf.keras.backend.sum(layers_6_F,axis=-2)\n",
    "\n",
    "    layers_6_G = tf.keras.layers.Conv2D(t, (5, 6), strides=(1,1), weights=[W_6[:,:,:,t*6:t*7],b_6[t*6:t*7]],activation=fun_act, use_bias=True, name='Mental_06_G')(inputs)\n",
    "    layers_6_G = tf.keras.backend.sum(layers_6_G,axis=-2)\n",
    "\n",
    "    layers_6_H = tf.keras.layers.Conv2D(t, (5, 6), strides=(1,1), weights=[W_6[:,:,:,t*7:t*8],b_6[t*7:t*8]],activation=fun_act, use_bias=True, name='Mental_06_H')(inputs)\n",
    "    layers_6_H = tf.keras.backend.sum(layers_6_H,axis=-2)\n",
    "\n",
    "    layers=tf.concat([layers_1,layers_2,layers_3, layers_4,layers_5_A, layers_5_B, layers_6_A,layers_6_B,layers_6_C,layers_6_D,layers_6_E,layers_6_F,layers_6_G,layers_6_H],2)\n",
    "    layers = tf.keras.layers.Flatten()(layers)\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=layers)\n",
    "\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    # loss function\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    # Compile model\n",
    "    #model.compile(loss=loss_fn, optimizer=opt, metrics=['acc', 'AUC', 'mse','mae','mape'])\n",
    "    model.compile(loss=loss_fn, optimizer=opt)\n",
    "    return model\n",
    "model_name='CONTEO_KMER'\n",
    "model = CONTEO_KMER()\n",
    "print(model.summary())\n",
    "\n",
    "Datos_train=X_train\n",
    "Labels_train=Y_train\n",
    "A=model.predict(Datos_train,batch_size=20)\n",
    "Diferencia=(A-Labels_train)\n",
    "print('dim A es',A.shape)\n",
    "print('Diferencia primer ejemplo = ',np.sum(Diferencia[0,:]))\n",
    "print('La cantidad de A en labels es de ', Y_train[0,0])\n",
    "print('La cantidad real de A es ',np.sum(X_train[0,0,:]))\n",
    "print('Diferencia total = ',np.sum(np.abs(Diferencia)))\n",
    "print('El vector de diferencia en sus primeros 50 elementos es ', Diferencia[0,0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIO6asj-thMQ"
   },
   "source": [
    "# **3. MODELO DE CLASIFICACIÓN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-0d3pHFLvr4"
   },
   "source": [
    "## **3.1 CARGA DE DATOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YYFU-lM2cYp",
    "outputId": "f862c5ae-b9d7-4d9f-f9da-27a0f8ae6801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2785, 5, 22576)\n",
      "(2785,)\n",
      "(2228, 5, 22576)\n",
      "(2228,)\n",
      "(278, 5, 22576)\n",
      "(278,)\n",
      "(279, 5, 22576)\n",
      "(279,)\n",
      "(2228, 21)\n",
      "time: 4.16 s (started: 2021-03-15 03:01:10 +00:00)\n"
     ]
    }
   ],
   "source": [
    "X = np.load('InpactorDB_Repbase_format.fa.filtered_center.npy')\n",
    "Y = open('InpactorDB_Repbase_format.fa.kmers','r')\n",
    "Texto=Y.read()\n",
    "Y.close()\n",
    "Texto=Texto.splitlines()\n",
    "Texto=Texto[1:]\n",
    "label=[]\n",
    "for i in range(len(Texto)):\n",
    "    A=[int(x) for x in Texto[i].split(',')]\n",
    "    label.append(A)\n",
    "\n",
    "label=np.array(label)\n",
    "label=label[:,0]\n",
    "\n",
    "print(X.shape)\n",
    "print(label.shape)\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "X_train, X_test_dev, Y_train, Y_test_dev = train_test_split(X, label, test_size=validation_size, random_state=seed)\n",
    "\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_test_dev, Y_test_dev, test_size=0.5, random_state=seed)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(Y_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "X = None\n",
    "Y = None\n",
    "\n",
    "X_test_dev = None\n",
    "Y_test_dev = None\n",
    "\n",
    "\n",
    "one_hot_labels_train = tf.keras.utils.to_categorical(Y_train, num_classes=21)\n",
    "one_hot_labels_dev = tf.keras.utils.to_categorical(Y_dev, num_classes=21)\n",
    "one_hot_labels_test = tf.keras.utils.to_categorical(Y_test, num_classes=21)\n",
    "\n",
    "print(one_hot_labels_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NxCHb0EL5YE"
   },
   "source": [
    "## **3.2 MODELO 01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ynSrSqvctlmV",
    "outputId": "9338c4fa-72cc-47bb-cc4a-8759384e4fd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14 ms (started: 2021-03-14 23:35:39 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def Clasificacion_SL(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2], 1), name=\"input_1\")\n",
    "    # layer 1\n",
    "    k=10\n",
    "    layers = tf.keras.layers.Conv2D(256, (5, k), strides=(1,1),activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    #layers=tf.keras.backend.sum(layers,axis=-2)\n",
    "    layers = tf.keras.layers.AveragePooling2D((1, X_train.shape[2]-k+1))(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization()(layers)\n",
    "    layers = tf.keras.layers.Flatten()(layers)\n",
    "    layers = tf.keras.layers.Dense(21, activation=\"softmax\", name=\"output_1\")(layers)\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=layers)\n",
    "    # optimizer\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    # loss function\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    # Compile model\n",
    "    #model.compile(loss=loss_fn, optimizer=opt, metrics=['acc', 'AUC', 'mse','mae','mape'])\n",
    "    model.compile(loss=loss_fn, optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwRO9MUXF5Q_",
    "outputId": "50278e6f-2213-4c2e-868b-f2225c9faad1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 5, 22576, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 1, 22567, 256)     13056     \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1, 1, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output_1 (Dense)             (None, 21)                5397      \n",
      "=================================================================\n",
      "Total params: 19,477\n",
      "Trainable params: 18,965\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "70/70 [==============================] - 39s 84ms/step - loss: 1.6319 - accuracy: 0.4853 - val_loss: 2.1268 - val_accuracy: 0.2086\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 4s 61ms/step - loss: 1.0267 - accuracy: 0.6085 - val_loss: 1.8328 - val_accuracy: 0.3345\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 4s 62ms/step - loss: 0.9704 - accuracy: 0.6559 - val_loss: 1.8589 - val_accuracy: 0.3381\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 4s 62ms/step - loss: 0.8556 - accuracy: 0.7026 - val_loss: 1.5892 - val_accuracy: 0.4820\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 4s 62ms/step - loss: 0.9413 - accuracy: 0.6552 - val_loss: 3.7492 - val_accuracy: 0.3309\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 4s 62ms/step - loss: 0.8313 - accuracy: 0.7128 - val_loss: 2.4632 - val_accuracy: 0.3597\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 4s 62ms/step - loss: 0.8809 - accuracy: 0.6802 - val_loss: 1.3132 - val_accuracy: 0.4496\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 4s 62ms/step - loss: 0.7978 - accuracy: 0.7278 - val_loss: 6.5137 - val_accuracy: 0.4245\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 4s 62ms/step - loss: 0.7146 - accuracy: 0.7491 - val_loss: 10.1652 - val_accuracy: 0.3813\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 4s 63ms/step - loss: 0.7819 - accuracy: 0.7493 - val_loss: 2.6901 - val_accuracy: 0.4676\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 4s 63ms/step - loss: 0.6297 - accuracy: 0.7621 - val_loss: 0.9050 - val_accuracy: 0.7122\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.5924 - accuracy: 0.8000 - val_loss: 1.1132 - val_accuracy: 0.6259\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 4s 63ms/step - loss: 0.5601 - accuracy: 0.8159 - val_loss: 0.7779 - val_accuracy: 0.7698\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.5873 - accuracy: 0.8108 - val_loss: 11.1625 - val_accuracy: 0.4317\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.4668 - accuracy: 0.8438 - val_loss: 1.2433 - val_accuracy: 0.5719\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.5686 - accuracy: 0.8099 - val_loss: 2.2207 - val_accuracy: 0.5000\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.5075 - accuracy: 0.8415 - val_loss: 0.5938 - val_accuracy: 0.8094\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 5s 65ms/step - loss: 0.4278 - accuracy: 0.8599 - val_loss: 0.8249 - val_accuracy: 0.8022\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 5s 65ms/step - loss: 0.4977 - accuracy: 0.8345 - val_loss: 0.7765 - val_accuracy: 0.7806\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 5s 65ms/step - loss: 0.4398 - accuracy: 0.8437 - val_loss: 1.2289 - val_accuracy: 0.5612\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 5s 65ms/step - loss: 0.4551 - accuracy: 0.8497 - val_loss: 11.1351 - val_accuracy: 0.0935\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 5s 65ms/step - loss: 0.4074 - accuracy: 0.8610 - val_loss: 2.0172 - val_accuracy: 0.3669\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 5s 64ms/step - loss: 0.3775 - accuracy: 0.8757 - val_loss: 1.0877 - val_accuracy: 0.6043\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.3674 - accuracy: 0.8792 - val_loss: 0.5435 - val_accuracy: 0.8453\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.3161 - accuracy: 0.9007 - val_loss: 0.4260 - val_accuracy: 0.8705\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.3817 - accuracy: 0.8656 - val_loss: 0.6536 - val_accuracy: 0.7806\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 4s 63ms/step - loss: 0.3371 - accuracy: 0.8944 - val_loss: 0.6021 - val_accuracy: 0.7986\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.3170 - accuracy: 0.8884 - val_loss: 1.8257 - val_accuracy: 0.4460\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 4s 63ms/step - loss: 0.3456 - accuracy: 0.8837 - val_loss: 0.4079 - val_accuracy: 0.8381\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 4s 63ms/step - loss: 0.3325 - accuracy: 0.8900 - val_loss: 0.8161 - val_accuracy: 0.7230\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 4s 63ms/step - loss: 0.3402 - accuracy: 0.8938 - val_loss: 0.6773 - val_accuracy: 0.7698\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2941 - accuracy: 0.8997 - val_loss: 0.5455 - val_accuracy: 0.8381\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2659 - accuracy: 0.9036 - val_loss: 18.5407 - val_accuracy: 0.2698\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2834 - accuracy: 0.9156 - val_loss: 1.5114 - val_accuracy: 0.6331\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2566 - accuracy: 0.9172 - val_loss: 0.9669 - val_accuracy: 0.7302\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2793 - accuracy: 0.9090 - val_loss: 0.6995 - val_accuracy: 0.7842\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2419 - accuracy: 0.9292 - val_loss: 0.6105 - val_accuracy: 0.8201\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2326 - accuracy: 0.9187 - val_loss: 0.6423 - val_accuracy: 0.7986\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2671 - accuracy: 0.9097 - val_loss: 0.3460 - val_accuracy: 0.8921\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2527 - accuracy: 0.9190 - val_loss: 0.4464 - val_accuracy: 0.8561\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2159 - accuracy: 0.9329 - val_loss: 0.3443 - val_accuracy: 0.8849\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2131 - accuracy: 0.9368 - val_loss: 4.8126 - val_accuracy: 0.2302\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2503 - accuracy: 0.9267 - val_loss: 0.9333 - val_accuracy: 0.6547\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1841 - accuracy: 0.9432 - val_loss: 6.4157 - val_accuracy: 0.1583\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 5s 64ms/step - loss: 0.2297 - accuracy: 0.9314 - val_loss: 0.9534 - val_accuracy: 0.7338\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2114 - accuracy: 0.9358 - val_loss: 0.8383 - val_accuracy: 0.7518\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2069 - accuracy: 0.9265 - val_loss: 2.5924 - val_accuracy: 0.5252\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2088 - accuracy: 0.9324 - val_loss: 1.4964 - val_accuracy: 0.6151\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1730 - accuracy: 0.9488 - val_loss: 0.3171 - val_accuracy: 0.8741\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1635 - accuracy: 0.9460 - val_loss: 0.7064 - val_accuracy: 0.7842\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1756 - accuracy: 0.9402 - val_loss: 0.3638 - val_accuracy: 0.8849\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1464 - accuracy: 0.9541 - val_loss: 0.4595 - val_accuracy: 0.8309\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2051 - accuracy: 0.9296 - val_loss: 0.3033 - val_accuracy: 0.9209\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 5s 65ms/step - loss: 0.2245 - accuracy: 0.9219 - val_loss: 0.3089 - val_accuracy: 0.8885\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1575 - accuracy: 0.9395 - val_loss: 0.4351 - val_accuracy: 0.8489\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1399 - accuracy: 0.9530 - val_loss: 0.4347 - val_accuracy: 0.8633\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1817 - accuracy: 0.9337 - val_loss: 0.5497 - val_accuracy: 0.8705\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1671 - accuracy: 0.9499 - val_loss: 0.3195 - val_accuracy: 0.8741\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1434 - accuracy: 0.9486 - val_loss: 0.2123 - val_accuracy: 0.9137\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1040 - accuracy: 0.9733 - val_loss: 0.3744 - val_accuracy: 0.8849\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1230 - accuracy: 0.9566 - val_loss: 0.3920 - val_accuracy: 0.8741\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1208 - accuracy: 0.9603 - val_loss: 0.9124 - val_accuracy: 0.8417\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1817 - accuracy: 0.9461 - val_loss: 3.9856 - val_accuracy: 0.7014\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1352 - accuracy: 0.9493 - val_loss: 2.5570 - val_accuracy: 0.6906\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1259 - accuracy: 0.9618 - val_loss: 0.3864 - val_accuracy: 0.8741\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1310 - accuracy: 0.9475 - val_loss: 0.3184 - val_accuracy: 0.8849\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1105 - accuracy: 0.9624 - val_loss: 13.1492 - val_accuracy: 0.3022\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1249 - accuracy: 0.9558 - val_loss: 0.9823 - val_accuracy: 0.7230\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1109 - accuracy: 0.9660 - val_loss: 1.2364 - val_accuracy: 0.7878\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1423 - accuracy: 0.9469 - val_loss: 0.8300 - val_accuracy: 0.7230\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1260 - accuracy: 0.9628 - val_loss: 26.4007 - val_accuracy: 0.0971\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 5s 64ms/step - loss: 0.1197 - accuracy: 0.9593 - val_loss: 0.7650 - val_accuracy: 0.8453\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0999 - accuracy: 0.9678 - val_loss: 2.6498 - val_accuracy: 0.4964\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.2404 - accuracy: 0.9417 - val_loss: 12.9680 - val_accuracy: 0.2338\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0967 - accuracy: 0.9647 - val_loss: 2.7053 - val_accuracy: 0.5252\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1167 - accuracy: 0.9620 - val_loss: 0.6086 - val_accuracy: 0.8381\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0813 - accuracy: 0.9731 - val_loss: 0.6925 - val_accuracy: 0.7806\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0865 - accuracy: 0.9658 - val_loss: 0.5397 - val_accuracy: 0.8345\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0555 - accuracy: 0.9786 - val_loss: 0.4124 - val_accuracy: 0.8705\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0781 - accuracy: 0.9728 - val_loss: 0.4478 - val_accuracy: 0.9029\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0948 - accuracy: 0.9771 - val_loss: 0.3505 - val_accuracy: 0.9101\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 5s 65ms/step - loss: 0.0614 - accuracy: 0.9816 - val_loss: 0.4832 - val_accuracy: 0.8705\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0703 - accuracy: 0.9788 - val_loss: 0.5422 - val_accuracy: 0.8165\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0820 - accuracy: 0.9717 - val_loss: 0.4266 - val_accuracy: 0.8669\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0762 - accuracy: 0.9756 - val_loss: 2.0041 - val_accuracy: 0.7410\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0616 - accuracy: 0.9816 - val_loss: 0.4125 - val_accuracy: 0.8813\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0777 - accuracy: 0.9767 - val_loss: 0.6326 - val_accuracy: 0.8489\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1653 - accuracy: 0.9527 - val_loss: 0.6501 - val_accuracy: 0.8921\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0749 - accuracy: 0.9795 - val_loss: 0.7320 - val_accuracy: 0.8237\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0646 - accuracy: 0.9782 - val_loss: 1.3923 - val_accuracy: 0.6978\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.1104 - accuracy: 0.9615 - val_loss: 0.5880 - val_accuracy: 0.8417\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0644 - accuracy: 0.9813 - val_loss: 0.6070 - val_accuracy: 0.8957\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0905 - accuracy: 0.9775 - val_loss: 0.5237 - val_accuracy: 0.8993\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0885 - accuracy: 0.9632 - val_loss: 0.6524 - val_accuracy: 0.8453\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0688 - accuracy: 0.9755 - val_loss: 0.1889 - val_accuracy: 0.9424\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0631 - accuracy: 0.9794 - val_loss: 0.1393 - val_accuracy: 0.9568\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0469 - accuracy: 0.9825 - val_loss: 0.1933 - val_accuracy: 0.9532\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0298 - accuracy: 0.9923 - val_loss: 0.3773 - val_accuracy: 0.9173\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0749 - accuracy: 0.9751 - val_loss: 0.5925 - val_accuracy: 0.8633\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 4s 64ms/step - loss: 0.0816 - accuracy: 0.9692 - val_loss: 0.1812 - val_accuracy: 0.9640\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.0689 - accuracy: 0.9780\n",
      "9/9 [==============================] - 0s 31ms/step - loss: 0.1812 - accuracy: 0.9640\n",
      "9/9 [==============================] - 1s 114ms/step - loss: 0.2816 - accuracy: 0.9247\n",
      "time: 8min 11s (started: 2021-03-14 23:35:40 +00:00)\n"
     ]
    }
   ],
   "source": [
    "model_name='Clasificacion_SL_01'\n",
    "model = Clasificacion_SL(lr=0.01)\n",
    "\n",
    "print(model.summary())\n",
    "filepath='./Modelos_kmer/{}.hdf5'.format(model_name)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss',save_weights_only=True)\n",
    "history=model.fit(X_train, one_hot_labels_train, epochs=100, callbacks=[checkpoint], batch_size=32, validation_data=(X_dev,one_hot_labels_dev))\n",
    "train_loss=model.evaluate(X_train,one_hot_labels_train)\n",
    "val_loss=model.evaluate(X_dev,one_hot_labels_dev)\n",
    "test_loss=model.evaluate(X_test,one_hot_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9JwQFStzsRh"
   },
   "source": [
    "## **3.2 MODELO 02**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vxEGFud1zqwA",
    "outputId": "ebc12ab8-7959-4772-a70f-60b592418c4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 32.3 ms (started: 2021-03-14 23:43:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def Clasificacion_SL(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2], 1), name=\"input_1\")\n",
    "    # layer 1\n",
    "    k=10\n",
    "    layers = tf.keras.layers.Conv2D(256, (5, k), strides=(1,1),activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    \n",
    "    layers_1 = tf.keras.layers.AveragePooling2D((1, X_train.shape[2]-k+1))(layers)\n",
    "    layers_1 = tf.keras.layers.BatchNormalization()(layers_1)\n",
    "\n",
    "    \n",
    "    layers_2 = tf.keras.layers.Conv2D(10, (1, 8), strides=(1,1),activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers_2 = tf.keras.layers.AveragePooling2D((1, X_train.shape[2]-k+1-8+1))(layers_2)\n",
    "    layers_2 = tf.keras.layers.BatchNormalization()(layers_2)\n",
    "\n",
    "    layers=tf.concat([layers_1,layers_2],3)\n",
    "    layers = tf.keras.layers.Flatten()(layers)\n",
    "    layers = tf.keras.layers.Dense(21, activation=\"softmax\", name=\"output_1\")(layers)\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=layers)\n",
    "    # optimizer\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    # loss function\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    # Compile model\n",
    "    #model.compile(loss=loss_fn, optimizer=opt, metrics=['acc', 'AUC', 'mse','mae','mape'])\n",
    "    model.compile(loss=loss_fn, optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMMUDyRjF9nX",
    "outputId": "1d406220-6180-4f6b-b4b7-2c7fb0ab04f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 5, 22576, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 1, 22567, 256 13056       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 1, 22560, 10) 20490       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 1, 1, 256)    0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 10)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1, 1, 256)    1024        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1, 1, 10)     40          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 1, 1, 266)    0           batch_normalization[0][0]        \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 266)          0           tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_1 (Dense)                (None, 21)           5607        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 40,217\n",
      "Trainable params: 39,685\n",
      "Non-trainable params: 532\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "70/70 [==============================] - 18s 226ms/step - loss: 1.6383 - accuracy: 0.4661 - val_loss: 2.3486 - val_accuracy: 0.2086\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 14s 203ms/step - loss: 1.1059 - accuracy: 0.5974 - val_loss: 1.9103 - val_accuracy: 0.3201\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 1.0020 - accuracy: 0.6407 - val_loss: 2.0501 - val_accuracy: 0.3273\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.9325 - accuracy: 0.6452 - val_loss: 2.1894 - val_accuracy: 0.3345\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.9211 - accuracy: 0.6445 - val_loss: 1.8316 - val_accuracy: 0.3813\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.8560 - accuracy: 0.6949 - val_loss: 1.1864 - val_accuracy: 0.5036\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.8044 - accuracy: 0.7120 - val_loss: 1.8076 - val_accuracy: 0.4676\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.8207 - accuracy: 0.7196 - val_loss: 1.6390 - val_accuracy: 0.4388\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.7236 - accuracy: 0.7390 - val_loss: 6.7534 - val_accuracy: 0.5252\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.7738 - accuracy: 0.7249 - val_loss: 1.8304 - val_accuracy: 0.5108\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.6785 - accuracy: 0.7602 - val_loss: 6.1093 - val_accuracy: 0.4065\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.6514 - accuracy: 0.7796 - val_loss: 0.8196 - val_accuracy: 0.7338\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.6168 - accuracy: 0.7961 - val_loss: 0.7050 - val_accuracy: 0.7410\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.5789 - accuracy: 0.8091 - val_loss: 1.2835 - val_accuracy: 0.5971\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.5679 - accuracy: 0.8005 - val_loss: 1.0845 - val_accuracy: 0.6259\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.4967 - accuracy: 0.8354 - val_loss: 1.0421 - val_accuracy: 0.6978\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.4721 - accuracy: 0.8388 - val_loss: 1.3825 - val_accuracy: 0.5252\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.5026 - accuracy: 0.8352 - val_loss: 8.7503 - val_accuracy: 0.4137\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.4200 - accuracy: 0.8685 - val_loss: 3.2242 - val_accuracy: 0.5468\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.4826 - accuracy: 0.8340 - val_loss: 1.1682 - val_accuracy: 0.6799\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.4053 - accuracy: 0.8571 - val_loss: 1.1151 - val_accuracy: 0.6799\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.3808 - accuracy: 0.8793 - val_loss: 0.6879 - val_accuracy: 0.8022\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.3712 - accuracy: 0.8722 - val_loss: 20.8856 - val_accuracy: 0.4029\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.3269 - accuracy: 0.8904 - val_loss: 4.7805 - val_accuracy: 0.4029\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.3164 - accuracy: 0.8985 - val_loss: 1.2376 - val_accuracy: 0.6223\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.3085 - accuracy: 0.8956 - val_loss: 0.9219 - val_accuracy: 0.7554\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.2686 - accuracy: 0.9150 - val_loss: 1.1101 - val_accuracy: 0.7554\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.2425 - accuracy: 0.9258 - val_loss: 26.1427 - val_accuracy: 0.2230\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.2164 - accuracy: 0.9309 - val_loss: 5.4011 - val_accuracy: 0.2986\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.2263 - accuracy: 0.9264 - val_loss: 0.8475 - val_accuracy: 0.7338\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.1974 - accuracy: 0.9358 - val_loss: 2.9515 - val_accuracy: 0.3561\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.2147 - accuracy: 0.9308 - val_loss: 1.8539 - val_accuracy: 0.6906\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.1491 - accuracy: 0.9505 - val_loss: 23.9007 - val_accuracy: 0.2878\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.1809 - accuracy: 0.9425 - val_loss: 6.0320 - val_accuracy: 0.3381\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.1856 - accuracy: 0.9423 - val_loss: 3.4038 - val_accuracy: 0.4784\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.1516 - accuracy: 0.9506 - val_loss: 4.5806 - val_accuracy: 0.3597\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.1488 - accuracy: 0.9455 - val_loss: 3.3691 - val_accuracy: 0.5072\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.1588 - accuracy: 0.9468 - val_loss: 1.3423 - val_accuracy: 0.6151\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.1217 - accuracy: 0.9565 - val_loss: 5.5396 - val_accuracy: 0.3669\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.0934 - accuracy: 0.9692 - val_loss: 5.4656 - val_accuracy: 0.3849\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.1127 - accuracy: 0.9610 - val_loss: 2.2026 - val_accuracy: 0.5216\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.1399 - accuracy: 0.9535 - val_loss: 2.2517 - val_accuracy: 0.5180\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.0761 - accuracy: 0.9744 - val_loss: 1.3127 - val_accuracy: 0.6511\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 14s 203ms/step - loss: 0.0742 - accuracy: 0.9785 - val_loss: 5.9078 - val_accuracy: 0.5468\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.0940 - accuracy: 0.9699 - val_loss: 2.5922 - val_accuracy: 0.7014\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.1074 - accuracy: 0.9637 - val_loss: 1.1440 - val_accuracy: 0.8129\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.0931 - accuracy: 0.9746 - val_loss: 0.6926 - val_accuracy: 0.8453\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.0693 - accuracy: 0.9753 - val_loss: 4.6401 - val_accuracy: 0.3669\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.0782 - accuracy: 0.9748 - val_loss: 0.7862 - val_accuracy: 0.7986\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.0916 - accuracy: 0.9745 - val_loss: 0.2809 - val_accuracy: 0.9317\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.0762 - accuracy: 0.9790 - val_loss: 2.0613 - val_accuracy: 0.6691\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.0565 - accuracy: 0.9808 - val_loss: 2.4277 - val_accuracy: 0.6835\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 0.1202 - accuracy: 0.9544 - val_loss: 0.6049 - val_accuracy: 0.8201\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0757 - accuracy: 0.9750 - val_loss: 6.4987 - val_accuracy: 0.5396\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0806 - accuracy: 0.9750 - val_loss: 7.3259 - val_accuracy: 0.3058\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0770 - accuracy: 0.9730 - val_loss: 0.5102 - val_accuracy: 0.8561\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0483 - accuracy: 0.9891 - val_loss: 0.7695 - val_accuracy: 0.7806\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0458 - accuracy: 0.9884 - val_loss: 0.8138 - val_accuracy: 0.8165\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0407 - accuracy: 0.9883 - val_loss: 0.5127 - val_accuracy: 0.8957\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0305 - accuracy: 0.9917 - val_loss: 2.0822 - val_accuracy: 0.5396\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0252 - accuracy: 0.9959 - val_loss: 1.0745 - val_accuracy: 0.7842\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0286 - accuracy: 0.9925 - val_loss: 1.0969 - val_accuracy: 0.7842\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0558 - accuracy: 0.9836 - val_loss: 0.6595 - val_accuracy: 0.8705\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0674 - accuracy: 0.9763 - val_loss: 3.7874 - val_accuracy: 0.4928\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0541 - accuracy: 0.9843 - val_loss: 3.1202 - val_accuracy: 0.5755\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.1497 - accuracy: 0.9596 - val_loss: 9.9112 - val_accuracy: 0.2122\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0499 - accuracy: 0.9822 - val_loss: 0.6397 - val_accuracy: 0.8094\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0241 - accuracy: 0.9947 - val_loss: 1.3756 - val_accuracy: 0.6942\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0433 - accuracy: 0.9884 - val_loss: 0.8973 - val_accuracy: 0.7770\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0609 - accuracy: 0.9798 - val_loss: 1.0165 - val_accuracy: 0.8309\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0532 - accuracy: 0.9847 - val_loss: 0.4325 - val_accuracy: 0.8921\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0308 - accuracy: 0.9907 - val_loss: 0.3212 - val_accuracy: 0.9137\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0522 - accuracy: 0.9848 - val_loss: 0.4540 - val_accuracy: 0.9029\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0211 - accuracy: 0.9953 - val_loss: 0.2281 - val_accuracy: 0.9424\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0476 - accuracy: 0.9859 - val_loss: 1.7951 - val_accuracy: 0.6655\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.1220 - accuracy: 0.9654 - val_loss: 0.8183 - val_accuracy: 0.8489\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0492 - accuracy: 0.9852 - val_loss: 5.9658 - val_accuracy: 0.4173\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0514 - accuracy: 0.9792 - val_loss: 11.4384 - val_accuracy: 0.3345\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0462 - accuracy: 0.9847 - val_loss: 2.7904 - val_accuracy: 0.5827\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0415 - accuracy: 0.9857 - val_loss: 11.1717 - val_accuracy: 0.4928\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0812 - accuracy: 0.9777 - val_loss: 17.6726 - val_accuracy: 0.2086\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0554 - accuracy: 0.9843 - val_loss: 6.1662 - val_accuracy: 0.4640\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0556 - accuracy: 0.9830 - val_loss: 2.3843 - val_accuracy: 0.6942\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0359 - accuracy: 0.9886 - val_loss: 1.9903 - val_accuracy: 0.6655\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0340 - accuracy: 0.9938 - val_loss: 0.5688 - val_accuracy: 0.8813\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0159 - accuracy: 0.9943 - val_loss: 0.3900 - val_accuracy: 0.9245\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0172 - accuracy: 0.9962 - val_loss: 0.4713 - val_accuracy: 0.9065\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0753 - accuracy: 0.9780 - val_loss: 126.9422 - val_accuracy: 0.0576\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0399 - accuracy: 0.9872 - val_loss: 4.3494 - val_accuracy: 0.4928\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0418 - accuracy: 0.9886 - val_loss: 20.2351 - val_accuracy: 0.2050\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0290 - accuracy: 0.9886 - val_loss: 1.0789 - val_accuracy: 0.7734\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0545 - accuracy: 0.9799 - val_loss: 1.0942 - val_accuracy: 0.7410\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0420 - accuracy: 0.9896 - val_loss: 4.1893 - val_accuracy: 0.6619\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.0188 - accuracy: 0.9940 - val_loss: 0.1690 - val_accuracy: 0.9460\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0108 - accuracy: 0.9961 - val_loss: 0.9940 - val_accuracy: 0.8022\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0945 - accuracy: 0.9751 - val_loss: 0.4117 - val_accuracy: 0.9029\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0203 - accuracy: 0.9923 - val_loss: 1.0810 - val_accuracy: 0.7698\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0338 - accuracy: 0.9905 - val_loss: 0.3436 - val_accuracy: 0.9317\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0110 - accuracy: 0.9978 - val_loss: 0.2803 - val_accuracy: 0.9245\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.0171 - accuracy: 0.9959 - val_loss: 0.3673 - val_accuracy: 0.9281\n",
      "70/70 [==============================] - 4s 52ms/step - loss: 0.0227 - accuracy: 0.9906\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 0.3673 - accuracy: 0.9281\n",
      "9/9 [==============================] - 1s 107ms/step - loss: 0.4652 - accuracy: 0.9176\n",
      "time: 23min 56s (started: 2021-03-14 23:43:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "model_name='Clasificacion_SL_02'\n",
    "model = Clasificacion_SL(lr=0.01)\n",
    "\n",
    "print(model.summary())\n",
    "filepath='./Modelos_kmer/{}.hdf5'.format(model_name)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss',save_weights_only=True)\n",
    "history=model.fit(X_train, one_hot_labels_train, epochs=100, callbacks=[checkpoint], batch_size=32, validation_data=(X_dev,one_hot_labels_dev))\n",
    "train_loss=model.evaluate(X_train,one_hot_labels_train)\n",
    "val_loss=model.evaluate(X_dev,one_hot_labels_dev)\n",
    "test_loss=model.evaluate(X_test,one_hot_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp75PIv3yWvt"
   },
   "source": [
    "## **3.4 MODELO 03**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qimVXJvIybqW",
    "outputId": "bbe85f9a-411f-4844-ce37-df9021b9ced6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31.9 ms (started: 2021-03-15 00:07:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def Clasificacion_SL(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2], 1), name=\"input_1\")\n",
    "    # layer 1\n",
    "    k=10\n",
    "    layers = tf.keras.layers.Conv2D(128, (5, k), strides=(1,1),activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    \n",
    "    layers_1 = tf.keras.layers.AveragePooling2D((1, X_train.shape[2]-k+1))(layers)\n",
    "    layers_1 = tf.keras.layers.BatchNormalization()(layers_1)\n",
    "\n",
    "    kk=35\n",
    "    layers_2 = tf.keras.layers.Conv2D(128, (1, kk), strides=(1,1),activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers_2 = tf.keras.layers.AveragePooling2D((1, X_train.shape[2]-k+1-kk+1))(layers_2)\n",
    "    layers_2 = tf.keras.layers.BatchNormalization()(layers_2)\n",
    "\n",
    "    layers=tf.concat([layers_1,layers_2],3)\n",
    "    layers = tf.keras.layers.Flatten()(layers)\n",
    "    layers = tf.keras.layers.Dense(21, activation=\"softmax\", name=\"output_1\")(layers)\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=layers)\n",
    "    # optimizer\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    # loss function\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    # Compile model\n",
    "    #model.compile(loss=loss_fn, optimizer=opt, metrics=['acc', 'AUC', 'mse','mae','mape'])\n",
    "    model.compile(loss=loss_fn, optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BaO-QwJSGAZB",
    "outputId": "577d52ef-e34f-440a-b275-1c3c4760c643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 5, 22576, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 1, 22567, 128 6528        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 1, 22533, 128 573568      conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 1, 1, 128)    0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 128)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1, 1, 128)    512         average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1, 1, 128)    512         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 1, 1, 256)    0           batch_normalization[0][0]        \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 256)          0           tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_1 (Dense)                (None, 21)           5397        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 586,517\n",
      "Trainable params: 586,005\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "70/70 [==============================] - 37s 433ms/step - loss: 1.5532 - accuracy: 0.4997 - val_loss: 16.0861 - val_accuracy: 0.1331\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 25s 361ms/step - loss: 0.9586 - accuracy: 0.6499 - val_loss: 4.5230 - val_accuracy: 0.1079\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 25s 358ms/step - loss: 0.9790 - accuracy: 0.6543 - val_loss: 2.8370 - val_accuracy: 0.3705\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 25s 356ms/step - loss: 0.7899 - accuracy: 0.7274 - val_loss: 43.1337 - val_accuracy: 0.0396\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 25s 354ms/step - loss: 0.7298 - accuracy: 0.7257 - val_loss: 4.2359 - val_accuracy: 0.2410\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 25s 355ms/step - loss: 0.6279 - accuracy: 0.7926 - val_loss: 2.3453 - val_accuracy: 0.4604\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 25s 355ms/step - loss: 0.4982 - accuracy: 0.8334 - val_loss: 2.1623 - val_accuracy: 0.3453\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 25s 354ms/step - loss: 0.4148 - accuracy: 0.8616 - val_loss: 1.9410 - val_accuracy: 0.4101\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 25s 354ms/step - loss: 0.3820 - accuracy: 0.8793 - val_loss: 1.1756 - val_accuracy: 0.5576\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 25s 354ms/step - loss: 0.4090 - accuracy: 0.8724 - val_loss: 0.6563 - val_accuracy: 0.7446\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 25s 354ms/step - loss: 0.2913 - accuracy: 0.8980 - val_loss: 47.6377 - val_accuracy: 0.2698\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.3348 - accuracy: 0.8920 - val_loss: 4.0843 - val_accuracy: 0.3022\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.2069 - accuracy: 0.9298 - val_loss: 1.7769 - val_accuracy: 0.5180\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.2084 - accuracy: 0.9233 - val_loss: 4.2239 - val_accuracy: 0.4137\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 0.1561 - accuracy: 0.9492 - val_loss: 1.0374 - val_accuracy: 0.6978\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 0.1808 - accuracy: 0.9425 - val_loss: 1.1235 - val_accuracy: 0.7194\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.1166 - accuracy: 0.9698 - val_loss: 1.5977 - val_accuracy: 0.5540\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0853 - accuracy: 0.9697 - val_loss: 1.2319 - val_accuracy: 0.6799\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0868 - accuracy: 0.9749 - val_loss: 0.5331 - val_accuracy: 0.8597\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0699 - accuracy: 0.9761 - val_loss: 1.4127 - val_accuracy: 0.6475\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 0.0812 - accuracy: 0.9731 - val_loss: 0.8814 - val_accuracy: 0.8237\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 0.1081 - accuracy: 0.9625 - val_loss: 4.4398 - val_accuracy: 0.4281\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0939 - accuracy: 0.9702 - val_loss: 1.3623 - val_accuracy: 0.7410\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 0.0363 - accuracy: 0.9890 - val_loss: 0.8854 - val_accuracy: 0.8417\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0531 - accuracy: 0.9835 - val_loss: 0.6070 - val_accuracy: 0.8165\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 0.0424 - accuracy: 0.9855 - val_loss: 1.1258 - val_accuracy: 0.7410\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 0.0337 - accuracy: 0.9886 - val_loss: 1.0214 - val_accuracy: 0.8022\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 0.0563 - accuracy: 0.9863 - val_loss: 0.7516 - val_accuracy: 0.8633\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0346 - accuracy: 0.9875 - val_loss: 2.4167 - val_accuracy: 0.6367\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0156 - accuracy: 0.9953 - val_loss: 1.0914 - val_accuracy: 0.7626\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 0.0160 - accuracy: 0.9946 - val_loss: 0.8975 - val_accuracy: 0.7806\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 0.0103 - accuracy: 0.9987 - val_loss: 0.3506 - val_accuracy: 0.8993\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 0.0151 - accuracy: 0.9961 - val_loss: 0.6688 - val_accuracy: 0.8597\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0202 - accuracy: 0.9959 - val_loss: 0.3927 - val_accuracy: 0.9281\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0084 - accuracy: 0.9991 - val_loss: 4.4951 - val_accuracy: 0.3417\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0166 - accuracy: 0.9941 - val_loss: 0.8423 - val_accuracy: 0.7950\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 2.9591 - val_accuracy: 0.5072\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 0.0542 - accuracy: 0.9839 - val_loss: 0.5134 - val_accuracy: 0.9065\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 0.0125 - accuracy: 0.9953 - val_loss: 1.2533 - val_accuracy: 0.7050\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0253 - accuracy: 0.9942 - val_loss: 2.1287 - val_accuracy: 0.6295\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0239 - accuracy: 0.9927 - val_loss: 0.5270 - val_accuracy: 0.8957\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 12.0281 - val_accuracy: 0.2842\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0227 - accuracy: 0.9920 - val_loss: 17.8904 - val_accuracy: 0.3525\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0386 - accuracy: 0.9874 - val_loss: 1.2688 - val_accuracy: 0.7590\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0239 - accuracy: 0.9878 - val_loss: 0.8672 - val_accuracy: 0.8381\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0324 - accuracy: 0.9896 - val_loss: 1.2078 - val_accuracy: 0.8201\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0523 - accuracy: 0.9873 - val_loss: 8.6006 - val_accuracy: 0.3381\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0271 - accuracy: 0.9899 - val_loss: 1.1114 - val_accuracy: 0.8058\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0100 - accuracy: 0.9958 - val_loss: 0.7028 - val_accuracy: 0.8417\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0209 - accuracy: 0.9915 - val_loss: 0.5637 - val_accuracy: 0.8777\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0107 - accuracy: 0.9983 - val_loss: 0.3384 - val_accuracy: 0.9460\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 2.6635 - val_accuracy: 0.7914\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0152 - accuracy: 0.9940 - val_loss: 0.3025 - val_accuracy: 0.9317\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0060 - accuracy: 0.9993 - val_loss: 0.3673 - val_accuracy: 0.9245\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0169 - accuracy: 0.9944 - val_loss: 9.6251 - val_accuracy: 0.4640\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0332 - accuracy: 0.9915 - val_loss: 1.8562 - val_accuracy: 0.7482\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0183 - accuracy: 0.9956 - val_loss: 12.6953 - val_accuracy: 0.2698\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0144 - accuracy: 0.9950 - val_loss: 0.8097 - val_accuracy: 0.8273\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.2331 - val_accuracy: 0.9496\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2843 - val_accuracy: 0.9568\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2749 - val_accuracy: 0.9460\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2267 - val_accuracy: 0.9604\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0016 - accuracy: 0.9991 - val_loss: 0.2197 - val_accuracy: 0.9640\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 7.7973e-04 - accuracy: 1.0000 - val_loss: 0.2232 - val_accuracy: 0.9604\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 8.1146e-04 - accuracy: 0.9999 - val_loss: 1.4563 - val_accuracy: 0.7590\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0141 - accuracy: 0.9966 - val_loss: 20.5415 - val_accuracy: 0.2374\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0552 - accuracy: 0.9846 - val_loss: 2.4045 - val_accuracy: 0.7482\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0368 - accuracy: 0.9878 - val_loss: 1.2680 - val_accuracy: 0.8561\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0528 - accuracy: 0.9833 - val_loss: 49.5867 - val_accuracy: 0.1906\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0579 - accuracy: 0.9793 - val_loss: 2.0682 - val_accuracy: 0.7590\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0163 - accuracy: 0.9952 - val_loss: 2.6001 - val_accuracy: 0.7338\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 25s 350ms/step - loss: 0.0301 - accuracy: 0.9915 - val_loss: 0.5349 - val_accuracy: 0.9065\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 0.0218 - accuracy: 0.9956 - val_loss: 1.9800 - val_accuracy: 0.7266\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0096 - accuracy: 0.9976 - val_loss: 1.0907 - val_accuracy: 0.8525\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 25s 350ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 1.0009 - val_accuracy: 0.8669\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 25s 350ms/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 7.0263 - val_accuracy: 0.5432\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0067 - accuracy: 0.9984 - val_loss: 0.3465 - val_accuracy: 0.9496\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 25s 350ms/step - loss: 3.6744e-04 - accuracy: 1.0000 - val_loss: 0.2590 - val_accuracy: 0.9604\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2381 - val_accuracy: 0.9676\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 5.7650e-04 - accuracy: 1.0000 - val_loss: 0.3921 - val_accuracy: 0.9281\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 9.1369e-04 - accuracy: 0.9996 - val_loss: 0.3000 - val_accuracy: 0.9604\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3169 - val_accuracy: 0.9496\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 1.2352 - val_accuracy: 0.7914\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0283 - accuracy: 0.9928 - val_loss: 3.5891 - val_accuracy: 0.6619\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 24s 350ms/step - loss: 0.0303 - accuracy: 0.9928 - val_loss: 6.7494 - val_accuracy: 0.5324\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0304 - accuracy: 0.9898 - val_loss: 2.1307 - val_accuracy: 0.7122\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 2.1219 - val_accuracy: 0.6835\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0330 - accuracy: 0.9921 - val_loss: 0.9308 - val_accuracy: 0.8453\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0074 - accuracy: 0.9974 - val_loss: 0.4504 - val_accuracy: 0.9388\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0081 - accuracy: 0.9972 - val_loss: 0.8583 - val_accuracy: 0.8705\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0086 - accuracy: 0.9956 - val_loss: 0.3189 - val_accuracy: 0.9568\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0175 - accuracy: 0.9975 - val_loss: 1.8464 - val_accuracy: 0.7518\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0210 - accuracy: 0.9961 - val_loss: 1.7631 - val_accuracy: 0.7626\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0065 - accuracy: 0.9990 - val_loss: 1.7771 - val_accuracy: 0.8597\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 25s 351ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.3823 - val_accuracy: 0.9353\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.4370 - val_accuracy: 0.9317\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.5217 - val_accuracy: 0.9137\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 25s 352ms/step - loss: 3.9467e-04 - accuracy: 0.9999 - val_loss: 0.3885 - val_accuracy: 0.9496\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 2.8152e-04 - accuracy: 1.0000 - val_loss: 0.2736 - val_accuracy: 0.9568\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 25s 353ms/step - loss: 5.4860e-04 - accuracy: 1.0000 - val_loss: 0.2844 - val_accuracy: 0.9604\n",
      "70/70 [==============================] - 3s 47ms/step - loss: 0.0095 - accuracy: 0.9996\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.2844 - accuracy: 0.9604\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 0.1960 - accuracy: 0.9713\n",
      "time: 41min 49s (started: 2021-03-15 00:07:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "model_name='Clasificacion_SL_03'\n",
    "model = Clasificacion_SL(lr=0.01)\n",
    "\n",
    "print(model.summary())\n",
    "filepath='./Modelos_kmer/{}.hdf5'.format(model_name)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss',save_weights_only=True)\n",
    "history=model.fit(X_train, one_hot_labels_train, epochs=100, callbacks=[checkpoint], batch_size=32, validation_data=(X_dev,one_hot_labels_dev))\n",
    "train_loss=model.evaluate(X_train,one_hot_labels_train)\n",
    "val_loss=model.evaluate(X_dev,one_hot_labels_dev)\n",
    "test_loss=model.evaluate(X_test,one_hot_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DK61UdFf-Cr5"
   },
   "source": [
    "## **3.5 MODELO 04**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfvCLKtu-JJa",
    "outputId": "6a9a36ee-07fb-469b-9cf4-2da5aafaa269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 61 ms (started: 2021-03-15 00:49:37 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def Clasificacion_SL(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2], 1), name=\"input_1\")\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv2D(4, (5, 1), strides=(1,1),activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers_1 = tf.keras.layers.AveragePooling2D((1, X_train.shape[2]))(layers_1)\n",
    "    layers_1 = tf.keras.layers.BatchNormalization()(layers_1)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Conv2D(12, (5, 2), strides=(1,1),activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers_2 = tf.keras.layers.AveragePooling2D((1, X_train.shape[2]-2+1))(layers_2)\n",
    "    layers_2 = tf.keras.layers.BatchNormalization()(layers_2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Conv2D(52, (5, 3), strides=(1,1),activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers_3 = tf.keras.layers.AveragePooling2D((1, X_train.shape[2]-3+1))(layers_3)\n",
    "    layers_3 = tf.keras.layers.BatchNormalization()(layers_3)\n",
    "\n",
    "    layers_4 = tf.keras.layers.Conv2D(161, (5, 4), strides=(1,1),activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers_4 = tf.keras.layers.AveragePooling2D((1, X_train.shape[2]-4+1))(layers_4)\n",
    "    layers_4 = tf.keras.layers.BatchNormalization()(layers_4)\n",
    "\n",
    "    layers_5 = tf.keras.layers.Conv2D(190, (5, 5), strides=(1,1),activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers_5 = tf.keras.layers.AveragePooling2D((1, X_train.shape[2]-5+1))(layers_5)\n",
    "    layers_5 = tf.keras.layers.BatchNormalization()(layers_5)\n",
    "\n",
    "    layers_6 = tf.keras.layers.Conv2D(89, (5, 6), strides=(1,1),activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers_6 = tf.keras.layers.AveragePooling2D((1, X_train.shape[2]-6+1))(layers_6)\n",
    "    layers_6 = tf.keras.layers.BatchNormalization()(layers_6)\n",
    "\n",
    "    layers=tf.concat([layers_1,layers_2,layers_3,layers_4,layers_5,layers_6],3)\n",
    "    layers = tf.keras.layers.Flatten()(layers)\n",
    "    layers = tf.keras.layers.Dense(21, activation=\"softmax\", name=\"output_1\")(layers)\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=layers)\n",
    "    # optimizer\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    # loss function\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    # Compile model\n",
    "    #model.compile(loss=loss_fn, optimizer=opt, metrics=['acc', 'AUC', 'mse','mae','mape'])\n",
    "    model.compile(loss=loss_fn, optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tz4s9XhHY7Qu",
    "outputId": "19258a09-6ab6-4951-8900-d01197ae69e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 5, 22576, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 1, 22576, 4)  24          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 1, 22575, 12) 132         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1, 22574, 52) 832         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 1, 22573, 161 3381        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 1, 22572, 190 4940        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 1, 22571, 89) 2759        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 1, 1, 4)      0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 12)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 1, 1, 52)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 1, 1, 161)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 1, 1, 190)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 1, 1, 89)     0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1, 1, 4)      16          average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1, 1, 12)     48          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1, 1, 52)     208         average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 1, 161)    644         average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1, 1, 190)    760         average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1, 1, 89)     356         average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 1, 1, 508)    0           batch_normalization[0][0]        \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 508)          0           tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_1 (Dense)                (None, 21)           10689       flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 24,789\n",
      "Trainable params: 23,773\n",
      "Non-trainable params: 1,016\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "70/70 [==============================] - 14s 157ms/step - loss: 1.6859 - accuracy: 0.4369 - val_loss: 2.2567 - val_accuracy: 0.2086\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 1.0200 - accuracy: 0.6118 - val_loss: 1.9265 - val_accuracy: 0.2086\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.9945 - accuracy: 0.6300 - val_loss: 1.8968 - val_accuracy: 0.2158\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.9284 - accuracy: 0.6628 - val_loss: 1.8910 - val_accuracy: 0.3309\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.8626 - accuracy: 0.6746 - val_loss: 1.4432 - val_accuracy: 0.4353\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.7452 - accuracy: 0.7389 - val_loss: 1.2905 - val_accuracy: 0.4640\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.7039 - accuracy: 0.7634 - val_loss: 1.2384 - val_accuracy: 0.5468\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.6451 - accuracy: 0.7970 - val_loss: 6.6915 - val_accuracy: 0.2410\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.5612 - accuracy: 0.7970 - val_loss: 1.4199 - val_accuracy: 0.5504\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.5267 - accuracy: 0.8310 - val_loss: 0.4912 - val_accuracy: 0.8597\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.4872 - accuracy: 0.8239 - val_loss: 0.6535 - val_accuracy: 0.7914\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.4498 - accuracy: 0.8429 - val_loss: 1.7926 - val_accuracy: 0.6043\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.4379 - accuracy: 0.8470 - val_loss: 0.5872 - val_accuracy: 0.7986\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.4428 - accuracy: 0.8523 - val_loss: 0.8177 - val_accuracy: 0.7338\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.4130 - accuracy: 0.8632 - val_loss: 13.5503 - val_accuracy: 0.1871\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.4250 - accuracy: 0.8520 - val_loss: 1.7985 - val_accuracy: 0.4928\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.4074 - accuracy: 0.8630 - val_loss: 0.4659 - val_accuracy: 0.8741\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.3718 - accuracy: 0.8766 - val_loss: 0.5198 - val_accuracy: 0.8129\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.4159 - accuracy: 0.8702 - val_loss: 6.8765 - val_accuracy: 0.1439\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.3512 - accuracy: 0.8752 - val_loss: 1.1512 - val_accuracy: 0.7446\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.3638 - accuracy: 0.8799 - val_loss: 0.4826 - val_accuracy: 0.8561\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.3489 - accuracy: 0.8900 - val_loss: 0.5021 - val_accuracy: 0.8453\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.3141 - accuracy: 0.8924 - val_loss: 1.9483 - val_accuracy: 0.6079\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.3022 - accuracy: 0.8983 - val_loss: 0.6303 - val_accuracy: 0.7662\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.3108 - accuracy: 0.9016 - val_loss: 0.3978 - val_accuracy: 0.8777\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.3150 - accuracy: 0.8962 - val_loss: 0.3272 - val_accuracy: 0.8885\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2595 - accuracy: 0.9181 - val_loss: 0.3012 - val_accuracy: 0.9173\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.3131 - accuracy: 0.9013 - val_loss: 0.3549 - val_accuracy: 0.8777\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2531 - accuracy: 0.9087 - val_loss: 0.4055 - val_accuracy: 0.8993\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.2540 - accuracy: 0.9168 - val_loss: 0.6995 - val_accuracy: 0.7842\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2834 - accuracy: 0.9064 - val_loss: 0.4079 - val_accuracy: 0.8849\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2672 - accuracy: 0.9124 - val_loss: 0.2715 - val_accuracy: 0.9065\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.2445 - accuracy: 0.9247 - val_loss: 3.8123 - val_accuracy: 0.5935\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.2643 - accuracy: 0.9162 - val_loss: 1.5207 - val_accuracy: 0.6223\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2354 - accuracy: 0.9287 - val_loss: 0.6545 - val_accuracy: 0.7914\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.2794 - accuracy: 0.9079 - val_loss: 0.4244 - val_accuracy: 0.8525\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2311 - accuracy: 0.9218 - val_loss: 0.3051 - val_accuracy: 0.9065\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.2101 - accuracy: 0.9335 - val_loss: 0.3542 - val_accuracy: 0.8777\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2390 - accuracy: 0.9202 - val_loss: 0.4362 - val_accuracy: 0.8669\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2640 - accuracy: 0.9101 - val_loss: 0.3615 - val_accuracy: 0.8453\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2144 - accuracy: 0.9288 - val_loss: 0.4812 - val_accuracy: 0.8705\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2267 - accuracy: 0.9242 - val_loss: 0.7606 - val_accuracy: 0.7914\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2393 - accuracy: 0.9162 - val_loss: 0.4061 - val_accuracy: 0.8669\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.2629 - accuracy: 0.9081 - val_loss: 0.3373 - val_accuracy: 0.8705\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1600 - accuracy: 0.9532 - val_loss: 0.3880 - val_accuracy: 0.8705\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1951 - accuracy: 0.9367 - val_loss: 0.4368 - val_accuracy: 0.8597\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2030 - accuracy: 0.9322 - val_loss: 0.3592 - val_accuracy: 0.8705\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1922 - accuracy: 0.9422 - val_loss: 0.2724 - val_accuracy: 0.9353\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1995 - accuracy: 0.9359 - val_loss: 0.2890 - val_accuracy: 0.9209\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1875 - accuracy: 0.9441 - val_loss: 0.7962 - val_accuracy: 0.7302\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1713 - accuracy: 0.9506 - val_loss: 0.2323 - val_accuracy: 0.9281\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1869 - accuracy: 0.9309 - val_loss: 0.2022 - val_accuracy: 0.9281\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2022 - accuracy: 0.9377 - val_loss: 0.2766 - val_accuracy: 0.9065\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1791 - accuracy: 0.9325 - val_loss: 0.3290 - val_accuracy: 0.9101\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1762 - accuracy: 0.9402 - val_loss: 0.2204 - val_accuracy: 0.9173\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1490 - accuracy: 0.9532 - val_loss: 0.7219 - val_accuracy: 0.8094\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2014 - accuracy: 0.9415 - val_loss: 0.2490 - val_accuracy: 0.9317\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.2162 - accuracy: 0.9237 - val_loss: 0.6362 - val_accuracy: 0.8417\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2172 - accuracy: 0.9255 - val_loss: 0.3741 - val_accuracy: 0.9065\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.1680 - accuracy: 0.9455 - val_loss: 0.4692 - val_accuracy: 0.8597\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.1561 - accuracy: 0.9570 - val_loss: 0.7603 - val_accuracy: 0.8273\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1648 - accuracy: 0.9469 - val_loss: 0.2654 - val_accuracy: 0.9245\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.1707 - accuracy: 0.9534 - val_loss: 0.4934 - val_accuracy: 0.8417\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.2253 - accuracy: 0.9161 - val_loss: 0.3907 - val_accuracy: 0.8957\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1969 - accuracy: 0.9349 - val_loss: 0.2490 - val_accuracy: 0.9101\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1565 - accuracy: 0.9495 - val_loss: 27.1947 - val_accuracy: 0.1079\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.1886 - accuracy: 0.9481 - val_loss: 0.9630 - val_accuracy: 0.7554\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1366 - accuracy: 0.9585 - val_loss: 0.2123 - val_accuracy: 0.9388\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1220 - accuracy: 0.9632 - val_loss: 0.2658 - val_accuracy: 0.8957\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.1743 - accuracy: 0.9397 - val_loss: 0.2734 - val_accuracy: 0.9173\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1738 - accuracy: 0.9474 - val_loss: 0.3891 - val_accuracy: 0.8885\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.1598 - accuracy: 0.9386 - val_loss: 0.2427 - val_accuracy: 0.9317\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1323 - accuracy: 0.9544 - val_loss: 0.3513 - val_accuracy: 0.9137\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1585 - accuracy: 0.9474 - val_loss: 0.2521 - val_accuracy: 0.9209\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.1549 - accuracy: 0.9481 - val_loss: 0.4126 - val_accuracy: 0.8489\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1407 - accuracy: 0.9498 - val_loss: 0.4578 - val_accuracy: 0.8525\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1369 - accuracy: 0.9538 - val_loss: 0.2953 - val_accuracy: 0.9029\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1553 - accuracy: 0.9470 - val_loss: 0.2970 - val_accuracy: 0.9245\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1283 - accuracy: 0.9604 - val_loss: 0.3704 - val_accuracy: 0.8993\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.1399 - accuracy: 0.9540 - val_loss: 0.1961 - val_accuracy: 0.9424\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1148 - accuracy: 0.9565 - val_loss: 0.3922 - val_accuracy: 0.8849\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1020 - accuracy: 0.9594 - val_loss: 0.5202 - val_accuracy: 0.8453\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1298 - accuracy: 0.9600 - val_loss: 0.2774 - val_accuracy: 0.9173\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1608 - accuracy: 0.9493 - val_loss: 0.4403 - val_accuracy: 0.8813\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1473 - accuracy: 0.9524 - val_loss: 0.2592 - val_accuracy: 0.9245\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1167 - accuracy: 0.9654 - val_loss: 0.3179 - val_accuracy: 0.9245\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1513 - accuracy: 0.9579 - val_loss: 1.4736 - val_accuracy: 0.8058\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.1245 - accuracy: 0.9584 - val_loss: 0.5922 - val_accuracy: 0.8489\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.0989 - accuracy: 0.9630 - val_loss: 0.3786 - val_accuracy: 0.9029\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1337 - accuracy: 0.9543 - val_loss: 0.8268 - val_accuracy: 0.8237\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1366 - accuracy: 0.9584 - val_loss: 0.1520 - val_accuracy: 0.9532\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1347 - accuracy: 0.9468 - val_loss: 0.2701 - val_accuracy: 0.9173\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1354 - accuracy: 0.9558 - val_loss: 0.5421 - val_accuracy: 0.8633\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1485 - accuracy: 0.9387 - val_loss: 0.2883 - val_accuracy: 0.9101\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1174 - accuracy: 0.9601 - val_loss: 0.3513 - val_accuracy: 0.9137\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.1089 - accuracy: 0.9599 - val_loss: 0.3196 - val_accuracy: 0.9173\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.1107 - accuracy: 0.9609 - val_loss: 0.3493 - val_accuracy: 0.8993\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.1184 - accuracy: 0.9607 - val_loss: 0.5155 - val_accuracy: 0.8633\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 8s 119ms/step - loss: 0.1350 - accuracy: 0.9522 - val_loss: 0.3913 - val_accuracy: 0.8885\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.1394 - accuracy: 0.9601 - val_loss: 0.6898 - val_accuracy: 0.8201\n",
      "70/70 [==============================] - 4s 60ms/step - loss: 0.5784 - accuracy: 0.8416\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.6898 - accuracy: 0.8201\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 0.8199 - accuracy: 0.8100\n",
      "time: 14min 29s (started: 2021-03-15 00:49:37 +00:00)\n"
     ]
    }
   ],
   "source": [
    "model_name='Clasificacion_SL_04'\n",
    "model = Clasificacion_SL(lr=0.01)\n",
    "\n",
    "print(model.summary())\n",
    "filepath='./Modelos_kmer/{}.hdf5'.format(model_name)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss',save_weights_only=True)\n",
    "history=model.fit(X_train, one_hot_labels_train, epochs=100, callbacks=[checkpoint], batch_size=32, validation_data=(X_dev,one_hot_labels_dev))\n",
    "train_loss=model.evaluate(X_train,one_hot_labels_train)\n",
    "val_loss=model.evaluate(X_dev,one_hot_labels_dev)\n",
    "test_loss=model.evaluate(X_test,one_hot_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_NXCkZ0HjDj"
   },
   "source": [
    "## **3.6 TERL ORIGINAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLddhyjjHngP",
    "outputId": "7f96ed24-cb3c-454e-91b3-e41d4eac7999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 39.7 ms (started: 2021-03-15 01:46:59 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def TERL(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.05,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2], 1), name=\"input_1\")\n",
    "    # layer 1\n",
    "    layers = tf.keras.layers.Conv2D(64, (5, 20), strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers = tf.keras.layers.AveragePooling2D((1, 10), strides= (1, 10))(layers)\n",
    "    #layers=tf.keras.layers.BatchNormalization()(layers)\n",
    "    # layer 2\n",
    "    layers = tf.keras.layers.Conv2D(32, (1, 20), strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.AveragePooling2D((1, 15), strides= (1, 15))(layers)\n",
    "    #layers=tf.keras.layers.BatchNormalization()(layers)\n",
    "    # layer 3\n",
    "    layers = tf.keras.layers.Conv2D(32, (1, 35), strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.AveragePooling2D((1, 15), strides= (1, 15))(layers)\n",
    "    #layers=tf.keras.layers.BatchNormalization()(layers)\n",
    "    \n",
    "    # layer 4\n",
    "    layers = tf.keras.layers.Flatten()(layers)\n",
    "    layers = tf.keras.layers.Dense(1000,activation=fun_act,kernel_regularizer=regularizer(w_reg))(layers)\n",
    "    layers = tf.keras.layers.Dropout(dp)(layers)\n",
    "    layers = tf.keras.layers.Dense(500,activation=fun_act,kernel_regularizer=regularizer(w_reg))(layers)\n",
    "    # layer 5\n",
    "    predictions = tf.keras.layers.Dense(21, activation=\"softmax\", name=\"output_1\")(layers)\n",
    "    # model generation\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    # optimizer\n",
    "    #opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    opt = optimizador(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    # loss function\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    # Compile model\n",
    "    #model.compile(loss=loss_fn, optimizer=opt, metrics=['acc', 'AUC', 'mse','mae','mape'])\n",
    "    model.compile(loss=loss_fn, optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfjicCh3HtJE",
    "outputId": "8c7cabc2-1788-4fac-8d5d-6ee3adb7df95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 5, 22576, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 1, 22557, 64)      6464      \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 1, 2255, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 1, 2236, 32)       40992     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 1, 149, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 115, 32)        35872     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 1, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              225000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "output_1 (Dense)             (None, 21)                10521     \n",
      "=================================================================\n",
      "Total params: 819,349\n",
      "Trainable params: 819,349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 2.0761 - accuracy: 0.2648 - val_loss: 1.4351 - val_accuracy: 0.4029\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1.4316 - accuracy: 0.3976 - val_loss: 1.3980 - val_accuracy: 0.4820\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1.4131 - accuracy: 0.4277 - val_loss: 1.1135 - val_accuracy: 0.5252\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 1.0574 - accuracy: 0.5905 - val_loss: 0.9113 - val_accuracy: 0.6942\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 0.8395 - accuracy: 0.6970 - val_loss: 0.7578 - val_accuracy: 0.7446\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 0.6884 - accuracy: 0.7552 - val_loss: 0.8480 - val_accuracy: 0.7482\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 2s 32ms/step - loss: 0.6786 - accuracy: 0.7638 - val_loss: 0.6496 - val_accuracy: 0.7806\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.5496 - accuracy: 0.8128 - val_loss: 0.6115 - val_accuracy: 0.8058\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.4229 - accuracy: 0.8548 - val_loss: 0.4910 - val_accuracy: 0.8633\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.3319 - accuracy: 0.8907 - val_loss: 0.4349 - val_accuracy: 0.8561\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.2902 - accuracy: 0.9114 - val_loss: 0.4088 - val_accuracy: 0.8813\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.2610 - accuracy: 0.9162 - val_loss: 0.4007 - val_accuracy: 0.8885\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.1968 - accuracy: 0.9295 - val_loss: 0.3476 - val_accuracy: 0.9137\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.1726 - accuracy: 0.9441 - val_loss: 0.4327 - val_accuracy: 0.8885\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.1858 - accuracy: 0.9341 - val_loss: 0.3674 - val_accuracy: 0.9173\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.1584 - accuracy: 0.9486 - val_loss: 0.3616 - val_accuracy: 0.9209\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.1013 - accuracy: 0.9686 - val_loss: 0.4186 - val_accuracy: 0.8993\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.0936 - accuracy: 0.9733 - val_loss: 0.3556 - val_accuracy: 0.9245\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.0819 - accuracy: 0.9706 - val_loss: 0.4812 - val_accuracy: 0.9101\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.0522 - accuracy: 0.9838 - val_loss: 0.5177 - val_accuracy: 0.9065\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.0509 - accuracy: 0.9856 - val_loss: 0.4465 - val_accuracy: 0.9065\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0768 - accuracy: 0.9753 - val_loss: 0.4033 - val_accuracy: 0.9173\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.0748 - accuracy: 0.9801 - val_loss: 0.5232 - val_accuracy: 0.8957\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.0430 - accuracy: 0.9876 - val_loss: 0.3885 - val_accuracy: 0.9317\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 2s 33ms/step - loss: 0.0282 - accuracy: 0.9915 - val_loss: 0.3868 - val_accuracy: 0.9424\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0508 - accuracy: 0.9812 - val_loss: 0.4333 - val_accuracy: 0.9209\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0322 - accuracy: 0.9891 - val_loss: 0.4672 - val_accuracy: 0.9353\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0201 - accuracy: 0.9957 - val_loss: 0.4191 - val_accuracy: 0.9460\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0305 - accuracy: 0.9928 - val_loss: 0.5219 - val_accuracy: 0.9281\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0507 - accuracy: 0.9832 - val_loss: 0.7216 - val_accuracy: 0.8633\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.1181 - accuracy: 0.9647 - val_loss: 0.5102 - val_accuracy: 0.9173\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0440 - accuracy: 0.9841 - val_loss: 0.4920 - val_accuracy: 0.8921\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0819 - accuracy: 0.9780 - val_loss: 0.3449 - val_accuracy: 0.9317\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0179 - accuracy: 0.9961 - val_loss: 0.4224 - val_accuracy: 0.9245\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0061 - accuracy: 0.9988 - val_loss: 0.4519 - val_accuracy: 0.9245\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.4358 - val_accuracy: 0.9353\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0065 - accuracy: 0.9986 - val_loss: 0.5295 - val_accuracy: 0.9209\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0105 - accuracy: 0.9974 - val_loss: 0.5417 - val_accuracy: 0.9317\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0124 - accuracy: 0.9972 - val_loss: 0.5871 - val_accuracy: 0.9245\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0245 - accuracy: 0.9948 - val_loss: 0.3485 - val_accuracy: 0.9388\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0413 - accuracy: 0.9872 - val_loss: 0.3790 - val_accuracy: 0.9317\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 0.0333 - accuracy: 0.9919 - val_loss: 0.4646 - val_accuracy: 0.9424\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 0.0355 - accuracy: 0.9902 - val_loss: 0.5115 - val_accuracy: 0.9317\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 0.0084 - accuracy: 0.9973 - val_loss: 0.4700 - val_accuracy: 0.9317\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5232 - val_accuracy: 0.9245\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 5.4419e-04 - accuracy: 1.0000 - val_loss: 0.5570 - val_accuracy: 0.9245\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 4.4251e-04 - accuracy: 1.0000 - val_loss: 0.5626 - val_accuracy: 0.9281\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 3.7609e-04 - accuracy: 1.0000 - val_loss: 0.6055 - val_accuracy: 0.9209\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 2.0804e-04 - accuracy: 1.0000 - val_loss: 0.6090 - val_accuracy: 0.9281\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 1.7845e-04 - accuracy: 1.0000 - val_loss: 0.6200 - val_accuracy: 0.9281\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 3.0383e-04 - accuracy: 1.0000 - val_loss: 0.6553 - val_accuracy: 0.9245\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 3.3588e-04 - accuracy: 0.9998 - val_loss: 0.6319 - val_accuracy: 0.9245\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 8.6701e-04 - accuracy: 1.0000 - val_loss: 0.6690 - val_accuracy: 0.9353\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 2.5811e-04 - accuracy: 1.0000 - val_loss: 0.6484 - val_accuracy: 0.9317\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 1.0931e-04 - accuracy: 1.0000 - val_loss: 0.6534 - val_accuracy: 0.9317\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 9.8128e-05 - accuracy: 1.0000 - val_loss: 0.6563 - val_accuracy: 0.9317\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 6.7596e-05 - accuracy: 1.0000 - val_loss: 0.6637 - val_accuracy: 0.9281\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 6.4210e-05 - accuracy: 1.0000 - val_loss: 0.6769 - val_accuracy: 0.9281\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 4.2450e-05 - accuracy: 1.0000 - val_loss: 0.6781 - val_accuracy: 0.9281\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 6.1432e-05 - accuracy: 1.0000 - val_loss: 0.6813 - val_accuracy: 0.9281\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 6.3340e-05 - accuracy: 1.0000 - val_loss: 0.6826 - val_accuracy: 0.9281\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 4.1831e-05 - accuracy: 1.0000 - val_loss: 0.6866 - val_accuracy: 0.9281\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 4.8247e-05 - accuracy: 1.0000 - val_loss: 0.6956 - val_accuracy: 0.9281\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 4.9583e-05 - accuracy: 1.0000 - val_loss: 0.7023 - val_accuracy: 0.9245\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 5.0291e-05 - accuracy: 1.0000 - val_loss: 0.7069 - val_accuracy: 0.9245\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 4.6981e-05 - accuracy: 1.0000 - val_loss: 0.7126 - val_accuracy: 0.9245\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 3.3899e-05 - accuracy: 1.0000 - val_loss: 0.7112 - val_accuracy: 0.9245\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 3.2356e-05 - accuracy: 1.0000 - val_loss: 0.7182 - val_accuracy: 0.9281\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 3.2912e-05 - accuracy: 1.0000 - val_loss: 0.7196 - val_accuracy: 0.9281\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 2.7155e-05 - accuracy: 1.0000 - val_loss: 0.7233 - val_accuracy: 0.9281\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 1.8162e-05 - accuracy: 1.0000 - val_loss: 0.7281 - val_accuracy: 0.9281\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 1.7855e-05 - accuracy: 1.0000 - val_loss: 0.7319 - val_accuracy: 0.9281\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 2.6149e-05 - accuracy: 1.0000 - val_loss: 0.7358 - val_accuracy: 0.9281\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 1.9145e-05 - accuracy: 1.0000 - val_loss: 0.7382 - val_accuracy: 0.9281\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 1.7757e-05 - accuracy: 1.0000 - val_loss: 0.7440 - val_accuracy: 0.9281\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 1.4930e-05 - accuracy: 1.0000 - val_loss: 0.7533 - val_accuracy: 0.9281\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 1.4209e-05 - accuracy: 1.0000 - val_loss: 0.7573 - val_accuracy: 0.9281\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 1.4823e-05 - accuracy: 1.0000 - val_loss: 0.7539 - val_accuracy: 0.9281\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 1.2290e-05 - accuracy: 1.0000 - val_loss: 0.7577 - val_accuracy: 0.9281\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 1.4128e-05 - accuracy: 1.0000 - val_loss: 0.7658 - val_accuracy: 0.9281\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 1.5991e-05 - accuracy: 1.0000 - val_loss: 0.7718 - val_accuracy: 0.9281\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 1.1562e-05 - accuracy: 1.0000 - val_loss: 0.7721 - val_accuracy: 0.9281\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 9.7158e-06 - accuracy: 1.0000 - val_loss: 0.7781 - val_accuracy: 0.9281\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 8.8913e-06 - accuracy: 1.0000 - val_loss: 0.7734 - val_accuracy: 0.9281\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 1.0884e-05 - accuracy: 1.0000 - val_loss: 0.7812 - val_accuracy: 0.9281\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 1.3290e-05 - accuracy: 1.0000 - val_loss: 0.7922 - val_accuracy: 0.9245\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 9.6818e-06 - accuracy: 1.0000 - val_loss: 0.8002 - val_accuracy: 0.9245\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 9.3466e-06 - accuracy: 1.0000 - val_loss: 0.7859 - val_accuracy: 0.9245\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 8.1933e-06 - accuracy: 1.0000 - val_loss: 0.7950 - val_accuracy: 0.9245\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 6.2709e-06 - accuracy: 1.0000 - val_loss: 0.7948 - val_accuracy: 0.9281\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 5.3227e-06 - accuracy: 1.0000 - val_loss: 0.7970 - val_accuracy: 0.9245\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 6.0814e-06 - accuracy: 1.0000 - val_loss: 0.8114 - val_accuracy: 0.9245\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 2s 35ms/step - loss: 6.0636e-06 - accuracy: 1.0000 - val_loss: 0.8225 - val_accuracy: 0.9281\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 1.2810e-05 - accuracy: 1.0000 - val_loss: 0.8472 - val_accuracy: 0.9209\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0684 - accuracy: 0.9852 - val_loss: 0.6571 - val_accuracy: 0.8237\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.2554 - accuracy: 0.9270 - val_loss: 0.3633 - val_accuracy: 0.9209\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.1159 - accuracy: 0.9702 - val_loss: 0.4748 - val_accuracy: 0.8993\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0540 - accuracy: 0.9825 - val_loss: 0.4608 - val_accuracy: 0.9281\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0496 - accuracy: 0.9865 - val_loss: 0.4338 - val_accuracy: 0.9209\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 2s 34ms/step - loss: 0.0186 - accuracy: 0.9975 - val_loss: 0.3454 - val_accuracy: 0.9424\n",
      "70/70 [==============================] - 1s 14ms/step - loss: 0.0070 - accuracy: 0.9987\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.3454 - accuracy: 0.9424\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 0.2402 - accuracy: 0.9462\n",
      "time: 4min 3s (started: 2021-03-15 01:47:01 +00:00)\n"
     ]
    }
   ],
   "source": [
    "model_name='TERL'\n",
    "model = TERL()\n",
    "\n",
    "print(model.summary())\n",
    "filepath='./Modelos_kmer/{}.hdf5'.format(model_name)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss',save_weights_only=True)\n",
    "history=model.fit(X_train, one_hot_labels_train, epochs=100, callbacks=[checkpoint], batch_size=32, validation_data=(X_dev,one_hot_labels_dev))\n",
    "train_loss=model.evaluate(X_train,one_hot_labels_train)\n",
    "val_loss=model.evaluate(X_dev,one_hot_labels_dev)\n",
    "test_loss=model.evaluate(X_test,one_hot_labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeVT3CsUwcob"
   },
   "source": [
    "# **4. FILTROS A MANO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbEfy18W7xi2",
    "outputId": "af81b6f1-f5f9-401c-d06c-91c6ad0e1df6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "def weights():\n",
    "    k=0\n",
    "    n_1=4**1\n",
    "    W_1=np.zeros((5,1,1,n_1))\n",
    "    b_1=np.zeros((1,n_1)).reshape(n_1,)\n",
    "    for i in range(4):\n",
    "      W_1[i,0,0,i]=1\n",
    "    \n",
    "    k=0\n",
    "    n_2=4**2\n",
    "    W_2=np.zeros((5,2,1,n_2))\n",
    "    b_2=-np.ones((1,n_2)).reshape(n_2,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        W_2[i,0,0,k]=1\n",
    "        W_2[j,1,0,k]=1\n",
    "        k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_3=4**3\n",
    "    W_3=np.zeros((5,3,1,n_3))\n",
    "    b_3=-2*np.ones((1,n_3)).reshape(n_3,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          W_3[i,0,0,k]=1\n",
    "          W_3[j,1,0,k]=1\n",
    "          W_3[l,2,0,k]=1\n",
    "          k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_4=4**4\n",
    "    W_4=np.zeros((5,4,1,n_4))\n",
    "    b_4=-3*np.ones((1,n_4)).reshape(n_4,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          for m in range(4):\n",
    "            W_4[i,0,0,k]=1\n",
    "            W_4[j,1,0,k]=1\n",
    "            W_4[l,2,0,k]=1\n",
    "            W_4[m,3,0,k]=1\n",
    "            k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_5=4**5\n",
    "    W_5=np.zeros((5,5,1,n_5))\n",
    "    b_5=-4*np.ones((1,n_5)).reshape(n_5,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          for m in range(4):\n",
    "            for n in range(4):\n",
    "              W_5[i,0,0,k]=1\n",
    "              W_5[j,1,0,k]=1\n",
    "              W_5[l,2,0,k]=1\n",
    "              W_5[m,3,0,k]=1\n",
    "              W_5[n,4,0,k]=1\n",
    "              k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_6=4**6\n",
    "    W_6=np.zeros((5,6,1,n_6))\n",
    "    b_6=-5*np.ones((1,n_6)).reshape(n_6,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          for m in range(4):\n",
    "            for n in range(4):\n",
    "              for p in range(4):\n",
    "                W_6[i,0,0,k]=1\n",
    "                W_6[j,1,0,k]=1\n",
    "                W_6[l,2,0,k]=1\n",
    "                W_6[m,3,0,k]=1\n",
    "                W_6[n,4,0,k]=1\n",
    "                W_6[p,5,0,k]=1\n",
    "                k=k+1\n",
    "    return [W_1,b_1,W_2,b_2,W_3,b_3,W_4,b_4,W_5,b_5,W_6,b_6]\n",
    "A=weights()\n",
    "np.save('Weights_SL'+'.npy', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yo2nXHm9HSsW"
   },
   "source": [
    "# **5. DETECCION DE LONG TERMINAL REPEATS (LTR) EN RETROTRANSPOSONES LTR-RT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4ogU8u9Huvf"
   },
   "source": [
    "## **5.1 CARGA DE DATOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dy8_iWNPHz_f",
    "outputId": "a0c5d3fc-e71b-4bd8-d4d6-e8d315b5a63d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2785, 5, 22576)\n",
      "(2785,)\n",
      "(2228, 5, 22576)\n",
      "(2228,)\n",
      "(278, 5, 22576)\n",
      "(278,)\n",
      "(279, 5, 22576)\n",
      "(279,)\n",
      "(2228, 21)\n",
      "time: 13.2 s (started: 2021-03-20 14:07:55 +00:00)\n"
     ]
    }
   ],
   "source": [
    "X = np.load('InpactorDB_Repbase_format.fa.filtered_center.npy')\n",
    "Y = open('InpactorDB_Repbase_format.fa.kmers','r')\n",
    "Texto=Y.read()\n",
    "Y.close()\n",
    "Texto=Texto.splitlines()\n",
    "Texto=Texto[1:]\n",
    "label=[]\n",
    "for i in range(len(Texto)):\n",
    "    A=[int(x) for x in Texto[i].split(',')]\n",
    "    label.append(A)\n",
    "\n",
    "label=np.array(label)\n",
    "label=label[:,0]\n",
    "\n",
    "print(X.shape)\n",
    "print(label.shape)\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "X_train, X_test_dev, Y_train, Y_test_dev = train_test_split(X, label, test_size=validation_size, random_state=seed)\n",
    "\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_test_dev, Y_test_dev, test_size=0.5, random_state=seed)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(Y_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "X = None\n",
    "Y = None\n",
    "\n",
    "X_test_dev = None\n",
    "Y_test_dev = None\n",
    "\n",
    "\n",
    "one_hot_labels_train = tf.keras.utils.to_categorical(Y_train, num_classes=21)\n",
    "one_hot_labels_dev = tf.keras.utils.to_categorical(Y_dev, num_classes=21)\n",
    "one_hot_labels_test = tf.keras.utils.to_categorical(Y_test, num_classes=21)\n",
    "\n",
    "print(one_hot_labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8B5t70ppF4t7",
    "outputId": "8ee187a9-a840-40cf-a114-930264da3fda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5, 23854)\n",
      "(1000, 1)\n",
      "(800, 5, 23854)\n",
      "(800, 1)\n",
      "(100, 5, 23854)\n",
      "(100, 1)\n",
      "(100, 5, 23854)\n",
      "(100, 1)\n",
      "time: 2min 28s (started: 2021-03-20 14:44:10 +00:00)\n"
     ]
    }
   ],
   "source": [
    "X = np.load('auto_curation_class0.fasta.filtered_center.npy')\n",
    "Y = np.load('auto_curation_class0.fasta.filtered_center_labels.npy')\n",
    "X=X[0:1000,:,:]\n",
    "Y=Y[0:1000,:]\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "X_train, X_test_dev, Y_train, Y_test_dev = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_test_dev, Y_test_dev, test_size=0.5, random_state=seed)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(Y_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "X = None\n",
    "Y = None\n",
    "\n",
    "X_test_dev = None\n",
    "Y_test_dev = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qG8EKcDUH1Xw"
   },
   "source": [
    "## **5.2 FILTROS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XE0hIA1SH7Qw",
    "outputId": "fd3c2029-76ee-491a-8927-7203167ae8c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 5, 23854, 1)]     0         \n",
      "_________________________________________________________________\n",
      "TAG (Conv2D)                 (None, 1, 23852, 1)       16        \n",
      "=================================================================\n",
      "Total params: 16\n",
      "Trainable params: 16\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "El primer ejemplo empieza en 9428.0 y termina en 14425.0\n",
      "La longitud del elemento transponible es de  4997.0\n",
      "El número de activaciones por encima de cero son  37\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9674,  9694,  9712,  9983,  9997, 10048, 10114, 10206, 10234,\n",
      "       10312, 10353, 10396, 10606, 10708, 10743, 10815, 10842, 11011,\n",
      "       11153, 11226, 11379, 11423, 11585, 11748, 11833, 12361, 12466,\n",
      "       12688, 12765, 13003, 13357, 13492, 13555, 13812, 14124, 14282,\n",
      "       14320]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9179.0 y termina en 14675.0\n",
      "La longitud del elemento transponible es de  5496.0\n",
      "El número de activaciones por encima de cero son  41\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9182,  9412,  9506,  9763,  9890, 10139, 10307, 10372, 10601,\n",
      "       10688, 10721, 10745, 10864, 11018, 11144, 11242, 11314, 11342,\n",
      "       11404, 11531, 11617, 11681, 11768, 12019, 12047, 12427, 12460,\n",
      "       12520, 13132, 13285, 13288, 13357, 13546, 13571, 13742, 13768,\n",
      "       13832, 13903, 14297, 14527, 14621]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 6551.0 y termina en 17303.0\n",
      "La longitud del elemento transponible es de  10752.0\n",
      "El número de activaciones por encima de cero son  88\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 6553,  6591,  6609,  6752,  6778,  6801,  6860,  6868,  6873,\n",
      "        7092,  7219,  7303,  7756,  7808,  7975,  8114,  8324,  8370,\n",
      "        8467,  8946,  9099,  9854, 10007, 10127, 10242, 10424, 10494,\n",
      "       10745, 10749, 10844, 10938, 10998, 11084, 11156, 11370, 11457,\n",
      "       11469, 11499, 11592, 11806, 11970, 12340, 12348, 12582, 12605,\n",
      "       12949, 13190, 13286, 13448, 13469, 13674, 13681, 13701, 13950,\n",
      "       14398, 14459, 14634, 14861, 14927, 14942, 14992, 15088, 15446,\n",
      "       15528, 15550, 15573, 15741, 15784, 15793, 15902, 16012, 16015,\n",
      "       16030, 16146, 16161, 16167, 16338, 16394, 16424, 16488, 16581,\n",
      "       16787, 16843, 16986, 17012, 17035, 17102, 17107]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9332.0 y termina en 14521.0\n",
      "La longitud del elemento transponible es de  5189.0\n",
      "El número de activaciones por encima de cero son  51\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0]), array([ 9335,  9482,  9503,  9596,  9696,  9917,  9928,  9943,  9978,\n",
      "       10083, 10180, 10245, 10270, 10316, 10542, 10570, 10650, 10773,\n",
      "       10848, 11015, 11143, 11157, 11169, 11459, 11571, 11661, 11763,\n",
      "       12376, 12399, 12625, 12760, 12782, 12832, 12919, 12970, 13068,\n",
      "       13274, 13287, 13312, 13374, 13445, 13449, 13476, 13589, 13639,\n",
      "       13939, 14034, 14181, 14202, 14295, 14395]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9603.0 y termina en 14251.0\n",
      "La longitud del elemento transponible es de  4648.0\n",
      "El número de activaciones por encima de cero son  41\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9659,  9695,  9711,  9718,  9730,  9741, 10127, 10154, 10309,\n",
      "       10363, 10411, 10438, 10586, 10649, 10837, 10976, 11678, 12035,\n",
      "       12067, 12136, 12217, 12244, 12335, 12517, 12673, 12682, 12769,\n",
      "       13117, 13426, 13520, 13726, 13891, 13966, 13998, 14003, 14070,\n",
      "       14106, 14122, 14129, 14141, 14152]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9290.0 y termina en 14564.0\n",
      "La longitud del elemento transponible es de  5274.0\n",
      "El número de activaciones por encima de cero son  50\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0]), array([ 9298,  9310,  9464,  9651,  9672,  9681,  9812,  9837,  9881,\n",
      "       10157, 10172, 10226, 10338, 10442, 10475, 10587, 10811, 10943,\n",
      "       10962, 10994, 11009, 11046, 11135, 11156, 11228, 11391, 11633,\n",
      "       11867, 11957, 12023, 12095, 12351, 12398, 12417, 12425, 12530,\n",
      "       12585, 12827, 12902, 12954, 12998, 13241, 13244, 13280, 13313,\n",
      "       13700, 13745, 14258, 14270, 14424]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9006.0 y termina en 14847.0\n",
      "La longitud del elemento transponible es de  5841.0\n",
      "El número de activaciones por encima de cero son  52\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]), array([ 9102,  9192,  9437,  9469,  9503,  9545,  9586,  9613,  9799,\n",
      "        9863,  9915, 10316, 10630, 10643, 10812, 10824, 10961, 11030,\n",
      "       11150, 11410, 11527, 11551, 11636, 11639, 11968, 12082, 12240,\n",
      "       12247, 12493, 12582, 12625, 12675, 12948, 13047, 13126, 13338,\n",
      "       13453, 13570, 13675, 13679, 13709, 13724, 13738, 13810, 13841,\n",
      "       13866, 13931, 14176, 14200, 14688, 14769, 14798]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9579.0 y termina en 14275.0\n",
      "La longitud del elemento transponible es de  4696.0\n",
      "El número de activaciones por encima de cero son  32\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9611,  9625, 10286, 10307, 10360, 10385, 10589, 10892, 11327,\n",
      "       11416, 11420, 11452, 11462, 11615, 11618, 11705, 11866, 11894,\n",
      "       11914, 11986, 12005, 12049, 12173, 12775, 12856, 13123, 13191,\n",
      "       13224, 13702, 13884, 14145, 14159]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9610.0 y termina en 14244.0\n",
      "La longitud del elemento transponible es de  4634.0\n",
      "El número de activaciones por encima de cero son  74\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]), array([ 9612,  9636,  9679,  9683,  9705,  9763,  9784,  9833,  9946,\n",
      "        9981, 10060, 10087, 10117, 10165, 10259, 10403, 10515, 10782,\n",
      "       10834, 11009, 11093, 11175, 11335, 11377, 11399, 11743, 11973,\n",
      "       12132, 12270, 12303, 12339, 12349, 12381, 12396, 12459, 12464,\n",
      "       12509, 12527, 12537, 12587, 12617, 12623, 12638, 12680, 12683,\n",
      "       12689, 12758, 12842, 12909, 12923, 12936, 12944, 12962, 12971,\n",
      "       13291, 13315, 13521, 13623, 13771, 13809, 13844, 13862, 13940,\n",
      "       13977, 14003, 14024, 14052, 14084, 14092, 14117, 14141, 14184,\n",
      "       14188, 14210]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9321.0 y termina en 14533.0\n",
      "La longitud del elemento transponible es de  5212.0\n",
      "El número de activaciones por encima de cero son  62\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9348,  9415,  9475,  9556,  9664,  9750,  9852, 10026, 10059,\n",
      "       10073, 10189, 10269, 10314, 10472, 10608, 10657, 10701, 10743,\n",
      "       10865, 10923, 10981, 11241, 11338, 11718, 11766, 11918, 11941,\n",
      "       11959, 12110, 12204, 12294, 12309, 12325, 12399, 12502, 12540,\n",
      "       12584, 12600, 12669, 12687, 12743, 12747, 12924, 13155, 13180,\n",
      "       13213, 13322, 13361, 13367, 13420, 13506, 13525, 13568, 13602,\n",
      "       13623, 13986, 14049, 14152, 14212, 14279, 14339, 14528]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9604.0 y termina en 14250.0\n",
      "La longitud del elemento transponible es de  4646.0\n",
      "El número de activaciones por encima de cero son  49\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0]), array([ 9613,  9653,  9710,  9716,  9748,  9755, 10011, 10086, 10239,\n",
      "       10289, 10397, 10572, 10599, 10617, 11015, 11022, 11103, 11118,\n",
      "       11226, 11561, 11691, 11912, 12026, 12048, 12062, 12099, 12128,\n",
      "       12131, 12206, 12521, 12545, 12644, 12740, 13022, 13073, 13182,\n",
      "       13379, 13517, 13523, 13543, 13589, 13774, 13817, 14006, 14046,\n",
      "       14103, 14109, 14141, 14148]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9481.0 y termina en 14373.0\n",
      "La longitud del elemento transponible es de  4892.0\n",
      "El número de activaciones por encima de cero son  34\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9497,  9549,  9559,  9562,  9834, 10032, 10080, 10098, 10222,\n",
      "       10476, 10782, 10831, 10872, 10957, 11194, 11220, 11535, 11661,\n",
      "       12292, 12703, 12781, 13054, 13200, 13228, 13329, 13723, 13832,\n",
      "       13844, 13956, 14060, 14184, 14236, 14246, 14249]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9182.0 y termina en 14671.0\n",
      "La longitud del elemento transponible es de  5489.0\n",
      "El número de activaciones por encima de cero son  44\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9356,  9429,  9446,  9685, 10253, 10372, 10828, 10912, 11041,\n",
      "       11096, 11114, 11170, 11269, 11398, 11596, 11645, 11693, 11740,\n",
      "       11767, 11845, 12026, 12172, 12466, 12622, 12787, 12790, 12838,\n",
      "       12935, 12985, 13360, 13406, 13596, 13745, 13803, 13880, 13884,\n",
      "       13934, 14030, 14093, 14108, 14115, 14445, 14518, 14535]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9271.0 y termina en 14582.0\n",
      "La longitud del elemento transponible es de  5311.0\n",
      "El número de activaciones por encima de cero son  84\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9376,  9561,  9607,  9651,  9672,  9691,  9762,  9864,  9875,\n",
      "        9879,  9886,  9915,  9961, 10005, 10023, 10086, 10098, 10162,\n",
      "       10256, 10432, 10459, 10547, 10561, 10594, 10630, 10732, 10818,\n",
      "       10880, 10930, 11019, 11079, 11096, 11304, 11346, 11349, 11389,\n",
      "       11437, 11499, 11536, 11608, 11627, 11709, 11724, 11783, 12004,\n",
      "       12142, 12155, 12230, 12276, 12299, 12333, 12458, 12478, 12511,\n",
      "       12698, 12957, 13023, 13056, 13083, 13090, 13236, 13244, 13250,\n",
      "       13268, 13348, 13377, 13445, 13448, 13464, 13572, 13608, 13625,\n",
      "       13689, 13726, 13740, 13814, 13821, 14035, 14131, 14136, 14147,\n",
      "       14157, 14300, 14490]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9418.0 y termina en 14436.0\n",
      "La longitud del elemento transponible es de  5018.0\n",
      "El número de activaciones por encima de cero son  71\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0]), array([ 9421,  9435,  9445,  9449,  9481,  9684,  9968,  9998, 10011,\n",
      "       10034, 10055, 10059, 10098, 10158, 10206, 10331, 10407, 10517,\n",
      "       10625, 10814, 10853, 10905, 11079, 11106, 11127, 11133, 11156,\n",
      "       11201, 11292, 11388, 11480, 11502, 11510, 11675, 11684, 11757,\n",
      "       11817, 11834, 11912, 12006, 12032, 12038, 12084, 12186, 12201,\n",
      "       12207, 12225, 12248, 12284, 12290, 12315, 12627, 12920, 12980,\n",
      "       13082, 13200, 13281, 13313, 13401, 13754, 13839, 13944, 14063,\n",
      "       14108, 14135, 14183, 14288, 14302, 14312, 14316, 14348]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 6190.0 y termina en 17663.0\n",
      "La longitud del elemento transponible es de  11473.0\n",
      "El número de activaciones por encima de cero son  90\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0]), array([ 6192,  6199,  6239,  6288,  6459,  6464,  6684,  6692,  6811,\n",
      "        6895,  6953,  7150,  7351,  7403,  7573,  7666,  7717,  7834,\n",
      "        8154,  8250,  8709,  8758,  9124,  9312,  9411,  9526,  9576,\n",
      "        9805, 10062, 10482, 10629, 10722, 10744, 10767, 11128, 11214,\n",
      "       11217, 11431, 11460, 11626, 11782, 12015, 12070, 12192, 12284,\n",
      "       12333, 12701, 12984, 13042, 13078, 13231, 13526, 13616, 13844,\n",
      "       14088, 14108, 14158, 14354, 14581, 14700, 14788, 14813, 14837,\n",
      "       15007, 15339, 15548, 15625, 15640, 15791, 15859, 15964, 16077,\n",
      "       16198, 16207, 16316, 16752, 16838, 16906, 16914, 16988, 17002,\n",
      "       17063, 17157, 17194, 17201, 17241, 17290, 17370, 17461, 17466]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9558.0 y termina en 14296.0\n",
      "La longitud del elemento transponible es de  4738.0\n",
      "El número de activaciones por encima de cero son  51\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0]), array([ 9577,  9631,  9657,  9663, 10007, 10136, 10173, 10206, 10236,\n",
      "       10316, 10328, 10371, 10551, 11102, 11142, 11243, 11271, 11636,\n",
      "       11682, 11742, 11786, 11810, 11820, 11898, 11918, 11976, 12039,\n",
      "       12119, 12122, 12290, 12327, 12375, 12413, 12525, 12584, 12713,\n",
      "       12921, 12972, 13151, 13200, 13388, 13637, 13788, 13827, 13845,\n",
      "       13871, 13893, 14032, 14086, 14112, 14118]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9582.0 y termina en 14272.0\n",
      "La longitud del elemento transponible es de  4690.0\n",
      "El número de activaciones por encima de cero son  34\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9655,  9820,  9866,  9878,  9906,  9917,  9990, 10331, 10585,\n",
      "       10609, 10661, 10750, 10765, 10942, 10969, 10997, 11024, 11290,\n",
      "       11471, 11524, 11648, 11675, 12181, 12844, 13009, 13578, 13633,\n",
      "       13895, 13971, 14136, 14182, 14194, 14222, 14233]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9493.0 y termina en 14361.0\n",
      "La longitud del elemento transponible es de  4868.0\n",
      "El número de activaciones por encima de cero son  44\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9498,  9508,  9541,  9583,  9597,  9638,  9643,  9814,  9855,\n",
      "       10090, 10158, 10368, 10578, 10911, 10965, 10968, 11171, 11328,\n",
      "       11358, 11536, 11737, 11798, 11866, 12093, 12177, 12357, 12679,\n",
      "       12723, 12771, 12787, 13153, 13170, 13299, 13513, 13900, 14007,\n",
      "       14064, 14144, 14154, 14187, 14229, 14243, 14284, 14289]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9294.0 y termina en 14559.0\n",
      "La longitud del elemento transponible es de  5265.0\n",
      "El número de activaciones por encima de cero son  39\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9297,  9302,  9357,  9438,  9452,  9483,  9583,  9655,  9917,\n",
      "        9929,  9957, 10704, 10785, 10907, 11009, 11054, 11108, 11153,\n",
      "       11345, 11556, 11681, 12479, 12699, 12761, 12866, 13412, 13610,\n",
      "       13613, 13706, 13865, 13964, 14196, 14201, 14256, 14337, 14351,\n",
      "       14382, 14482, 14554]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 10262.0 y termina en 13591.0\n",
      "La longitud del elemento transponible es de  3329.0\n",
      "El número de activaciones por encima de cero son  40\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([10322, 10338, 10379, 10423, 10503, 10517, 10596, 10600, 10690,\n",
      "       10699, 10822, 10947, 11037, 11070, 11194, 11215, 11223, 11295,\n",
      "       11331, 11358, 11385, 11547, 11769, 11823, 11904, 12082, 12099,\n",
      "       12177, 12207, 12273, 12526, 12571, 12577, 12687, 13068, 13191,\n",
      "       13392, 13441, 13484, 13578]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 6077.0 y termina en 17777.0\n",
      "La longitud del elemento transponible es de  11700.0\n",
      "El número de activaciones por encima de cero son  92\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0]), array([ 6079,  6219,  6276,  6302,  6392,  6397,  6450,  6536,  6616,\n",
      "        6743,  6822,  7077,  7279,  7331,  7501,  7634,  7762,  7850,\n",
      "        8465,  8975,  9084,  9361,  9526,  9830, 10012, 10029, 10082,\n",
      "       10337, 10432, 10526, 10586, 10672, 10717, 10744, 11117, 11132,\n",
      "       11381, 11683, 11758, 11972, 12136, 12244, 12272, 12554, 12685,\n",
      "       12794, 12818, 13195, 13314, 13761, 13902, 13920, 13974, 14220,\n",
      "       14223, 14671, 14907, 15003, 15137, 15218, 15269, 15412, 15610,\n",
      "       15806, 15842, 15853, 15921, 16003, 16026, 16049, 16217, 16260,\n",
      "       16269, 16378, 16473, 16488, 16491, 16506, 16622, 16814, 16870,\n",
      "       16900, 16964, 16977, 17247, 17262, 17460, 17486, 17576, 17581,\n",
      "       17634, 17720]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9522.0 y termina en 14331.0\n",
      "La longitud del elemento transponible es de  4809.0\n",
      "El número de activaciones por encima de cero son  54\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9553,  9573,  9579,  9628,  9632,  9641,  9650,  9667,  9715,\n",
      "        9721, 10003, 10104, 10333, 10382, 10447, 10604, 10769, 10793,\n",
      "       10832, 10844, 11093, 11446, 11497, 11560, 12146, 12182, 12201,\n",
      "       12272, 12472, 12544, 12561, 12564, 12858, 12873, 12965, 13164,\n",
      "       13172, 13223, 13239, 13472, 13505, 13599, 13619, 13712, 13761,\n",
      "       14071, 14091, 14097, 14146, 14150, 14159, 14185, 14239, 14246]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9335.0 y termina en 14518.0\n",
      "La longitud del elemento transponible es de  5183.0\n",
      "El número de activaciones por encima de cero son  93\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0]), array([ 9536,  9569,  9691,  9766,  9806,  9857,  9988,  9998, 10033,\n",
      "       10157, 10185, 10229, 10232, 10264, 10316, 10370, 10396, 10424,\n",
      "       10478, 10556, 10569, 10585, 10645, 10652, 10695, 10786, 10868,\n",
      "       10960, 11000, 11036, 11098, 11205, 11219, 11327, 11396, 11445,\n",
      "       11486, 11540, 11639, 11777, 11786, 11806, 11873, 11914, 11965,\n",
      "       11973, 11978, 12177, 12261, 12348, 12403, 12419, 12424, 12464,\n",
      "       12518, 12521, 12533, 12554, 12587, 12653, 12707, 12712, 12745,\n",
      "       12755, 12795, 12837, 12853, 12866, 12874, 12883, 12958, 13005,\n",
      "       13090, 13139, 13146, 13291, 13359, 13398, 13459, 13489, 13506,\n",
      "       13684, 13695, 13822, 13846, 13879, 13968, 14028, 14036, 14268,\n",
      "       14301, 14423, 14498]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9869.0 y termina en 13985.0\n",
      "La longitud del elemento transponible es de  4116.0\n",
      "El número de activaciones por encima de cero son  47\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0]), array([ 9935,  9955,  9970, 10025, 10048, 10219, 10272, 10328, 10517,\n",
      "       10578, 10640, 10676, 10784, 10956, 11099, 11178, 11209, 11253,\n",
      "       11324, 11441, 11463, 11494, 11520, 11574, 11771, 11990, 12116,\n",
      "       12131, 12238, 12420, 12524, 12567, 12696, 12725, 12789, 12823,\n",
      "       12912, 13050, 13198, 13591, 13603, 13641, 13644, 13795, 13813,\n",
      "       13830, 13908]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9081.0 y termina en 14772.0\n",
      "La longitud del elemento transponible es de  5691.0\n",
      "El número de activaciones por encima de cero son  103\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9112,  9127,  9145,  9238,  9447,  9628,  9806, 10000, 10160,\n",
      "       10187, 10630, 10672, 10781, 10790, 10861, 10906, 10910, 10955,\n",
      "       10984, 11097, 11176, 11198, 11205, 11270, 11314, 11522, 11530,\n",
      "       11558, 11740, 11766, 11771, 11779, 11794, 11801, 11930, 11964,\n",
      "       11978, 11993, 11998, 12004, 12035, 12070, 12112, 12209, 12279,\n",
      "       12325, 12362, 12490, 12514, 12541, 12717, 12723, 12743, 12775,\n",
      "       12843, 12857, 12981, 13025, 13152, 13209, 13240, 13257, 13264,\n",
      "       13273, 13319, 13328, 13335, 13415, 13476, 13504, 13530, 13567,\n",
      "       13598, 13643, 13752, 13870, 13878, 13952, 14011, 14020, 14026,\n",
      "       14032, 14068, 14084, 14097, 14120, 14185, 14193, 14240, 14265,\n",
      "       14315, 14422, 14462, 14512, 14544, 14566, 14572, 14580, 14602,\n",
      "       14607, 14671, 14686, 14704]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9500.0 y termina en 14353.0\n",
      "La longitud del elemento transponible es de  4853.0\n",
      "El número de activaciones por encima de cero son  32\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9526,  9567,  9576,  9610, 10006, 10017, 10470, 10632, 10735,\n",
      "       10887, 10923, 10933, 11328, 11332, 11473, 11514, 11742, 11758,\n",
      "       11819, 12088, 12457, 12562, 12853, 12913, 12989, 13240, 13712,\n",
      "       13754, 13766, 14223, 14273, 14307]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9014.0 y termina en 14840.0\n",
      "La longitud del elemento transponible es de  5826.0\n",
      "El número de activaciones por encima de cero son  103\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9055,  9070,  9181,  9363,  9390,  9574,  9752,  9946, 10010,\n",
      "       10025, 10106, 10133, 10561, 10575, 10617, 10725, 10734, 10805,\n",
      "       10819, 10848, 10852, 10897, 11032, 11111, 11133, 11140, 11205,\n",
      "       11249, 11454, 11462, 11490, 11672, 11698, 11703, 11711, 11726,\n",
      "       11847, 11864, 11870, 11898, 11912, 11927, 11932, 11938, 11969,\n",
      "       12004, 12143, 12213, 12245, 12259, 12296, 12424, 12451, 12613,\n",
      "       12619, 12639, 12671, 12738, 12752, 12875, 12929, 12987, 13187,\n",
      "       13211, 13220, 13266, 13275, 13282, 13337, 13344, 13395, 13424,\n",
      "       13485, 13513, 13539, 13607, 13653, 13762, 13887, 13895, 13969,\n",
      "       13998, 14007, 14029, 14044, 14050, 14102, 14115, 14138, 14159,\n",
      "       14203, 14211, 14258, 14283, 14606, 14628, 14634, 14642, 14664,\n",
      "       14677, 14739, 14754, 14772]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9577.0 y termina en 14276.0\n",
      "La longitud del elemento transponible es de  4699.0\n",
      "El número de activaciones por encima de cero son  66\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9616,  9640,  9665,  9700,  9738,  9761,  9906, 10171, 10179,\n",
      "       10204, 10263, 10329, 10341, 10401, 10476, 10567, 10677, 10986,\n",
      "       10999, 11265, 11413, 11471, 11522, 11593, 11630, 11639, 11689,\n",
      "       11720, 11860, 11867, 11996, 12010, 12025, 12079, 12094, 12268,\n",
      "       12274, 12293, 12517, 12532, 12631, 12664, 12695, 13006, 13021,\n",
      "       13108, 13187, 13304, 13315, 13343, 13361, 13366, 13379, 13414,\n",
      "       13508, 13514, 13517, 13598, 13826, 14033, 14057, 14082, 14111,\n",
      "       14117, 14155, 14178]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9365.0 y termina en 14489.0\n",
      "La longitud del elemento transponible es de  5124.0\n",
      "El número de activaciones por encima de cero son  35\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9709,  9726,  9902,  9996, 10428, 10497, 10596, 10790, 10871,\n",
      "       11002, 11104, 11186, 11230, 11282, 11405, 11446, 11965, 12187,\n",
      "       12229, 12661, 12700, 13006, 13009, 13034, 13099, 13115, 13202,\n",
      "       13681, 13690, 13727, 13819, 13852, 14228, 14373, 14390]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 4894.0 y termina en 18959.0\n",
      "La longitud del elemento transponible es de  14065.0\n",
      "El número de activaciones por encima de cero son  230\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 4930,  5034,  5171,  5177,  5206,  5293,  5348,  5400,  5413,\n",
      "        5452,  5488,  5494,  5509,  5646,  5676,  5680,  5694,  5707,\n",
      "        5713,  5823,  5862,  6161,  6402,  6491,  6522,  6529,  6547,\n",
      "        6557,  6629,  6681,  6763,  6783,  6874,  6972,  6994,  7010,\n",
      "        7038,  7119,  7146,  7153,  7176,  7429,  7559,  7567,  7579,\n",
      "        7612,  7649,  7687,  7700,  7816,  7921,  8000,  8120,  8333,\n",
      "        8404,  8416,  8635,  8654,  8796,  8870,  9003,  9090,  9118,\n",
      "        9143,  9161,  9215,  9236,  9355,  9364,  9401,  9530,  9550,\n",
      "        9605,  9690,  9736,  9766,  9812,  9851, 10007, 10023, 10052,\n",
      "       10057, 10069, 10125, 10181, 10197, 10208, 10256, 10307, 10382,\n",
      "       10425, 10460, 10475, 10534, 10542, 10613, 10642, 10650, 10663,\n",
      "       10903, 10934, 10990, 11044, 11075, 11082, 11102, 11221, 11227,\n",
      "       11233, 11527, 11597, 11617, 11659, 11680, 11685, 11740, 11890,\n",
      "       11898, 11976, 12116, 12135, 12154, 12232, 12472, 12748, 12765,\n",
      "       12769, 12811, 12828, 12852, 12873, 12894, 12912, 12943, 12951,\n",
      "       13048, 13163, 13181, 13226, 13444, 13495, 13564, 13597, 13606,\n",
      "       13632, 13711, 13829, 13875, 13933, 13973, 14097, 14120, 14161,\n",
      "       14184, 14228, 14314, 14319, 14336, 14527, 14581, 14635, 14686,\n",
      "       14775, 14837, 14864, 14879, 14901, 15013, 15137, 15229, 15253,\n",
      "       15269, 15296, 15332, 15539, 15569, 15582, 15619, 15676, 15821,\n",
      "       15907, 15957, 15996, 16073, 16105, 16150, 16158, 16210, 16230,\n",
      "       16249, 16292, 16351, 16384, 16488, 16598, 16611, 16668, 16706,\n",
      "       16747, 16762, 16778, 16807, 17094, 17158, 17180, 17306, 17312,\n",
      "       17409, 17451, 17615, 17752, 17758, 17787, 17874, 17929, 17981,\n",
      "       17994, 18033, 18069, 18075, 18090, 18227, 18257, 18261, 18275,\n",
      "       18288, 18294, 18404, 18443, 18729]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9278.0 y termina en 14576.0\n",
      "La longitud del elemento transponible es de  5298.0\n",
      "El número de activaciones por encima de cero son  37\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9281,  9451,  9729,  9787, 10069, 10099, 10156, 10183, 10320,\n",
      "       10779, 11187, 11290, 11326, 11344, 11367, 11374, 11485, 11575,\n",
      "       11656, 11847, 11941, 12069, 12109, 12463, 12814, 12832, 12865,\n",
      "       12928, 12931, 13133, 13166, 13228, 13381, 13873, 13903, 14272,\n",
      "       14442]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9499.0 y termina en 14355.0\n",
      "La longitud del elemento transponible es de  4856.0\n",
      "El número de activaciones por encima de cero son  79\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9599,  9619,  9629,  9685,  9699,  9764,  9841,  9856, 10093,\n",
      "       10185, 10212, 10230, 10242, 10270, 10293, 10389, 10469, 10771,\n",
      "       10783, 10835, 10859, 11080, 11133, 11159, 11171, 11355, 11370,\n",
      "       11400, 11484, 11530, 11559, 11595, 11792, 11805, 11846, 11948,\n",
      "       12033, 12058, 12100, 12265, 12465, 12471, 12595, 12644, 12679,\n",
      "       12762, 12773, 12816, 12864, 12958, 12990, 13050, 13085, 13106,\n",
      "       13224, 13301, 13344, 13440, 13459, 13478, 13549, 13561, 13584,\n",
      "       13620, 13685, 13776, 13818, 13850, 13907, 13947, 13951, 14094,\n",
      "       14114, 14124, 14180, 14194, 14259, 14336, 14351]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9708.0 y termina en 14145.0\n",
      "La longitud del elemento transponible es de  4437.0\n",
      "El número de activaciones por encima de cero son  51\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0]), array([ 9776,  9794,  9805,  9840, 10072, 10085, 10105, 10192, 10201,\n",
      "       10271, 10301, 10393, 10459, 10513, 10571, 10718, 10828, 10882,\n",
      "       10906, 10942, 11011, 11162, 11402, 11738, 11839, 11876, 11954,\n",
      "       12175, 12234, 12517, 12578, 12598, 12705, 12781, 12853, 12874,\n",
      "       12935, 12942, 12947, 13129, 13312, 13384, 13449, 13456, 13563,\n",
      "       13566, 13822, 14010, 14028, 14039, 14074]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 6056.0 y termina en 17798.0\n",
      "La longitud del elemento transponible es de  11742.0\n",
      "El número de activaciones por encima de cero son  142\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 6128,  6163,  6411,  6431,  6479,  6572,  6580,  6589,  6598,\n",
      "        6624,  6696,  6717,  6729,  6783,  6790,  6795,  6859,  6932,\n",
      "        6996,  7001,  7029,  7080,  7113,  7170,  7187,  7323,  7379,\n",
      "        7388,  7413,  7626,  7645,  7658,  7829,  7955,  8063,  8223,\n",
      "        8373,  8910,  9169,  9241,  9411,  9501,  9640,  9672,  9705,\n",
      "        9855,  9865,  9951, 10180, 10200, 10212, 10252, 10263, 10287,\n",
      "       10353, 10384, 10396, 10410, 10605, 10828, 10935, 11104, 11151,\n",
      "       11181, 11341, 11427, 11550, 11562, 11676, 11772, 12150, 12228,\n",
      "       12241, 12313, 12558, 12615, 12712, 12862, 12957, 13146, 13535,\n",
      "       13556, 13562, 13612, 13688, 13697, 13750, 14045, 14282, 14295,\n",
      "       14523, 14846, 14855, 15034, 15210, 15312, 15355, 15412, 15510,\n",
      "       15540, 15604, 15665, 15732, 15736, 15747, 15751, 15770, 16057,\n",
      "       16146, 16203, 16222, 16299, 16332, 16348, 16452, 16547, 16728,\n",
      "       16763, 17011, 17031, 17079, 17172, 17180, 17189, 17198, 17224,\n",
      "       17296, 17317, 17329, 17383, 17390, 17395, 17459, 17532, 17596,\n",
      "       17601, 17629, 17680, 17685, 17713, 17771, 17788]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9136.0 y termina en 14718.0\n",
      "La longitud del elemento transponible es de  5582.0\n",
      "El número de activaciones por encima de cero son  69\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0]), array([ 9346,  9450,  9482,  9581,  9770,  9778, 10064, 10089, 10181,\n",
      "       10189, 10223, 10297, 10306, 10332, 10357, 10409, 10417, 10524,\n",
      "       10610, 10665, 10729, 10761, 10767, 10786, 10824, 11040, 11092,\n",
      "       11136, 11148, 11254, 11363, 11533, 11543, 11835, 11910, 11932,\n",
      "       11966, 12161, 12172, 12185, 12326, 12351, 12410, 12541, 12552,\n",
      "       12642, 12651, 12925, 12982, 13013, 13140, 13161, 13170, 13314,\n",
      "       13370, 13445, 13511, 13548, 13637, 13694, 13740, 13778, 13799,\n",
      "       13921, 14056, 14147, 14162, 14480, 14505]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9931.0 y termina en 13922.0\n",
      "La longitud del elemento transponible es de  3991.0\n",
      "El número de activaciones por encima de cero son  42\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9934,  9943,  9960,  9967, 10117, 10497, 10709, 10718, 10793,\n",
      "       10895, 10904, 11112, 11141, 11154, 11632, 11668, 11827, 11833,\n",
      "       11902, 11986, 11989, 12014, 12242, 12262, 12376, 12383, 12442,\n",
      "       12446, 12538, 12635, 12681, 12883, 12904, 12953, 13078, 13303,\n",
      "       13575, 13656, 13755, 13764, 13781, 13788]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 6040.0 y termina en 17813.0\n",
      "La longitud del elemento transponible es de  11773.0\n",
      "El número de activaciones por encima de cero son  148\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 6111,  6255,  6298,  6318,  6343,  6354,  6375,  6459,  6554,\n",
      "        6566,  6571,  6579,  6674,  6686,  6701,  6753,  6766,  6773,\n",
      "        6844,  6917,  6983,  6988,  7073,  7164,  7182,  7251,  7292,\n",
      "        7312,  7377,  7674,  7815,  7882,  7895,  8052,  8157,  8173,\n",
      "        8221,  8233,  8649,  8910,  8919,  9039,  9138,  9393,  9495,\n",
      "        9564,  9618,  9693,  9714,  9783,  9855, 10080, 10092, 10138,\n",
      "       10169, 10181, 10195, 10255, 10389, 10495, 10569, 10768, 10889,\n",
      "       10966, 11030, 11110, 11212, 11311, 11374, 11407, 11908, 11983,\n",
      "       11986, 12019, 12118, 12180, 12229, 12294, 12493, 12554, 12610,\n",
      "       12810, 12837, 12897, 13024, 13061, 13355, 13366, 13407, 13419,\n",
      "       13437, 13451, 13456, 13515, 13521, 13556, 13578, 13731, 13755,\n",
      "       13776, 14045, 14102, 14272, 14512, 14887, 14920, 14971, 14979,\n",
      "       15219, 15381, 15529, 15759, 15770, 15774, 15925, 16059, 16147,\n",
      "       16227, 16246, 16321, 16358, 16523, 16579, 16740, 16878, 16884,\n",
      "       16892, 16927, 16947, 16972, 17088, 17183, 17195, 17200, 17208,\n",
      "       17303, 17315, 17330, 17382, 17402, 17411, 17473, 17546, 17612,\n",
      "       17617, 17695, 17785, 17803]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9297.0 y termina en 14556.0\n",
      "La longitud del elemento transponible es de  5259.0\n",
      "El número de activaciones por encima de cero son  65\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9323,  9526,  9548,  9632,  9653,  9681,  9742,  9914,  9929,\n",
      "        9982, 10257, 10367, 10444, 10604, 10635, 10671, 10794, 10876,\n",
      "       10937, 11036, 11050, 11108, 11194, 11197, 11277, 11358, 11564,\n",
      "       11624, 11637, 11685, 11775, 11787, 11790, 11811, 11942, 12058,\n",
      "       12113, 12328, 12412, 12435, 12486, 12524, 12529, 12700, 12790,\n",
      "       12799, 12847, 12958, 13078, 13156, 13271, 13475, 13564, 13612,\n",
      "       13725, 13733, 13854, 14002, 14079, 14159, 14362, 14384, 14468,\n",
      "       14489, 14517]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9555.0 y termina en 14298.0\n",
      "La longitud del elemento transponible es de  4743.0\n",
      "El número de activaciones por encima de cero son  19\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9612,  9639,  9749,  9923, 10315, 10357, 10390, 10737, 11243,\n",
      "       11621, 12429, 12455, 12696, 13290, 13339, 13735, 13962, 14196,\n",
      "       14223]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 8825.0 y termina en 15028.0\n",
      "La longitud del elemento transponible es de  6203.0\n",
      "El número de activaciones por encima de cero son  71\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0]), array([ 8989,  9012,  9153,  9178,  9231,  9471,  9476,  9599,  9635,\n",
      "        9743,  9915, 10110, 10168, 10283, 10400, 10453, 10479, 10533,\n",
      "       10629, 10730, 10822, 10949, 11090, 11105, 11225, 11259, 11267,\n",
      "       11304, 11318, 11321, 11394, 11469, 11628, 11842, 11883, 11900,\n",
      "       11943, 11979, 12028, 12056, 12147, 12245, 12497, 12513, 12662,\n",
      "       12908, 12938, 12951, 13014, 13158, 13188, 13203, 13208, 13293,\n",
      "       13351, 13542, 13663, 13691, 13702, 13930, 14063, 14212, 14364,\n",
      "       14459, 14548, 14587, 14644, 14682, 14708, 14934, 14963]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9660.0 y termina en 14193.0\n",
      "La longitud del elemento transponible es de  4533.0\n",
      "El número de activaciones por encima de cero son  71\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0]), array([ 9662,  9781,  9830,  9845,  9899,  9911,  9936, 10029, 10089,\n",
      "       10137, 10148, 10216, 10266, 10363, 10420, 10437, 10470, 10682,\n",
      "       10768, 10808, 10926, 10971, 10982, 11005, 11175, 11214, 11226,\n",
      "       11243, 11504, 11627, 11695, 11749, 11754, 11764, 11777, 11845,\n",
      "       11850, 11908, 11928, 11984, 12096, 12173, 12228, 12240, 12270,\n",
      "       12321, 12374, 12424, 12445, 12526, 12532, 12682, 12717, 12877,\n",
      "       13066, 13109, 13152, 13302, 13485, 13571, 13574, 13673, 13681,\n",
      "       13724, 13794, 13801, 13843, 13892, 14014, 14038, 14157]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9232.0 y termina en 14621.0\n",
      "La longitud del elemento transponible es de  5389.0\n",
      "El número de activaciones por encima de cero son  63\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9298,  9316,  9411,  9584,  9637,  9693,  9877,  9881,  9942,\n",
      "       10004, 10148, 10226, 10472, 10521, 10573, 10688, 10805, 10858,\n",
      "       10938, 11135, 11354, 11402, 11480, 11504, 11615, 11649, 11694,\n",
      "       11711, 11784, 11859, 11887, 11979, 12060, 12080, 12138, 12333,\n",
      "       12418, 12421, 12635, 12713, 12887, 12935, 12980, 13025, 13052,\n",
      "       13103, 13226, 13439, 13461, 13541, 13705, 13887, 13991, 14034,\n",
      "       14163, 14192, 14256, 14290, 14428, 14449, 14467, 14562, 14567]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9625.0 y termina en 14228.0\n",
      "La longitud del elemento transponible es de  4603.0\n",
      "El número de activaciones por encima de cero son  28\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0]), array([ 9627,  9642,  9660,  9878, 10001, 10342, 10435, 10637, 10862,\n",
      "       10945, 11224, 11290, 11476, 11866, 11969, 12664, 12815, 12925,\n",
      "       13277, 13321, 13427, 13451, 13702, 13894, 13933, 14112, 14127,\n",
      "       14145]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9497.0 y termina en 14356.0\n",
      "La longitud del elemento transponible es de  4859.0\n",
      "El número de activaciones por encima de cero son  78\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9582,  9605,  9610,  9624,  9666,  9736,  9767,  9867, 10049,\n",
      "       10098, 10115, 10163, 10176, 10179, 10292, 10364, 10384, 10436,\n",
      "       10544, 10644, 10783, 10789, 10927, 10953, 11137, 11182, 11214,\n",
      "       11227, 11253, 11257, 11261, 11285, 11296, 11323, 11332, 11407,\n",
      "       11493, 11619, 11667, 11681, 11787, 11857, 12092, 12154, 12206,\n",
      "       12228, 12321, 12372, 12396, 12447, 12621, 12673, 12724, 12735,\n",
      "       12838, 12975, 12984, 13032, 13164, 13170, 13185, 13277, 13341,\n",
      "       13411, 13549, 13578, 13614, 13629, 13715, 13741, 13866, 14024,\n",
      "       14178, 14213, 14276, 14281, 14295, 14337]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9590.0 y termina en 14264.0\n",
      "La longitud del elemento transponible es de  4674.0\n",
      "El número de activaciones por encima de cero son  50\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0]), array([ 9617,  9656,  9684,  9866, 10187, 10228, 10428, 10440, 10468,\n",
      "       10524, 10777, 10906, 11111, 11236, 11359, 11476, 11638, 11675,\n",
      "       12007, 12038, 12118, 12128, 12233, 12445, 12494, 12574, 12643,\n",
      "       12703, 12712, 12772, 12911, 13033, 13063, 13072, 13119, 13146,\n",
      "       13239, 13261, 13299, 13336, 13372, 13509, 13525, 13528, 13837,\n",
      "       13887, 14032, 14071, 14099, 14156]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9663.0 y termina en 14191.0\n",
      "La longitud del elemento transponible es de  4528.0\n",
      "El número de activaciones por encima de cero son  41\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9674,  9760,  9775,  9789,  9994, 10124, 10136, 10139, 10297,\n",
      "       10322, 10367, 10681, 10768, 10774, 10855, 11035, 11131, 11316,\n",
      "       11547, 11605, 11632, 12038, 12048, 12468, 12630, 12674, 12695,\n",
      "       12698, 12921, 12960, 13050, 13209, 13277, 13293, 13304, 13322,\n",
      "       13368, 13803, 13824, 14026, 14112]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9540.0 y termina en 14314.0\n",
      "La longitud del elemento transponible es de  4774.0\n",
      "El número de activaciones por encima de cero son  39\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9560,  9567,  9581,  9685,  9712,  9721,  9844, 10067, 10140,\n",
      "       10224, 10322, 10433, 10752, 10797, 10896, 11287, 11311, 11634,\n",
      "       11770, 11818, 12459, 12612, 12712, 13186, 13206, 13254, 13383,\n",
      "       13513, 13585, 13686, 13705, 13806, 13858, 13896, 13995, 14043,\n",
      "       14195, 14202, 14216]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 9290.0 y termina en 14563.0\n",
      "La longitud del elemento transponible es de  5273.0\n",
      "El número de activaciones por encima de cero son  53\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9332,  9456,  9602,  9605, 10007, 10259, 10331, 10524, 10655,\n",
      "       10769, 10819, 10896, 10924, 10947, 11051, 11137, 11214, 11269,\n",
      "       11324, 11331, 11372, 11410, 11469, 11581, 11601, 11683, 11810,\n",
      "       11870, 12130, 12145, 12153, 12325, 12346, 12505, 12698, 12716,\n",
      "       12775, 12803, 12907, 12913, 12947, 13101, 13145, 13153, 13213,\n",
      "       13246, 13650, 13685, 13853, 14026, 14277, 14401, 14542]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "El primer ejemplo empieza en 8977.0 y termina en 14876.0\n",
      "La longitud del elemento transponible es de  5899.0\n",
      "El número de activaciones por encima de cero son  58\n",
      "La primera activacion esta en  (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([ 9007,  9032,  9085,  9143,  9304,  9340,  9359,  9422,  9460,\n",
      "        9575,  9675, 10132, 10150, 10162, 10243, 10472, 10651, 10751,\n",
      "       10870, 11011, 11052, 11085, 11151, 11208, 11242, 11568, 11713,\n",
      "       11767, 11963, 12123, 12340, 12361, 12469, 12595, 12628, 12734,\n",
      "       12871, 12946, 13072, 13123, 13169, 13396, 13477, 13624, 13892,\n",
      "       13960, 14061, 14136, 14215, 14282, 14307, 14360, 14418, 14616,\n",
      "       14698, 14736, 14764, 14851]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "\n",
      "\n",
      "time: 5.4 s (started: 2021-03-20 15:20:18 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def indices(X):\n",
    "  posiciones=np.zeros((X.shape[0],3))\n",
    "  for i in range(0, X.shape[0]):\n",
    "    posiciones[i,2]=np.sum(X[i,0:4,:])\n",
    "    for j in range(0,X.shape[2]):\n",
    "      if np.sum(X[i,0:4,j])==1:\n",
    "        posiciones[i,0] = j\n",
    "        break\n",
    "  for i in range(0, X.shape[0]):\n",
    "    for j in range(int(-posiciones[i,0]+5),int(-posiciones[i,0]-5),-1):\n",
    "      if np.sum(X[i,0:4,j])==1:\n",
    "        posiciones[i,1] = j+1\n",
    "        break\n",
    "  return posiciones\n",
    "\n",
    "def DETECCION_LTR(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2], 1), name=\"input_1\")\n",
    "\n",
    "\n",
    "    # Filtro TATA box:  5'-TATA(A/T)A(A/T)-3'\n",
    "    n_1=7\n",
    "    W_1= np.zeros((5,n_1,1,1))\n",
    "    W_1[3,0,0,0]=1\n",
    "    W_1[0,1,0,0]=1\n",
    "    W_1[3,2,0,0]=1\n",
    "    W_1[0,3,0,0]=1\n",
    "    W_1[0,4,0,0]=1\n",
    "    W_1[3,4,0,0]=1\n",
    "    W_1[0,5,0,0]=1\n",
    "    W_1[0,6,0,0]=1\n",
    "    W_1[3,6,0,0]=1\n",
    "    b_1=-(n_1-1)*np.ones((1,1)).reshape(1,)\n",
    "\n",
    "    # Filtro CAAT box:  3'-TGATTGG(T/C)(T/C)(A/G)-5'\n",
    "    n_2=10\n",
    "    W_2= np.zeros((5,n_2,1,1))\n",
    "    W_2[3,0,0,0]=1\n",
    "    W_2[2,1,0,0]=1\n",
    "    W_2[0,2,0,0]=1\n",
    "    W_2[3,3,0,0]=1\n",
    "    W_2[3,4,0,0]=1\n",
    "    W_2[2,5,0,0]=1\n",
    "    W_2[2,6,0,0]=1\n",
    "    W_2[3,7,0,0]=1\n",
    "    W_2[1,7,0,0]=1\n",
    "    W_2[3,8,0,0]=1\n",
    "    W_2[1,8,0,0]=1\n",
    "    W_2[0,9,0,0]=1\n",
    "    W_2[2,9,0,0]=1\n",
    "    b_2=-(n_2-1)*np.ones((1,1)).reshape(1,)\n",
    "\n",
    "    # Filtro transcription start: CCCATGG\n",
    "    n_3=7\n",
    "    W_3= np.zeros((5,n_3,1,1))\n",
    "    W_3[1,0,0,0]=1\n",
    "    W_3[1,1,0,0]=1\n",
    "    W_3[1,2,0,0]=1\n",
    "    W_3[0,3,0,0]=1\n",
    "    W_3[3,4,0,0]=1\n",
    "    W_3[2,5,0,0]=1\n",
    "    W_3[2,6,0,0]=1\n",
    "    b_3=-(n_3-1)*np.ones((1,1)).reshape(1,)\n",
    "\n",
    "    # Filtro polyadenylation signal: AATAAG\n",
    "    n_4=6\n",
    "    W_4= np.zeros((5,n_4,1,1))\n",
    "    W_4[0,0,0,0]=1\n",
    "    W_4[0,1,0,0]=1\n",
    "    W_4[3,2,0,0]=1\n",
    "    W_4[0,3,0,0]=1\n",
    "    W_4[0,4,0,0]=1\n",
    "    W_4[2,5,0,0]=1\n",
    "    b_4=-(n_4-1)*np.ones((1,1)).reshape(1,)\n",
    "\n",
    "    # Filtro polyadenylation start TAGT\n",
    "    n_5=6\n",
    "    W_5= np.zeros((5,n_5,1,1))\n",
    "    W_5[3,0,0,0]=1\n",
    "    W_5[0,1,0,0]=1\n",
    "    W_5[2,2,0,0]=1\n",
    "    W_5[3,3,0,0]=1\n",
    "    b_5=-(n_5-1)*np.ones((1,1)).reshape(1,)\n",
    "\n",
    "    # Filtro codon de inicio: ATG\n",
    "    n_6=3\n",
    "    W_6= np.zeros((5,n_6,1,1))\n",
    "    W_6[0,0,0,0]=1\n",
    "    W_6[3,1,0,0]=1\n",
    "    W_6[2,2,0,0]=1\n",
    "    b_6=-(n_6-1)*np.ones((1,1)).reshape(1,)\n",
    "\n",
    "    # Filtro codon de parada: TAG\n",
    "    n_7=3\n",
    "    W_7= np.zeros((5,n_7,1,1))\n",
    "    W_7[3,0,0,0]=1\n",
    "    W_7[0,1,0,0]=1\n",
    "    W_7[2,2,0,0]=1\n",
    "    b_7=-(n_7-1)*np.ones((1,1)).reshape(1,)\n",
    "\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv2D(1, (5, n_1), strides=(1,1), weights=[W_1,b_1],activation=fun_act, use_bias=True, name='TATA')(inputs)\n",
    "    layers_2 = tf.keras.layers.Conv2D(1, (5, n_2), strides=(1,1), weights=[W_2,b_2],activation=fun_act, use_bias=True, name='CAAT')(inputs)\n",
    "    layers_3 = tf.keras.layers.Conv2D(1, (5, n_3), strides=(1,1), weights=[W_3,b_3],activation=fun_act, use_bias=True, name='transcription')(inputs)\n",
    "    layers_4 = tf.keras.layers.Conv2D(1, (5, n_4), strides=(1,1), weights=[W_4,b_4],activation=fun_act, use_bias=True, name='polyA_signal')(inputs)\n",
    "    layers_5 = tf.keras.layers.Conv2D(1, (5, n_5), strides=(1,1), weights=[W_5,b_5],activation=fun_act, use_bias=True, name='polyA_start')(inputs)\n",
    "    layers_6 = tf.keras.layers.Conv2D(1, (5, n_6), strides=(1,1), weights=[W_6,b_6],activation=fun_act, use_bias=True, name='ATG')(inputs)\n",
    "    layers_7 = tf.keras.layers.Conv2D(1, (5, n_7), strides=(1,1), weights=[W_7,b_7],activation=fun_act, use_bias=True, name='TAG')(inputs)\n",
    "\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=layers_7)\n",
    "\n",
    "    #opt = optimizador(learning_rate=lr)\n",
    "    # loss function\n",
    "    #loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    # Compile model\n",
    "    #model.compile(loss=loss_fn, optimizer=opt, metrics=['acc', 'AUC', 'mse','mae','mape'])\n",
    "    #model.compile(loss=loss_fn, optimizer=opt)\n",
    "    return model\n",
    "model_name='DETECCION_LTR'\n",
    "model = DETECCION_LTR()\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for k in range(50):\n",
    "  Datos_train=X_train[k:k+1,:,:]\n",
    "  longitud=indices(Datos_train)\n",
    "  A=model.predict(Datos_train,batch_size=20)\n",
    "  termino=X_train.shape[2]+longitud[0,1]\n",
    "  print('El primer ejemplo empieza en {} y termina en {}'.format(longitud[0,0],termino))\n",
    "  print('La longitud del elemento transponible es de ',longitud[0,2])\n",
    "  cantidad=np.sum(A>0)\n",
    "  print('El número de activaciones por encima de cero son ',cantidad)\n",
    "  B=(A>0)\n",
    "  result = np.where(B==1)\n",
    "  print('La primera activacion esta en ',result)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vy9PdFwcvDsb",
    "outputId": "39a76639-863c-4e5c-ecb9-e358cd72c996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 12 01:54:45 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   48C    P0    27W /  70W |   9028MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "awSQ2ZVh8q78",
    "outputId": "26fa3b50-26bc-47c6-d9b5-5e13a13df8f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hacer conteo para 5 millones de cadenas de ADN de longitud de 22576 se tardaría 0.4363654498304408 dias\n",
      "time: 1.71 ms (started: 2021-03-12 19:29:43 +00:00)\n"
     ]
    }
   ],
   "source": [
    "print('Hacer conteo para 5 millones de cadenas de ADN de longitud de 22576 se tardaría',21.2/2785*5000000*(1/3600*1/24),'dias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzwHz_W66Waw",
    "outputId": "90e0a04a-7d52-4838-c2e5-edfbe61034b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  4,  9, 12, 13, 14, 16, 17, 18, 19, 20])"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.06 ms (started: 2021-03-15 03:01:27 +00:00)\n"
     ]
    }
   ],
   "source": [
    "np.unique(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ri5_JgRG1MI_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Prediccion_kmers.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
