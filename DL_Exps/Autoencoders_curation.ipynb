{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZ6fmT2-MEkf"
   },
   "source": [
    "# **1. IMPORTACIÓN DE LIBRERÍAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pjv_21J4t3qQ",
    "outputId": "63b080f7-44a2-4e7a-c49a-4a17d6be4857"
   },
   "outputs": [],
   "source": [
    "from scipy import misc, ndimage, signal                                         # \"scipy\" is one of the core packages that make up the SciPy stack. It provides many user-friendly and efficient numerical routines, such as routines for numerical integration, interpolation, optimization, linear algebra, and statistics.\n",
    "                                                                                # \"misc\" Various utilities that don’t have another home: ascent(), central_diff_weights(), derivative(), face(), electrocardiogram()\n",
    "                                                                                # \"ndimage\" This package contains various functions for multidimensional image processing\n",
    "                                                                                # \"signal\" This package contains various functions for signal processing: convolution, B-splines, filtering, filter design, Matlab-style IIR filter design, Continuous-time linear systems, Discrete-time linear systems, LTI representations, Waveforms, Window functions¶, Wavelets, Peak finding, Spectral analysis.\n",
    "import numpy as np                                                              # \"numpy\" is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more\n",
    "import random                                                                   # \"random\" This module implements pseudo-random number generators for various distributions\n",
    "#import ntpath                                                                   # ----------------------------------¿Para qué sirve?\n",
    "import os                                                                       # \"os\" This module provides a portable way of using operating system dependent functionality. If you just want to read or write a file see open(), if you want to manipulate paths, see the os.path module, and if you want to read all the lines in all the files on the command line see the fileinput module. \n",
    "import pandas as pd                                                             # \"pandas\" is a Python package that provides fast, flexible, and expressive data structures designed to make working with structured (tabular, multidimensional, potentially heterogeneous) and time series data both easy and intuitive\n",
    "import matplotlib as mpl                                                        # \"matplotlib\" is a comprehensive library for creating static, animated, and interactive visualizations in Python\n",
    "import matplotlib.pyplot as plt                                                 # \"pyplot\" is a collection of functions that make matplotlib work like MATLAB\n",
    "import matplotlib.colors as colors                                              # \"colors\" A module for converting numbers or color arguments to RGB or RGBA\n",
    "import tensorflow as tf                                                         # \"tensorflow\" TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications\n",
    "                                                                                # \"keras\" Keras is a deep learning API (application programming interface) written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result as fast as possible is key to doing good research.\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import optimizers                                         # \"optimizers\" is a container of functions -----------¿De qué trata?\n",
    "from tensorflow.keras import regularizers                                       # \"regularizers\" is a container of functions -----------¿De qué trata?\n",
    "from tensorflow.keras import backend as K                                       # \"backend\" is a container of functions -----------¿De qué trata?\n",
    "from tensorflow.keras import datasets,layers,models,Input,Model\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, Adagrad, SGD, Adadelta, Adamax, Nadam\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, AveragePooling2D, Cropping2D\n",
    "from tensorflow.keras.layers import Dropout, Activation, Flatten, Concatenate, Dense, Reshape, Add, PReLU, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.activations import relu\n",
    "import time as tm                                                               # \"time\" This module provides various time-related functions\n",
    "from time import time                                                           # \"time.time\" Return the time in seconds since the epoch as a floating point number. On Windows and most Unix systems, the epoch is January 1, 1970, 00:00:00 (UTC).\n",
    "import datetime                                                                 # \"datetime\" supplies classes for manipulating dates and times\n",
    "from operator import itemgetter                                                 # \"operator\" exports a set of efficient functions that fall into categories that perform object comparisons, logical operations, mathematical operations and sequence operations.\n",
    "                                                                                # \"itemgetter\" is a container of functions -----------¿De qué trata?\n",
    "import glob                                                                     # \"glob\" is used to retrieve files/pathnames matching a specified pattern.\n",
    "import tensorflow.keras.utils                                                   # \"utils\" is a container of functions -----------¿De qué trata?\n",
    "from tensorflow.keras.utils import to_categorical                               # \"to_categorical\" Converts a class vector (integers) to binary class matrix.\n",
    "from numpy import argmax                                                        # \"argmax\" Returns the indices of the maximum values along an axis.\n",
    "import seaborn as sn                                                            # \"seaborn\" Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "from sklearn.model_selection  import train_test_split                           # \"sklearn\" features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n",
    "                                                                                # \"train_test_split\" splits arrays or matrices into random train and test subsets\n",
    "from sklearn.metrics import confusion_matrix                                    # \"confusion_matrix\" Compute confusion matrix to evaluate the accuracy of a classification.\n",
    "from sklearn.metrics import accuracy_score                                      # \"accuracy_score\" In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\n",
    "from sklearn.metrics import f1_score                                            # \"f1_score\" Compute the F1 score, also known as balanced F-score or F-measure\n",
    "from sklearn.metrics import recall_score                                        # \"recall_score\" The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The best value is 1 and the worst value is 0.\n",
    "from sklearn.metrics import precision_score                                     # \"precision_Score\" The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1 and the worst value is 0.\n",
    "from sklearn.metrics import classification_report                               # \"classification_report\" Build a text report showing the main classification metrics\n",
    "from tensorflow.keras import layers                                             # \"layers\" is a container of functions -----------¿De qué trata?\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import cv2\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#% cd /content/drive/MyDrive/AUTOENCODER/\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPOQe6gPqiMu"
   },
   "source": [
    "#**2. PRUEBA EXPLORATORIA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSVi08zlqwke"
   },
   "source": [
    "## **2.1 TRATAMIENTO DE IMAGENES CON DIGITOS MANUSCRITOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yf88-Jk4NFkE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nopcion='fully'\\nnoise_factor = 0.5\\n(x_train, _), (x_test, _) = mnist.load_data()\\nx_train = x_train.astype('float32') / 255.\\nx_test = x_test.astype('float32') / 255.\\nx_train = x_train[0:int(len(x_train)/12)]\\nx_test = x_test[0:int(len(x_train)/12)]\\n\\nx_train_FC = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\\nx_test_FC = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\\nx_train_noisy_FC = x_train_FC + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train_FC.shape) \\nx_test_noisy_FC = x_test_FC + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test_FC.shape) \\nx_train_noisy_FC = np.clip(x_train_noisy_FC, 0., 1.)\\nx_test_noisy_FC = np.clip(x_test_noisy_FC, 0., 1.)\\n\\n\\nx_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\\nx_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \\nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "opcion='fully'\n",
    "noise_factor = 0.5\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train[0:int(len(x_train)/12)]\n",
    "x_test = x_test[0:int(len(x_train)/12)]\n",
    "\n",
    "x_train_FC = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test_FC = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "x_train_noisy_FC = x_train_FC + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train_FC.shape) \n",
    "x_test_noisy_FC = x_test_FC + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test_FC.shape) \n",
    "x_train_noisy_FC = np.clip(x_train_noisy_FC, 0., 1.)\n",
    "x_test_noisy_FC = np.clip(x_test_noisy_FC, 0., 1.)\n",
    "\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26I_cRB_q88G"
   },
   "source": [
    "## **2.2 MODELOS SACADOS DE LITERATURA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vnIpZbUe6FTV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef Autoencoder_denoising_fully(optimizador=Adam,lr=0.001,momen=0,init_mode=\\'glorot_uniform\\',fun_act=\\'elu\\',dp=0.2,regularizer=l2,w_reg=0.001):\\n    tf.keras.backend.clear_session()\\n\\n    inputs = tf.keras.Input(shape=(784), name=\"input_1\")\\n    layers = tf.keras.layers.Dense(128, activation=fun_act)(inputs)\\n    layers = tf.keras.layers.Dense(64, activation=fun_act)(layers)\\n\\n    layers = tf.keras.layers.Dense(32, activation=fun_act, name=\"Bottleneck\")(layers)\\n\\n    layers = tf.keras.layers.Dense(64, activation=fun_act)(layers)\\n    layers = tf.keras.layers.Dense(128, activation=fun_act)(layers)\\n    outputs = tf.keras.layers.Dense(784, activation=fun_act, name=\"outputs\")(layers)\\n\\n    model = tf.keras.Model(inputs = inputs, outputs=outputs)\\n    opt = optimizador(learning_rate=lr)\\n    loss_fn = tf.keras.losses.binary_crossentropy\\n    model.compile(loss=loss_fn, optimizer=opt)\\n    return model\\ndef Autoencoder_denoising_conv(optimizador=Adam,lr=0.001,momen=0,init_mode=\\'glorot_uniform\\',fun_act=\\'relu\\',dp=0.2,regularizer=l2,w_reg=0):\\n    tf.keras.backend.clear_session()\\n\\n    # Inputs\\n    inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\\n    # layer 1\\n    layers = tf.keras.layers.Conv2D(32, (3, 3), padding=\\'same\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\\n    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\\n\\n    # layer 2\\n    layers = tf.keras.layers.Conv2D(32, (3, 3), padding=\\'same\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\\n\\n    # layer 3\\n    layers = tf.keras.layers.Conv2D(32, (3, 3), padding=\\'same\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\\n\\n    # layer 4\\n    layers = tf.keras.layers.Conv2D(32, (3, 3), padding=\\'same\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\\n\\n    predictions = tf.keras.layers.Conv2D(1,(3,3), activation=\"sigmoid\",padding=\\'same\\', name=\"output_1\")(layers)\\n    \\n    # model generation\\n    model = tf.keras.Model(inputs = inputs, outputs=predictions)\\n    # optimizer\\n    #opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\\n    opt = optimizador(learning_rate=lr)\\n    # loss function\\n    loss_fn = tf.keras.losses.binary_crossentropy\\n    # Compile model\\n    #model.compile(loss=loss_fn, optimizer=opt, metrics=[\\'acc\\', \\'AUC\\', \\'mse\\',\\'mae\\',\\'mape\\'])\\n    model.compile(loss=loss_fn, optimizer=opt)\\n    return model\\ndef Autoencoder_Art01(optimizador=Adam,lr=0.001,momen=0,init_mode=\\'glorot_uniform\\',fun_act=\\'relu\\',dp=0.2,regularizer=l2,w_reg=0):\\n    tf.keras.backend.clear_session()\\n\\n    # Inputs\\n    inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\\n    # layer 1\\n    layers = tf.keras.layers.Conv2D(50, (5, 5), padding=\\'valid\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\\n    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\\n\\n    # layer 2\\n    layers = tf.keras.layers.Conv2D(100, (2, 2), padding=\\'valid\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\\n\\n    # layer 3\\n    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\\n    layers = tf.keras.layers.Conv2DTranspose(100, (2, 2), padding=\\'valid\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    \\n\\n    # layer 4\\n    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\\n    layers = tf.keras.layers.Conv2DTranspose(50, (5, 5), padding=\\'valid\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    \\n\\n    predictions = tf.keras.layers.Conv2DTranspose(1,(3,3), activation=\"sigmoid\",padding=\\'valid\\', name=\"output_1\")(layers)\\n    \\n    # model generation\\n    model = tf.keras.Model(inputs = inputs, outputs=predictions)\\n    # optimizer\\n    #opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\\n    opt = optimizador(learning_rate=lr)\\n    # loss function\\n    loss_fn = tf.keras.losses.binary_crossentropy\\n    # Compile model\\n    #model.compile(loss=loss_fn, optimizer=opt, metrics=[\\'acc\\', \\'AUC\\', \\'mse\\',\\'mae\\',\\'mape\\'])\\n    model.compile(loss=loss_fn, optimizer=opt)\\n    return model\\n\\ndef Autoencoder_Art02(optimizador=Adam,lr=0.001,momen=0,init_mode=\\'glorot_uniform\\',fun_act=\\'relu\\',dp=0.2,regularizer=l2,w_reg=0):\\n    tf.keras.backend.clear_session()\\n\\n    # Inputs\\n    inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\\n    # layer 1\\n    layers = tf.keras.layers.Conv2D(64, (3, 3), padding=\\'same\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\\n    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\\n\\n    # layer 2\\n    layers = tf.keras.layers.Conv2D(64, (3, 3), padding=\\'same\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\\n\\n    # layer 3\\n    layers = tf.keras.layers.Conv2D(64, (3, 3), padding=\\'same\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\\n\\n    #layer 4\\n    layers = tf.keras.layers.Conv2D(64, (3, 3), padding=\\'same\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\\n\\n    predictions = tf.keras.layers.Conv2DTranspose(1,(3,3), activation=\"sigmoid\",padding=\\'same\\', name=\"output_1\")(layers)\\n    \\n    # model generation\\n    model = tf.keras.Model(inputs = inputs, outputs=predictions)\\n    # optimizer\\n    #opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\\n    opt = optimizador(learning_rate=lr)\\n    # loss function\\n    loss_fn = tf.keras.losses.binary_crossentropy\\n    # Compile model\\n    #model.compile(loss=loss_fn, optimizer=opt, metrics=[\\'acc\\', \\'AUC\\', \\'mse\\',\\'mae\\',\\'mape\\'])\\n    model.compile(loss=loss_fn, optimizer=opt)\\n    return model\\ndef Autoencoder_Art03(optimizador=Adam,lr=0.001,momen=0,init_mode=\\'glorot_uniform\\',fun_act=\\'relu\\',dp=0.2,regularizer=l2,w_reg=0):\\n    tf.keras.backend.clear_session()\\n\\n    # Inputs\\n    inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\\n    # layer 1\\n    layers = tf.keras.layers.Conv2D(128, (3, 3), padding=\\'same\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\\n    layers =tf.keras.layers.BatchNormalization()(layers)\\n    layers =tf.keras.layers.Activation(tf.nn.relu)(layers)\\n    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\\n\\n    # layer 2\\n    layers = tf.keras.layers.Conv2D(256, (3, 3), padding=\\'same\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    layers =tf.keras.layers.BatchNormalization()(layers)\\n    layers =tf.keras.layers.Activation(tf.nn.relu)(layers)\\n    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\\n\\n    # layer 3\\n    layers = tf.keras.layers.Conv2D(512, (3, 3), padding=\\'same\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    layers =tf.keras.layers.BatchNormalization()(layers)\\n    layers =tf.keras.layers.Activation(tf.nn.relu)(layers)\\n\\n    #layer 4\\n    layers = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2,2), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    layers =tf.keras.layers.BatchNormalization()(layers)\\n    layers = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2,2), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    layers =tf.keras.layers.BatchNormalization()(layers)\\n    layers = tf.keras.layers.Conv2D(1, (3, 3), padding=\\'same\\',strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    layers =tf.keras.layers.BatchNormalization()(layers)\\n    predictions =tf.keras.layers.Activation(tf.nn.relu)(layers)\\n    \\n    # model generation\\n    model = tf.keras.Model(inputs = inputs, outputs=predictions)\\n    # optimizer\\n    #opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\\n    opt = optimizador(learning_rate=lr)\\n    # loss function\\n    loss_fn = tf.keras.losses.binary_crossentropy\\n    # Compile model\\n    #model.compile(loss=loss_fn, optimizer=opt, metrics=[\\'acc\\', \\'AUC\\', \\'mse\\',\\'mae\\',\\'mape\\'])\\n    model.compile(loss=loss_fn, optimizer=opt)\\n    return model\\ndef Autoencoder_Art04(optimizador=Adam,lr=0.001,momen=0,init_mode=\\'glorot_uniform\\',fun_act=\\'relu\\',dp=0.2,regularizer=l2,w_reg=0):\\n    tf.keras.backend.clear_session()\\n\\n    # Inputs\\n    inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\\n    # layer 1\\n    layers = tf.keras.layers.Conv2D(32, (3, 3), padding=\\'same\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\\n    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\\n\\n    # layer 2\\n    layers = tf.keras.layers.Conv2D(32, (3, 3), padding=\\'same\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\\n\\n    # layer 3\\n    layers = tf.keras.layers.Conv2D(32, (3, 3), padding=\\'same\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\\n\\n    layers = tf.keras.layers.Conv2D(32, (3, 3), padding=\\'same\\', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer=\\'zeros\\', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\\n    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\\n\\n    predictions = tf.keras.layers.Conv2D(1,(3,3), activation=\"sigmoid\",padding=\\'same\\', name=\"output_1\")(layers)\\n\\n\\n    # model generation\\n    model = tf.keras.Model(inputs = inputs, outputs=predictions)\\n    # optimizer\\n    #opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\\n    opt = optimizador(learning_rate=lr)\\n    # loss function\\n    loss_fn = tf.keras.losses.binary_crossentropy\\n    # Compile model\\n    #model.compile(loss=loss_fn, optimizer=opt, metrics=[\\'acc\\', \\'AUC\\', \\'mse\\',\\'mae\\',\\'mape\\'])\\n    model.compile(loss=loss_fn, optimizer=opt)\\n    return model\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "def Autoencoder_denoising_fully(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='elu',dp=0.2,regularizer=l2,w_reg=0.001):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(784), name=\"input_1\")\n",
    "    layers = tf.keras.layers.Dense(128, activation=fun_act)(inputs)\n",
    "    layers = tf.keras.layers.Dense(64, activation=fun_act)(layers)\n",
    "\n",
    "    layers = tf.keras.layers.Dense(32, activation=fun_act, name=\"Bottleneck\")(layers)\n",
    "\n",
    "    layers = tf.keras.layers.Dense(64, activation=fun_act)(layers)\n",
    "    layers = tf.keras.layers.Dense(128, activation=fun_act)(layers)\n",
    "    outputs = tf.keras.layers.Dense(784, activation=fun_act, name=\"outputs\")(layers)\n",
    "\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=outputs)\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    loss_fn = tf.keras.losses.binary_crossentropy\n",
    "    model.compile(loss=loss_fn, optimizer=opt)\n",
    "    return model\n",
    "def Autoencoder_denoising_conv(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\n",
    "    # layer 1\n",
    "    layers = tf.keras.layers.Conv2D(32, (3, 3), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\n",
    "\n",
    "    # layer 2\n",
    "    layers = tf.keras.layers.Conv2D(32, (3, 3), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\n",
    "\n",
    "    # layer 3\n",
    "    layers = tf.keras.layers.Conv2D(32, (3, 3), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\n",
    "\n",
    "    # layer 4\n",
    "    layers = tf.keras.layers.Conv2D(32, (3, 3), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\n",
    "\n",
    "    predictions = tf.keras.layers.Conv2D(1,(3,3), activation=\"sigmoid\",padding='same', name=\"output_1\")(layers)\n",
    "    \n",
    "    # model generation\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    # optimizer\n",
    "    #opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    # loss function\n",
    "    loss_fn = tf.keras.losses.binary_crossentropy\n",
    "    # Compile model\n",
    "    #model.compile(loss=loss_fn, optimizer=opt, metrics=['acc', 'AUC', 'mse','mae','mape'])\n",
    "    model.compile(loss=loss_fn, optimizer=opt)\n",
    "    return model\n",
    "def Autoencoder_Art01(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\n",
    "    # layer 1\n",
    "    layers = tf.keras.layers.Conv2D(50, (5, 5), padding='valid', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\n",
    "\n",
    "    # layer 2\n",
    "    layers = tf.keras.layers.Conv2D(100, (2, 2), padding='valid', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\n",
    "\n",
    "    # layer 3\n",
    "    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\n",
    "    layers = tf.keras.layers.Conv2DTranspose(100, (2, 2), padding='valid', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    \n",
    "\n",
    "    # layer 4\n",
    "    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\n",
    "    layers = tf.keras.layers.Conv2DTranspose(50, (5, 5), padding='valid', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    \n",
    "\n",
    "    predictions = tf.keras.layers.Conv2DTranspose(1,(3,3), activation=\"sigmoid\",padding='valid', name=\"output_1\")(layers)\n",
    "    \n",
    "    # model generation\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    # optimizer\n",
    "    #opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    # loss function\n",
    "    loss_fn = tf.keras.losses.binary_crossentropy\n",
    "    # Compile model\n",
    "    #model.compile(loss=loss_fn, optimizer=opt, metrics=['acc', 'AUC', 'mse','mae','mape'])\n",
    "    model.compile(loss=loss_fn, optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def Autoencoder_Art02(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\n",
    "    # layer 1\n",
    "    layers = tf.keras.layers.Conv2D(64, (3, 3), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\n",
    "\n",
    "    # layer 2\n",
    "    layers = tf.keras.layers.Conv2D(64, (3, 3), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\n",
    "\n",
    "    # layer 3\n",
    "    layers = tf.keras.layers.Conv2D(64, (3, 3), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\n",
    "\n",
    "    #layer 4\n",
    "    layers = tf.keras.layers.Conv2D(64, (3, 3), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\n",
    "\n",
    "    predictions = tf.keras.layers.Conv2DTranspose(1,(3,3), activation=\"sigmoid\",padding='same', name=\"output_1\")(layers)\n",
    "    \n",
    "    # model generation\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    # optimizer\n",
    "    #opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    # loss function\n",
    "    loss_fn = tf.keras.losses.binary_crossentropy\n",
    "    # Compile model\n",
    "    #model.compile(loss=loss_fn, optimizer=opt, metrics=['acc', 'AUC', 'mse','mae','mape'])\n",
    "    model.compile(loss=loss_fn, optimizer=opt)\n",
    "    return model\n",
    "def Autoencoder_Art03(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\n",
    "    # layer 1\n",
    "    layers = tf.keras.layers.Conv2D(128, (3, 3), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers =tf.keras.layers.BatchNormalization()(layers)\n",
    "    layers =tf.keras.layers.Activation(tf.nn.relu)(layers)\n",
    "    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\n",
    "\n",
    "    # layer 2\n",
    "    layers = tf.keras.layers.Conv2D(256, (3, 3), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers =tf.keras.layers.BatchNormalization()(layers)\n",
    "    layers =tf.keras.layers.Activation(tf.nn.relu)(layers)\n",
    "    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\n",
    "\n",
    "    # layer 3\n",
    "    layers = tf.keras.layers.Conv2D(512, (3, 3), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers =tf.keras.layers.BatchNormalization()(layers)\n",
    "    layers =tf.keras.layers.Activation(tf.nn.relu)(layers)\n",
    "\n",
    "    #layer 4\n",
    "    layers = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2,2), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers =tf.keras.layers.BatchNormalization()(layers)\n",
    "    layers = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2,2), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers =tf.keras.layers.BatchNormalization()(layers)\n",
    "    layers = tf.keras.layers.Conv2D(1, (3, 3), padding='same',strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers =tf.keras.layers.BatchNormalization()(layers)\n",
    "    predictions =tf.keras.layers.Activation(tf.nn.relu)(layers)\n",
    "    \n",
    "    # model generation\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    # optimizer\n",
    "    #opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    # loss function\n",
    "    loss_fn = tf.keras.losses.binary_crossentropy\n",
    "    # Compile model\n",
    "    #model.compile(loss=loss_fn, optimizer=opt, metrics=['acc', 'AUC', 'mse','mae','mape'])\n",
    "    model.compile(loss=loss_fn, optimizer=opt)\n",
    "    return model\n",
    "def Autoencoder_Art04(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\n",
    "    # layer 1\n",
    "    layers = tf.keras.layers.Conv2D(32, (3, 3), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\n",
    "\n",
    "    # layer 2\n",
    "    layers = tf.keras.layers.Conv2D(32, (3, 3), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.MaxPooling2D((2, 2))(layers)\n",
    "\n",
    "    # layer 3\n",
    "    layers = tf.keras.layers.Conv2D(32, (3, 3), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\n",
    "\n",
    "    layers = tf.keras.layers.Conv2D(32, (3, 3), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.UpSampling2D((2, 2))(layers)\n",
    "\n",
    "    predictions = tf.keras.layers.Conv2D(1,(3,3), activation=\"sigmoid\",padding='same', name=\"output_1\")(layers)\n",
    "\n",
    "\n",
    "    # model generation\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    # optimizer\n",
    "    #opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    # loss function\n",
    "    loss_fn = tf.keras.losses.binary_crossentropy\n",
    "    # Compile model\n",
    "    #model.compile(loss=loss_fn, optimizer=opt, metrics=['acc', 'AUC', 'mse','mae','mape'])\n",
    "    model.compile(loss=loss_fn, optimizer=opt)\n",
    "    return model\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hzwp5tw2EQrO"
   },
   "source": [
    "## **2.3 COMPARACIÓN DE ARQUITECTURAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3ZgHZrtyEPzZ",
    "outputId": "bef82652-297f-46ed-8fe0-7a49b264c9ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint('FULLY CONNECTED')\\nmodel = Autoencoder_denoising_fully()\\nprint(model.summary())\\nmodel.fit(x_train_noisy_FC, x_train_FC, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test_noisy_FC, x_test_FC))\\ndecoded_imgs = model.predict(x_train_noisy_FC)\\n\\nn = 10  # How many digits we will display\\nplt.figure(figsize=(20, 4))\\nfor i in range(n):\\n    # Display original\\n    ax = plt.subplot(2, n, i + 1)\\n    plt.imshow(x_train_noisy_FC[i].reshape(28, 28))\\n    plt.gray()\\n    ax.get_xaxis().set_visible(False)\\n    ax.get_yaxis().set_visible(False)\\n\\n    # Display reconstruction\\n    ax = plt.subplot(2, n, i + 1 + n)\\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\\n    plt.gray()\\n    ax.get_xaxis().set_visible(False)\\n    ax.get_yaxis().set_visible(False)\\nplt.show()\\n\\nprint('FULLY CONVOLUTIONAL')\\nmodel = Autoencoder_denoising_conv()\\nprint(model.summary())\\nmodel.fit(x_train_noisy, x_train, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test_noisy, x_test))\\ndecoded_imgs = model.predict(x_train_noisy)\\n\\nn = 10  # How many digits we will display\\nplt.figure(figsize=(20, 4))\\nfor i in range(n):\\n    # Display original\\n    ax = plt.subplot(2, n, i + 1)\\n    plt.imshow(x_train_noisy[i].reshape(28, 28))\\n    plt.gray()\\n    ax.get_xaxis().set_visible(False)\\n    ax.get_yaxis().set_visible(False)\\n\\n    # Display reconstruction\\n    ax = plt.subplot(2, n, i + 1 + n)\\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\\n    plt.gray()\\n    ax.get_xaxis().set_visible(False)\\n    ax.get_yaxis().set_visible(False)\\nplt.show()\\n\\n\\n\\nprint('ARTICULO 01')\\nmodel = Autoencoder_Art01()\\nprint(model.summary())\\nmodel.fit(x_train_noisy, x_train, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test_noisy, x_test))\\ndecoded_imgs = model.predict(x_train_noisy)\\n\\nn = 10  # How many digits we will display\\nplt.figure(figsize=(20, 4))\\nfor i in range(n):\\n    # Display original\\n    ax = plt.subplot(2, n, i + 1)\\n    plt.imshow(x_train_noisy[i].reshape(28, 28))\\n    plt.gray()\\n    ax.get_xaxis().set_visible(False)\\n    ax.get_yaxis().set_visible(False)\\n\\n    # Display reconstruction\\n    ax = plt.subplot(2, n, i + 1 + n)\\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\\n    plt.gray()\\n    ax.get_xaxis().set_visible(False)\\n    ax.get_yaxis().set_visible(False)\\nplt.show()\\n\\n\\nprint('ARTICULO 02')\\nmodel = Autoencoder_Art02()\\nprint(model.summary())\\nmodel.fit(x_train_noisy, x_train, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test_noisy, x_test))\\ndecoded_imgs = model.predict(x_train_noisy)\\n\\nn = 10  # How many digits we will display\\nplt.figure(figsize=(20, 4))\\nfor i in range(n):\\n    # Display original\\n    ax = plt.subplot(2, n, i + 1)\\n    plt.imshow(x_train_noisy[i].reshape(28, 28))\\n    plt.gray()\\n    ax.get_xaxis().set_visible(False)\\n    ax.get_yaxis().set_visible(False)\\n\\n    # Display reconstruction\\n    ax = plt.subplot(2, n, i + 1 + n)\\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\\n    plt.gray()\\n    ax.get_xaxis().set_visible(False)\\n    ax.get_yaxis().set_visible(False)\\nplt.show()\\n\\n\\nprint('ARTICULO 03')\\nmodel = Autoencoder_Art03()\\nprint(model.summary())\\nmodel.fit(x_train_noisy, x_train, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test_noisy, x_test))\\ndecoded_imgs = model.predict(x_train_noisy)\\n\\nn = 10  # How many digits we will display\\nplt.figure(figsize=(20, 4))\\nfor i in range(n):\\n    # Display original\\n    ax = plt.subplot(2, n, i + 1)\\n    plt.imshow(x_train_noisy[i].reshape(28, 28))\\n    plt.gray()\\n    ax.get_xaxis().set_visible(False)\\n    ax.get_yaxis().set_visible(False)\\n\\n    # Display reconstruction\\n    ax = plt.subplot(2, n, i + 1 + n)\\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\\n    plt.gray()\\n    ax.get_xaxis().set_visible(False)\\n    ax.get_yaxis().set_visible(False)\\nplt.show()\\n\\n\\nprint('ARTICULO 04')\\nmodel = Autoencoder_Art04()\\nprint(model.summary())\\nmodel.fit(x_train_noisy, x_train, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test_noisy, x_test))\\ndecoded_imgs = model.predict(x_train_noisy)\\n\\nn = 10  # How many digits we will display\\nplt.figure(figsize=(20, 4))\\nfor i in range(n):\\n    # Display original\\n    ax = plt.subplot(2, n, i + 1)\\n    plt.imshow(x_train_noisy[i].reshape(28, 28))\\n    plt.gray()\\n    ax.get_xaxis().set_visible(False)\\n    ax.get_yaxis().set_visible(False)\\n\\n    # Display reconstruction\\n    ax = plt.subplot(2, n, i + 1 + n)\\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\\n    plt.gray()\\n    ax.get_xaxis().set_visible(False)\\n    ax.get_yaxis().set_visible(False)\\nplt.show()\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print('FULLY CONNECTED')\n",
    "model = Autoencoder_denoising_fully()\n",
    "print(model.summary())\n",
    "model.fit(x_train_noisy_FC, x_train_FC, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test_noisy_FC, x_test_FC))\n",
    "decoded_imgs = model.predict(x_train_noisy_FC)\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train_noisy_FC[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "print('FULLY CONVOLUTIONAL')\n",
    "model = Autoencoder_denoising_conv()\n",
    "print(model.summary())\n",
    "model.fit(x_train_noisy, x_train, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test_noisy, x_test))\n",
    "decoded_imgs = model.predict(x_train_noisy)\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print('ARTICULO 01')\n",
    "model = Autoencoder_Art01()\n",
    "print(model.summary())\n",
    "model.fit(x_train_noisy, x_train, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test_noisy, x_test))\n",
    "decoded_imgs = model.predict(x_train_noisy)\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('ARTICULO 02')\n",
    "model = Autoencoder_Art02()\n",
    "print(model.summary())\n",
    "model.fit(x_train_noisy, x_train, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test_noisy, x_test))\n",
    "decoded_imgs = model.predict(x_train_noisy)\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('ARTICULO 03')\n",
    "model = Autoencoder_Art03()\n",
    "print(model.summary())\n",
    "model.fit(x_train_noisy, x_train, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test_noisy, x_test))\n",
    "decoded_imgs = model.predict(x_train_noisy)\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('ARTICULO 04')\n",
    "model = Autoencoder_Art04()\n",
    "print(model.summary())\n",
    "model.fit(x_train_noisy, x_train, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test_noisy, x_test))\n",
    "decoded_imgs = model.predict(x_train_noisy)\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_train_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvmddLkbVghI"
   },
   "source": [
    "#**3. APLICACIÓN A LAS CADENAS DE ADN CON RUIDO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD5FTuXrFBhS"
   },
   "source": [
    "## **3.1 TRATAMIENTO DE DATOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43OvH5P3skd8"
   },
   "source": [
    "### **3.1.1 USANDO 100 DATOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GdN_wfZpVk-A",
    "outputId": "bc8ffd2b-9d0d-47ea-8882-28e8c9859448"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nX = np.load('auto_curation_class0.fasta.filtered_center.npy')\\nY = np.load('auto_curation_class0.fasta.filtered_center_labels.npy')\\nX=X[0:100,:,:]\\nY=Y[0:100,:]\\nprint(X.shape)\\nprint(Y.shape)\\nvalidation_size = 0.2\\nseed = 7\\nX_train, X_test_dev, Y_train, Y_test_dev = train_test_split(X, Y, test_size=validation_size, random_state=seed)\\n\\nX_dev, X_test, Y_dev, Y_test = train_test_split(X_test_dev, Y_test_dev, test_size=0.5, random_state=seed)\\n\\nprint(X_train.shape)\\nprint(Y_train.shape)\\nprint(X_dev.shape)\\nprint(Y_dev.shape)\\nprint(X_test.shape)\\nprint(Y_test.shape)\\nX = None\\nY = None\\n\\nX_test_dev = None\\nY_test_dev = None\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X = np.load('auto_curation_class0.fasta.filtered_center.npy')\n",
    "Y = np.load('auto_curation_class0.fasta.filtered_center_labels.npy')\n",
    "X=X[0:100,:,:]\n",
    "Y=Y[0:100,:]\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "X_train, X_test_dev, Y_train, Y_test_dev = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_test_dev, Y_test_dev, test_size=0.5, random_state=seed)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(Y_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "X = None\n",
    "Y = None\n",
    "\n",
    "X_test_dev = None\n",
    "Y_test_dev = None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9FLOuV8srjT"
   },
   "source": [
    "### **3.1.2 USANDO 1000 DATOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1-c3hDXzsyqn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nX = np.load('auto_curation_class0.fasta.filtered_center.npy')\\nY = np.load('auto_curation_class0.fasta.filtered_center_labels.npy')\\nX=X[0:1000,:,:]\\nY=Y[0:1000,:]\\nprint(X.shape)\\nprint(Y.shape)\\nvalidation_size = 0.2\\nseed = 7\\nX_train, X_test_dev, Y_train, Y_test_dev = train_test_split(X, Y, test_size=validation_size, random_state=seed)\\n\\nX_dev, X_test, Y_dev, Y_test = train_test_split(X_test_dev, Y_test_dev, test_size=0.5, random_state=seed)\\n\\nprint(X_train.shape)\\nprint(Y_train.shape)\\nprint(X_dev.shape)\\nprint(Y_dev.shape)\\nprint(X_test.shape)\\nprint(Y_test.shape)\\nX = None\\nY = None\\n\\nX_test_dev = None\\nY_test_dev = None\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X = np.load('auto_curation_class0.fasta.filtered_center.npy')\n",
    "Y = np.load('auto_curation_class0.fasta.filtered_center_labels.npy')\n",
    "X=X[0:1000,:,:]\n",
    "Y=Y[0:1000,:]\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "X_train, X_test_dev, Y_train, Y_test_dev = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_test_dev, Y_test_dev, test_size=0.5, random_state=seed)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(Y_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "X = None\n",
    "Y = None\n",
    "\n",
    "X_test_dev = None\n",
    "Y_test_dev = None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYdwP7u0sy5u"
   },
   "source": [
    "### **3.1.3 USANDO TODOS LOS DATOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5wn2z8Bts2bm",
    "outputId": "34ec4295-a155-4419-d072-84c5132988a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56442, 5, 23854)\n",
      "(56442, 1)\n",
      "(45153, 5, 23854)\n",
      "(45153, 1)\n",
      "(5644, 5, 23854)\n",
      "(5644, 1)\n",
      "(5645, 5, 23854)\n",
      "(5645, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.load('auto_curation_class0.fasta.filtered_center.npy')\n",
    "Y = np.load('auto_curation_class0.fasta.filtered_center_labels.npy')\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "X_train, X_test_dev, Y_train, Y_test_dev = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_test_dev, Y_test_dev, test_size=0.5, random_state=seed)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(Y_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "X = None\n",
    "Y = None\n",
    "\n",
    "X_test_dev = None\n",
    "Y_test_dev = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mbln8RgFKiJ"
   },
   "source": [
    "## **3.2 DEFINICIÓN DE MODELOS Y MÉTRICA DE COMPARACIÓN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FA9vzUEQV3KD"
   },
   "outputs": [],
   "source": [
    "def Autoencoder_Art02_ADN(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(5, 23854, 1), name=\"inputs\")\n",
    "    # layer 1\n",
    "    layers = tf.keras.layers.Conv2D(16, (1, 15), padding='valid', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers = tf.keras.layers.AveragePooling2D((1, 20))(layers)\n",
    "\n",
    "    # layer 2\n",
    "    layers = tf.keras.layers.Conv2D(16, (1, 4), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.AveragePooling2D((1, 8))(layers)\n",
    "\n",
    "    # layer 3\n",
    "    layers = tf.keras.layers.Conv2D(16, (1, 20), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.UpSampling2D((1, 8))(layers)\n",
    "\n",
    "    #layer 4\n",
    "    layers = tf.keras.layers.Conv2D(16, (1, 20), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.UpSampling2D((1, 20))(layers)\n",
    "\n",
    "    #Output layer\n",
    "    predictions = tf.keras.layers.Conv2DTranspose(1,(1,15), activation=\"sigmoid\",padding='valid', name=\"output_1\")(layers)\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    loss_fn = tf.keras.losses.binary_crossentropy\n",
    "    model.compile(loss=loss_fn, optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def Autoencoder_Art01_ADN(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(5, 23854, 1), name=\"inputs\")\n",
    "    # layer 1\n",
    "    layers = tf.keras.layers.Conv2D(16, (5, 17), padding='valid', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers = tf.keras.layers.AveragePooling2D((1, 29))(layers)\n",
    "\n",
    "    # layer 2\n",
    "    layers = tf.keras.layers.Conv2D(16, (1, 17), padding='valid', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.AveragePooling2D((1, 26))(layers)\n",
    "\n",
    "    # layer 3\n",
    "    layers = tf.keras.layers.UpSampling2D((1, 26))(layers)\n",
    "    layers = tf.keras.layers.Conv2DTranspose(16, (1, 17), padding='valid', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    \n",
    "\n",
    "    #layer 4\n",
    "    layers = tf.keras.layers.UpSampling2D((1, 29))(layers)\n",
    "\n",
    "    predictions = tf.keras.layers.Conv2DTranspose(1,(5,17), activation=\"sigmoid\",padding='valid', name=\"output_1\")(layers)\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    loss_fn = tf.keras.losses.binary_crossentropy\n",
    "    model.compile(loss=loss_fn, optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def Autoencoder_ADN01(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(5, 23854, 1), name=\"inputs\")\n",
    "    # layer 1\n",
    "    layers = tf.keras.layers.Conv2D(16, (1, 17), padding='valid', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers = tf.keras.layers.AveragePooling2D((1, 29))(layers)\n",
    "\n",
    "    # layer 2\n",
    "    layers = tf.keras.layers.Conv2D(16, (1, 17), padding='valid', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.AveragePooling2D((1, 26))(layers)\n",
    "\n",
    "    # layer 3\n",
    "    layers = tf.keras.layers.UpSampling2D((1, 26))(layers)\n",
    "    layers = tf.keras.layers.Conv2DTranspose(16, (1, 17), padding='valid', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    \n",
    "\n",
    "    #layer 4\n",
    "    layers = tf.keras.layers.UpSampling2D((1, 29))(layers)\n",
    "\n",
    "    predictions = tf.keras.layers.Conv2DTranspose(1,(1,17), activation=\"sigmoid\",padding='valid', name=\"output_1\")(layers)\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    loss_fn = tf.keras.losses.binary_crossentropy\n",
    "    model.compile(loss=loss_fn, optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def Autoencoder_ADN02(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(5, 23854, 1), name=\"inputs\")\n",
    "    # layer 1\n",
    "    layers = tf.keras.layers.Conv2D(16, (5, 15), padding='valid', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(inputs)\n",
    "    layers = tf.keras.layers.AveragePooling2D((1, 20))(layers)\n",
    "\n",
    "    # layer 2\n",
    "    layers = tf.keras.layers.Conv2D(16, (1, 4), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.AveragePooling2D((1, 8))(layers)\n",
    "\n",
    "    # layer 3\n",
    "    layers = tf.keras.layers.Conv2D(16, (1, 20), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.UpSampling2D((1, 8))(layers)\n",
    "\n",
    "    #layer 4\n",
    "    layers = tf.keras.layers.Conv2D(16, (1, 20), padding='same', strides=(1,1), activation=fun_act, use_bias=True, kernel_initializer=init_mode, bias_initializer='zeros', kernel_regularizer=regularizer(w_reg), bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(layers)\n",
    "    layers = tf.keras.layers.UpSampling2D((1, 20))(layers)\n",
    "\n",
    "    #Output layer\n",
    "    predictions = tf.keras.layers.Conv2DTranspose(1,(5,15), activation=\"sigmoid\",padding='valid', name=\"output_1\")(layers)\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    loss_fn = tf.keras.losses.binary_crossentropy\n",
    "    model.compile(loss=loss_fn, optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def Autoencoder_recu01(optimizador=Adam,lr=0.001,init_mode='glorot_uniform'):\n",
    "  tf.keras.backend.clear_session()\n",
    "  timesteps = X_train.shape[2]\n",
    "  input_dim = 5\n",
    "  latent_dim = 1\n",
    "\n",
    "  inputs = tf.keras.Input(shape=(timesteps, input_dim))\n",
    "  layers = tf.keras.layers.LSTM(latent_dim)(inputs)\n",
    "\n",
    "  layers = tf.keras.layers.RepeatVector(timesteps)(layers)\n",
    "  predictions = tf.keras.layers.LSTM(input_dim, return_sequences=True)(layers)\n",
    "\n",
    "  model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "  opt = optimizador(learning_rate=lr)\n",
    "  loss_fn = tf.keras.losses.binary_crossentropy\n",
    "  model.compile(loss=loss_fn, optimizer=opt)\n",
    "  return model\n",
    "\n",
    "\n",
    "def Autoencoder_recu02(optimizador=Adam,lr=0.001,init_mode='glorot_uniform'):\n",
    "  tf.keras.backend.clear_session()\n",
    "  timesteps = X_train.shape[2]\n",
    "  input_dim = 5\n",
    "  latent_dim = 1\n",
    "\n",
    "  inputs = tf.keras.Input(shape=(timesteps, input_dim))\n",
    "  layers = tf.keras.layers.LSTM(latent_dim, return_sequences=True)(inputs)\n",
    "  predictions = tf.keras.layers.LSTM(input_dim, return_sequences=True)(layers)\n",
    "\n",
    "  model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "  opt = optimizador(learning_rate=lr)\n",
    "  loss_fn = tf.keras.losses.binary_crossentropy\n",
    "  model.compile(loss=loss_fn, optimizer=opt)\n",
    "  return model\n",
    "\n",
    "\n",
    "def desempeno(X,model):\n",
    "  Error_prom=[]\n",
    "  for i in range(0,len(X)):\n",
    "    X_sim=model.predict(X[i])\n",
    "    X_mod=np.amax(X_sim,axis=1,keepdims=True)\n",
    "    X_mod=(X_sim>=X_mod)*1\n",
    "    X_mod=X_mod.reshape(X[i].shape)\n",
    "    Error=np.absolute(X[i]-X_mod)\n",
    "    NT=np.sum(X[i])\n",
    "    Error_prom.append(np.sum(Error)/NT*100)\n",
    "  print('El error de este modelo es de {:.1f} {}, {:.1f} {} y {:.1f} {} para train, dev y test, respectivamente'.format(Error_prom[0],'%',Error_prom[1],'%',Error_prom[2],'%'))\n",
    "  return Error_prom\n",
    "\n",
    "def verificacion_zona_ceros(X,model):\n",
    "  posiciones=np.zeros((X.shape[0],2))\n",
    "  for i in range(0, X.shape[0]):\n",
    "    for j in range(0,X.shape[2]):\n",
    "      if np.sum(X[i,:,j])==1:\n",
    "        posiciones[i,0] = j\n",
    "        break\n",
    "  for i in range(0, X.shape[0]):\n",
    "    for j in range(int(-posiciones[i,0]+5),int(-posiciones[i,0]-5),-1):\n",
    "      if np.sum(X[i,:,j])==1:\n",
    "        posiciones[i,1] = j+1\n",
    "        break\n",
    "  X_sim=model.predict(X)\n",
    "  X_mod=np.amax(X_sim,axis=1,keepdims=True)\n",
    "  X_mod=(X_sim>=X_mod)*1\n",
    "  X_mod=X_mod.reshape(X.shape)\n",
    "  Error=np.absolute(X-X_mod)\n",
    "  NT=np.sum(X)\n",
    "  acum=0\n",
    "  for i in range(0,X.shape[0]):\n",
    "    acum=acum+np.sum(Error[i,0:int(posiciones[i,0])])+np.sum(Error[i,int(posiciones[i,1]):])\n",
    "  Error_zona=acum/NT*100\n",
    "  print('El error asociado a las zonas laterales donde se encuentran los ceros es de {:.6f}'.format(Error_zona))\n",
    "  return Error_zona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ETO9cGNtnmh"
   },
   "source": [
    "## **3.3 COMPARACIÓN DE ARQUITECTURAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgBXEm1y2fIv",
    "outputId": "3f14c56b-9050-4aa8-e74a-adf99e00e050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTICLE 01\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 5, 23854, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 1, 23838, 16)      1376      \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 1, 822, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 1, 806, 16)        4368      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 1, 31, 16)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 1, 806, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 1, 822, 16)        4368      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 1, 23838, 16)      0         \n",
      "_________________________________________________________________\n",
      "output_1 (Conv2DTranspose)   (None, 5, 23854, 1)       1361      \n",
      "=================================================================\n",
      "Total params: 11,473\n",
      "Trainable params: 11,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "706/706 [==============================] - 20s 28ms/step - loss: 0.1678 - val_loss: 0.1532\n",
      "Epoch 2/50\n",
      "706/706 [==============================] - 20s 28ms/step - loss: 0.1540 - val_loss: 0.1525\n",
      "Epoch 3/50\n",
      "706/706 [==============================] - 19s 28ms/step - loss: 0.1536 - val_loss: 0.1524\n",
      "Epoch 4/50\n",
      "706/706 [==============================] - 20s 28ms/step - loss: 0.1535 - val_loss: 0.1522\n",
      "Epoch 5/50\n",
      "706/706 [==============================] - 19s 28ms/step - loss: 0.1534 - val_loss: 0.1521\n",
      "Epoch 6/50\n",
      "706/706 [==============================] - 20s 28ms/step - loss: 0.1533 - val_loss: 0.1520\n",
      "Epoch 7/50\n",
      "706/706 [==============================] - 19s 28ms/step - loss: 0.1533 - val_loss: 0.1520\n",
      "Epoch 8/50\n",
      "706/706 [==============================] - 19s 28ms/step - loss: 0.1532 - val_loss: 0.1520\n",
      "Epoch 9/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1532 - val_loss: 0.1520\n",
      "Epoch 10/50\n",
      "706/706 [==============================] - 20s 28ms/step - loss: 0.1532 - val_loss: 0.1520\n",
      "Epoch 11/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1532 - val_loss: 0.1519\n",
      "Epoch 12/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1531 - val_loss: 0.1521\n",
      "Epoch 13/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1529 - val_loss: 0.1516\n",
      "Epoch 14/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1528 - val_loss: 0.1516\n",
      "Epoch 15/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1528 - val_loss: 0.1515\n",
      "Epoch 16/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1527 - val_loss: 0.1515\n",
      "Epoch 17/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1527 - val_loss: 0.1516\n",
      "Epoch 18/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1527 - val_loss: 0.1515\n",
      "Epoch 19/50\n",
      "706/706 [==============================] - 20s 28ms/step - loss: 0.1527 - val_loss: 0.1515\n",
      "Epoch 20/50\n",
      "706/706 [==============================] - 20s 28ms/step - loss: 0.1527 - val_loss: 0.1515\n",
      "Epoch 21/50\n",
      "706/706 [==============================] - 21s 30ms/step - loss: 0.1527 - val_loss: 0.1515\n",
      "Epoch 22/50\n",
      "706/706 [==============================] - 20s 28ms/step - loss: 0.1527 - val_loss: 0.1515\n",
      "Epoch 23/50\n",
      "706/706 [==============================] - 20s 28ms/step - loss: 0.1527 - val_loss: 0.1515\n",
      "Epoch 24/50\n",
      "706/706 [==============================] - 19s 28ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 25/50\n",
      "706/706 [==============================] - 20s 28ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 26/50\n",
      "706/706 [==============================] - 19s 28ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 27/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 28/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 29/50\n",
      "706/706 [==============================] - 20s 28ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 30/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 31/50\n",
      "706/706 [==============================] - 20s 28ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 32/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 33/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 34/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 35/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 36/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 37/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 38/50\n",
      "706/706 [==============================] - 20s 28ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 39/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 40/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 41/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 42/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 43/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 44/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 45/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 46/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 47/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 48/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 49/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "Epoch 50/50\n",
      "706/706 [==============================] - 19s 27ms/step - loss: 0.1526 - val_loss: 0.1514\n",
      "1412/1412 [==============================] - 10s 7ms/step - loss: 0.1526\n",
      "177/177 [==============================] - 1s 7ms/step - loss: 0.1514\n",
      "177/177 [==============================] - 1s 8ms/step - loss: 0.1522\n",
      "La función de pérdida es de 0.1526, 0.1514 y 0.1522 para train, val y test sets, respectivamente\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[32,16,1,23838] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/output_1/conv2d_transpose-2-TransposeNHWCToNCHW-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_predict_function_46041]\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3209f20b2afc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'La función de pérdida es de {:.4f}, {:.4f} y {:.4f} para train, val y test sets, respectivamente'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcriterio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesempeno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mError_zona_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverificacion_zona_ceros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mError_zona_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverificacion_zona_ceros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-938961ac84c3>\u001b[0m in \u001b[0;36mdesempeno\u001b[0;34m(X, model)\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0mError_prom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mX_sim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0mX_mod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mX_mod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sim\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mX_mod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[32,16,1,23838] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/output_1/conv2d_transpose-2-TransposeNHWCToNCHW-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_predict_function_46041]\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "filepath='Modelos/Modelo01.hdf5'\n",
    "\n",
    "print('ARTICLE 01')\n",
    "model = Autoencoder_Art01_ADN()\n",
    "print(model.summary())\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss',save_weights_only=True)\n",
    "history=model.fit(X_train, X_train, epochs=50, callbacks=[checkpoint], batch_size=64, validation_data=(X_dev,X_dev))\n",
    "train_loss=model.evaluate(X_train,X_train)\n",
    "val_loss=model.evaluate(X_dev,X_dev)\n",
    "test_loss=model.evaluate(X_test,X_test)\n",
    "print('La función de pérdida es de {:.4f}, {:.4f} y {:.4f} para train, val y test sets, respectivamente'.format(train_loss,val_loss,test_loss))\n",
    "criterio=desempeno([X_train,X_dev,X_test],model)\n",
    "Error_zona_train=verificacion_zona_ceros(X_train,model)\n",
    "Error_zona_val=verificacion_zona_ceros(X_dev,model)\n",
    "Error_zona_test=verificacion_zona_ceros(X_test,model)\n",
    "\n",
    "\"\"\"\n",
    "filepath='/content/drive/MyDrive/AUTOENCODER/Modelos/Modelo01.hdf5'\n",
    "model2=Autoencoder_Art01_ADN()\n",
    "model2.load_weights(filepath)\n",
    "test_loss=model2.evaluate(X_test,X_test)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "filepath='Modelos/Modelo02.hdf5'\n",
    "print('ARTICLE 02')\n",
    "model = Autoencoder_Art02_ADN()\n",
    "print(model.summary())\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss',save_weights_only=True)\n",
    "history=model.fit(X_train, X_train, epochs=50, callbacks=[checkpoint], batch_size=64, validation_data=(X_dev,X_dev))\n",
    "train_loss=model.evaluate(X_train,X_train)\n",
    "val_loss=model.evaluate(X_dev,X_dev)\n",
    "test_loss=model.evaluate(X_test,X_test)\n",
    "print('La función de pérdida es de {:.4f}, {:.4f} y {:.4f} para train, val y test sets, respectivamente'.format(train_loss,val_loss,test_loss))\n",
    "criterio=desempeno([X_train,X_dev,X_test],model)\n",
    "Error_zona_train=verificacion_zona_ceros(X_train,model)\n",
    "Error_zona_val=verificacion_zona_ceros(X_dev,model)\n",
    "Error_zona_test=verificacion_zona_ceros(X_test,model)\n",
    "\n",
    "\n",
    "filepath='Modelos/Modelo01_mod.hdf5'\n",
    "print('ARTICLE 01 MODIFICADO')\n",
    "model = Autoencoder_ADN01()\n",
    "print(model.summary())\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss',save_weights_only=True)\n",
    "history=model.fit(X_train, X_train, epochs=50, callbacks=[checkpoint], batch_size=64, validation_data=(X_dev,X_dev))\n",
    "train_loss=model.evaluate(X_train,X_train)\n",
    "val_loss=model.evaluate(X_dev,X_dev)\n",
    "test_loss=model.evaluate(X_test,X_test)\n",
    "print('La función de pérdida es de {:.4f}, {:.4f} y {:.4f} para train, val y test sets, respectivamente'.format(train_loss,val_loss,test_loss))\n",
    "criterio=desempeno([X_train,X_dev,X_test],model)\n",
    "Error_zona_train=verificacion_zona_ceros(X_train,model)\n",
    "Error_zona_val=verificacion_zona_ceros(X_dev,model)\n",
    "Error_zona_test=verificacion_zona_ceros(X_test,model)\n",
    "\n",
    "\n",
    "filepath='Modelos/Modelo02_mod.hdf5'\n",
    "print('ARTICLE 02 MODIFICADO')\n",
    "model = Autoencoder_ADN02()\n",
    "print(model.summary())\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss',save_weights_only=True)\n",
    "history=model.fit(X_train, X_train, epochs=50, callbacks=[checkpoint], batch_size=64, validation_data=(X_dev,X_dev))\n",
    "train_loss=model.evaluate(X_train,X_train)\n",
    "val_loss=model.evaluate(X_dev,X_dev)\n",
    "test_loss=model.evaluate(X_test,X_test)\n",
    "print('La función de pérdida es de {:.4f}, {:.4f} y {:.4f} para train, val y test sets, respectivamente'.format(train_loss,val_loss,test_loss))\n",
    "criterio=desempeno([X_train,X_dev,X_test],model)\n",
    "Error_zona_train=verificacion_zona_ceros(X_train,model)\n",
    "Error_zona_val=verificacion_zona_ceros(X_dev,model)\n",
    "Error_zona_test=verificacion_zona_ceros(X_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Autoencoders.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
