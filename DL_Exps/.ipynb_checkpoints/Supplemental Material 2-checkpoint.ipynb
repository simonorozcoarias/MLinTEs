{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYKQKbQW7hbI"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas import set_option\n",
    "import pandas as pd\n",
    "import tensorflow.keras.utils\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model, model_from_json, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten, LSTM, Embedding\n",
    "import numpy as np \n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sn \n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import time as tm\n",
    "import datetime\n",
    "import os\n",
    "from operator import itemgetter\n",
    "from numpy import argmax\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZlN9BjT7vL0"
   },
   "outputs": [],
   "source": [
    "set_option(\"display.max_rows\", 15)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#non-redundant InpactorDB\n",
    "filename = 'InpactorDB_non-redudant.fasta.kmers'\n",
    "training_data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "kLgWzLCY7xkt",
    "outputId": "f9f2ec62-b28a-459e-e27d-1ebc3a21294b"
   },
   "outputs": [],
   "source": [
    "path_log_base = './logs'\n",
    "# class dist|ribution\n",
    "print(training_data.groupby('Label').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "g0Di_mgy70Cw",
    "outputId": "aa15766e-1d47-4f7a-8cf7-c855aef93196"
   },
   "outputs": [],
   "source": [
    "label_vectors = training_data['Label'].values\n",
    "feature_vectors = training_data.drop(['Label'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = preprocessing.StandardScaler().fit(feature_vectors)\n",
    "feature_vectors_scaler = scaler.transform(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_vectors)\n",
    "\n",
    "print(feature_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "M2_Kjw4e72DC",
    "outputId": "b0a02740-614a-47ac-863d-f01798233726"
   },
   "outputs": [],
   "source": [
    "#data split: 80% train, 10% dev and 10% test\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "X_trainScaler, X_test_dev, Y_train, Y_test_dev = train_test_split(feature_vectors_scaler, label_vectors, \n",
    "                                                                                        test_size=validation_size, \n",
    "                                                                                     random_state=seed)\n",
    "\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_test_dev, Y_test_dev, test_size=0.5, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=0.96,svd_solver='full',tol=1e-4)\n",
    "pca.fit(X_trainScaler)\n",
    "X_trainPCAScaler = pca.transform(X_trainScaler)\n",
    "X_validationPCAScaler = pca.transform(X_dev)\n",
    "X_testPCAScaler = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dTNS90C8BgS"
   },
   "outputs": [],
   "source": [
    "def metrics(Y_validation,predictions):\n",
    "    print('Accuracy:', accuracy_score(Y_validation, predictions))\n",
    "    print('F1 score:', f1_score(Y_validation, predictions,average='weighted'))\n",
    "    print('Recall:', recall_score(Y_validation, predictions,average='weighted'))\n",
    "    print('Precision:', precision_score(Y_validation, predictions, average='weighted'))\n",
    "    print('\\n clasification report:\\n', classification_report(Y_validation, predictions))\n",
    "    print('\\n confusion matrix:\\n',confusion_matrix(Y_validation, predictions))\n",
    "    #Creamos la matriz de confusión\n",
    "    num_classes = len(set(Y_validation))\n",
    "    snn_cm = confusion_matrix(Y_validation, predictions)\n",
    "\n",
    "    # Visualizamos la matriz de confusión\n",
    "    snn_df_cm = pd.DataFrame(snn_cm, range(num_classes), range(num_classes))  \n",
    "    plt.figure(figsize = (20,14))  \n",
    "    sn.set(font_scale=1.4) #for label size  \n",
    "    sn.heatmap(snn_df_cm, annot=True, annot_kws={\"size\": 12}) # font size  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cziZDJwp8HRy"
   },
   "outputs": [],
   "source": [
    "def graphics(history, AccTest, LossTest, log_Dir, model_Name, lossTEST, lossTRAIN, lossVALID, accuracyTEST, accuracyTRAIN, accuracyVALID):\n",
    "    numbers=AccTest\n",
    "    numbers_sort = sorted(enumerate(numbers), key=itemgetter(1),  reverse=True)\n",
    "    for i in range(int(len(numbers)*(0.05))): #5% Del total de las épocas\n",
    "        index, value = numbers_sort[i]\n",
    "        print(\"Test F1-Score {}, Época:{}\\n\".format(value, index+1))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    numbers=history.history['f1_m']\n",
    "    numbers_sort = sorted(enumerate(numbers), key=itemgetter(1),  reverse=True)\n",
    "    for i in range(int(len(numbers)*(0.05))): #5% Del total de las épocas\n",
    "        index, value = numbers_sort[i]\n",
    "        print(\"Train F1-Score {}, Época:{}\\n\".format(value, index+1))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    numbers=history.history['val_f1_m']\n",
    "    numbers_sort = sorted(enumerate(numbers), key=itemgetter(1),  reverse=True)\n",
    "    for i in range(int(len(numbers)*(0.05))): #5% Del total de las épocas\n",
    "        index, value = numbers_sort[i]\n",
    "        print(\"Validation F1-Score {}, Época:{}\\n\".format(value, index+1))\n",
    "\n",
    "    with plt.style.context('seaborn-white'):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #Plot training & validation accuracy values\n",
    "        plt.plot(np.concatenate([np.array([accuracyTRAIN]),np.array(history.history['f1_m'])],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([accuracyVALID]),np.array(history.history['val_f1_m'])],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([accuracyTEST]),np.array(AccTest)],axis=0)) #Test\n",
    "        plt.ylim(0.50, 1)\n",
    "        plt.title('F1-Score Vs Epoch')\n",
    "        plt.ylabel('F1-Score')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation', 'Test'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig('Nakano_25e_f1.eps', format='eps')\n",
    "        plt.savefig('Nakano_25e_f1.svg', format='svg')\n",
    "        plt.savefig('Nakano_25e_f1.pdf', format='pdf') \n",
    "        plt.savefig('Nakano_25e_f1.png', format='png')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #Plot training & validation loss values\n",
    "        plt.plot(np.concatenate([np.array([lossTRAIN]),np.array(history.history['loss'])],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([lossVALID]),np.array(history.history['val_loss'])],axis=0))\n",
    "        plt.plot(np.concatenate([np.array([lossTEST]),np.array(LossTest)],axis=0)) #Test\n",
    "        #plt.ylim(0, 10)\n",
    "        plt.title('Loss Vs Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation', 'Test'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig('Nakano_25e_loss.eps', format='eps')\n",
    "        plt.savefig('Nakano_25e_loss.svg', format='svg')\n",
    "        plt.savefig('Nakano_25e_loss.pdf', format='pdf')\n",
    "        plt.savefig('Nakano_25e_loss.png', format='png') \n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-white'):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #Plot training & validation accuracy values\n",
    "        plt.plot(np.arange(1,26),np.array([0.9381, 0.9690, 0.9814, 0.9901, 0.9939, 0.9966, 0.9958, 0.9970, 0.9970, 0.9983, 0.9977, 0.9984, 0.9989, 0.9988, 0.9994, 0.9994, 0.9989, 0.9991, 0.9996, 0.9995, 0.9997, 0.9987, 0.9997, 0.9996, 0.9997]))\n",
    "        plt.plot(np.arange(1,26),np.array([0.9199, 0.9483, 0.9542, 0.9627, 0.9621, 0.9687, 0.9655, 0.9659, 0.9642, 0.9686, 0.9659, 0.9651, 0.9696, 0.9686, 0.9708, 0.9716, 0.9689, 0.9687, 0.9727, 0.9715, 0.9713, 0.9626, 0.9730, 0.9705, 0.9725]))\n",
    "        plt.plot(np.arange(1,26),np.array([0.9262, 0.9502, 0.9560, 0.9639, 0.9642, 0.9718, 0.9706, 0.9693, 0.9679, 0.9698, 0.9669, 0.9679, 0.9741, 0.9728, 0.9708, 0.9742, 0.9710, 0.9710, 0.9754, 0.9724, 0.9741, 0.9670, 0.9773, 0.9732, 0.9777])) #Test\n",
    "        plt.ylim(0.50, 1.001)\n",
    "        plt.title('F1-Score Vs Epoch')\n",
    "        plt.ylabel('F1-Score')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation', 'Test'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig('DeepTE_25e_f1.eps', format='eps')\n",
    "        plt.savefig('DeepTE_25e_f1.svg', format='svg')\n",
    "        plt.savefig('DeepTE_25e_f1.pdf', format='pdf')   \n",
    "        plt.savefig('DeepTE_25e_f1.png', format='png')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        #Plot training & validation loss values\n",
    "        plt.plot(np.arange(1, 26),np.array([0.2305, 0.1294, 0.0759, 0.0398, 0.0262, 0.0130, 0.0160, 0.0105, 0.0094, 0.0072, 0.0075, 0.0063, 0.0037, 0.0032, 0.0022, 0.0029, 0.0035, 0.0030, 0.0016, 0.0016, 0.0018, 0.0036, 0.0014, 0.0015, 0.0024]))\n",
    "        plt.plot(np.arange(1, 26),np.array([0.2832, 0.1932, 0.1764, 0.1399, 0.1371, 0.1246, 0.1351, 0.1393, 0.1506, 0.1431, 0.1593, 0.1605, 0.1527, 0.1804, 0.1788, 0.1454, 0.1548, 0.1673, 0.1585, 0.1625, 0.1546, 0.2458, 0.1590, 0.1757, 0.1652]))\n",
    "        plt.plot(np.arange(1, 26),np.array([0.2706, 0.1834, 0.1635, 0.1259, 0.1188, 0.1069, 0.1196, 0.1269, 0.1360, 0.1308, 0.1403, 0.1427, 0.1360, 0.1534, 0.1634, 0.1268, 0.1325, 0.1427, 0.1400, 0.1561, 0.1385, 0.2310, 0.1391, 0.1481, 0.1357])) #Test\n",
    "        plt.ylim(0, 3)\n",
    "        plt.title('Loss Vs Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation', 'Test'], loc='upper left')\n",
    "        plt.grid('on')\n",
    "        plt.savefig('DeepTE_25e_loss.eps', format='eps')\n",
    "        plt.savefig('DeepTE_25e_loss.svg', format='svg')\n",
    "        plt.savefig('DeepTE_25e_loss.pdf', format='pdf') \n",
    "        plt.savefig('DeepTE_25e_loss.png', format='png')\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-I5EriM8KEX"
   },
   "outputs": [],
   "source": [
    "def Final_Results_Test(PATH_trained_models, X_test, Y_test):\n",
    "    global AccTest\n",
    "    global LossTest\n",
    "    AccTest = []\n",
    "    LossTest= [] \n",
    "    B_accuracy = 0 #B --> Best\n",
    "    for filename in sorted(os.listdir(PATH_trained_models)):\n",
    "        if filename != ('train') and filename != ('validation'):\n",
    "            print(filename)\n",
    "            model = tf.keras.models.load_model(PATH_trained_models+'/'+filename, custom_objects={'f1_m':f1_m})\n",
    "            loss,accuracy = model.evaluate(X_test, Y_test,verbose=0)\n",
    "            print(f'Loss={loss:.4f} and F1-score={accuracy:0.4f}'+'\\n')\n",
    "            BandAccTest  = accuracy\n",
    "            BandLossTest = loss\n",
    "            AccTest.append(BandAccTest)    #Valores de la precisión en Test, para graficar junto a valid y train\n",
    "            LossTest.append(BandLossTest)  #Valores de la perdida en Test, para graficar junto a valid y train\n",
    "            \n",
    "            if accuracy > B_accuracy:\n",
    "                B_accuracy = accuracy\n",
    "                B_loss = loss\n",
    "                B_name = filename\n",
    "    \n",
    "    print(\"\\n\\nBest\")\n",
    "    print(B_name)\n",
    "    print(f'Loss={B_loss:.4f} y Accuracy={B_accuracy:0.4f}'+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrN9k1Kg8OiT"
   },
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size, epochs, model_name=\"\"):\n",
    "    start_time = tm.time()\n",
    "    log_dir=path_log_base+\"/\"+model_name+\"_\"+str(datetime.datetime.now().isoformat()[:19].replace(\"T\", \"_\").replace(\":\",\"-\"))\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "    filepath = log_dir+\"/saved-model-{epoch:03d}-{val_f1_m:.4f}.hdf5\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_f1_m', save_best_only=False, mode='max')\n",
    "    model.reset_states()\n",
    "    \n",
    "    #VALORES EN TRAIN TEST Y VALIDACIÓN INICIALES, GRÁFICOS\n",
    "    global lossTEST\n",
    "    global accuracyTEST\n",
    "    global lossTRAIN\n",
    "    global accuracyTRAIN\n",
    "    global lossVALID\n",
    "    global accuracyVALID\n",
    "    lossTEST,accuracyTEST   = model.evaluate(X_test, y_test,verbose=None)\n",
    "    lossVALID,accuracyVALID = model.evaluate(X_valid, y_valid,verbose=None)\n",
    "    lossTRAIN,accuracyTRAIN = model.evaluate(X_train, y_train,verbose=None)\n",
    "    \n",
    "    global history\n",
    "    global model_Name\n",
    "    global log_Dir\n",
    "    model_Name = model_name\n",
    "    log_Dir = log_dir\n",
    "    \n",
    "    history=model.fit(X_train, y_train, epochs=epochs, \n",
    "                      callbacks=[tensorboard,checkpoint], \n",
    "                      batch_size=batch_size,validation_data=(X_valid, y_valid),verbose=1)\n",
    "    \n",
    "    metrics = model.evaluate(X_test, y_test, verbose=0)\n",
    "     \n",
    "    TIME = tm.time() - start_time\n",
    "    print(\"Time \"+model_name+\" = %s [seconds]\" % TIME)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(log_dir)\n",
    "    return {k:v for k,v in zip (model.metrics_names, metrics)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6t77sX08PzX"
   },
   "outputs": [],
   "source": [
    "# Implementing DeepTE published by Yan et al. (2020)\n",
    "def DeepTE():\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # FNN implemented by Nakano\n",
    "\n",
    "    #Inputs\n",
    "    inputs = tf.keras.Input(shape=(X_trainPCAScaler.shape[1],1), name=\"input_1\")\n",
    "    #layer 1\n",
    "    layers = tf.keras.layers.Conv1D(100, kernel_size=3, strides = 1, activation='relu')(inputs)\n",
    "    layers = tf.keras.layers.MaxPooling1D(pool_size=2, strides=1)(layers)\n",
    "    #layer 2\n",
    "    layers = tf.keras.layers.Conv1D(150, kernel_size=3, strides = 1, activation='relu')(layers)\n",
    "    layers = tf.keras.layers.MaxPooling1D(pool_size=2, strides=1)(layers)\n",
    "    #layer 3\n",
    "    layers = tf.keras.layers.Conv1D(225, kernel_size=3, strides = 1, activation='relu')(layers)\n",
    "    layers = tf.keras.layers.MaxPooling1D(pool_size=2, strides=1)(layers)\n",
    "    layers = tf.keras.layers.Flatten()(layers)\n",
    "    layers = tf.keras.layers.Dense(128,activation=\"relu\")(layers)    \n",
    "    # layer 4\n",
    "    layers = tf.keras.layers.Dropout(0.5)(layers)\n",
    "    predictions = tf.keras.layers.Dense(21, activation=\"softmax\", name=\"output_1\")(layers)\n",
    "    # model generation\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    # optimizer\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    # loss function\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    # Compile model\n",
    "    model.compile(loss=loss_fn, optimizer=opt, metrics=f1_m)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sg0Nf7Fd8aEi",
    "outputId": "26f345f5-c097-4b22-a432-014cabc5e3bb"
   },
   "outputs": [],
   "source": [
    "# baseline Nakano et al (2018) architecture \n",
    "model = DeepTE()\n",
    "# summarize layers\n",
    "print(model.summary())\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels_train = tf.keras.utils.to_categorical(Y_train, num_classes=21)\n",
    "one_hot_labels_validation = tf.keras.utils.to_categorical(Y_dev, num_classes=21)\n",
    "one_hot_labels_test = tf.keras.utils.to_categorical(Y_test, num_classes=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-GX0pULJ8d3L",
    "outputId": "442a6561-6490-4dc4-83b1-9c6db2a15d40"
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "train(model, X_trainPCAScaler, one_hot_labels_train, X_validationPCAScaler, one_hot_labels_validation, X_testPCAScaler, one_hot_labels_test, 128, 200, \"DeepTE\")\n",
    "Final_Results_Test(log_Dir, X_testPCAScaler, one_hot_labels_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_b5kd_pA8j1N",
    "outputId": "7e0e0c61-a8fc-440f-c4eb-2d6cc5d76462"
   },
   "outputs": [],
   "source": [
    "# plot metrics\n",
    "plt.plot(history.history['f1_m'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('Epoch vs Accuracy')\n",
    "plt.show()\n",
    "\n",
    "#GRÁFICOS DE LAS TRES CURVAS TRAIN TEST Y VALIDACIÓN\n",
    "graphics(history, AccTest, LossTest, log_Dir, model_Name, lossTEST, lossTRAIN, lossVALID, accuracyTEST, accuracyTRAIN, accuracyVALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LmCRsTPq8nLC",
    "outputId": "4d30da47-4631-4bb3-b4e7-158032de1144"
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_trainPCAScaler, one_hot_labels_train, verbose=0)\n",
    "print(\"Baseline Error train: %.2f%%\" % (100-scores[1]*100))\n",
    "\n",
    "scores = model.evaluate(X_validationPCAScaler, one_hot_labels_validation, verbose=0)\n",
    "print(\"Baseline Error dev: %.2f%%\" % (100-scores[1]*100))\n",
    "\n",
    "scores = model.evaluate(X_testPCAScaler, one_hot_labels_test, verbose=0)\n",
    "print(\"Baseline Error test: %.2f%%\" % (100-scores[1]*100))\n",
    "\n",
    "predictions = model.predict(X_trainPCAScaler)\n",
    "\n",
    "metrics(Y_train, [argmax(x) for x in predictions])\n",
    "\n",
    "predictions = model.predict(X_validationPCAScaler)\n",
    "\n",
    "metrics(Y_dev, [argmax(x) for x in predictions])\n",
    "\n",
    "predictions = model.predict(X_testPCAScaler)\n",
    "\n",
    "metrics(Y_test, [argmax(x) for x in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalization test\n",
    "# load Gardenia jasminoides data\n",
    "filename = \"/home/bioml/Projects/PhD/InpactorDB/version_final/Generalization_test/Gardenia_jasminoides.fasta.kmers\"\n",
    "gen_data = pd.read_csv(filename)\n",
    "label_vectors_gen = gen_data['Label'].values\n",
    "feature_vectors_gen = gen_data.drop(['Label'], axis=1).values\n",
    "feature_vectors_scaler = scaler.transform(feature_vectors_gen)\n",
    "X_gen_pca_scaling = pca.transform(feature_vectors_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained model\n",
    "predictions = model.predict(X_gen_pca_scaling)\n",
    "\n",
    "metrics(label_vectors_gen, [argmax(x) for x in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing FNN published by Nakano et al (2018)\n",
    "def Nakano_Net():\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # FNN implemented by Nakano\n",
    "\n",
    "    #Inputs\n",
    "    inputs = tf.keras.Input(shape=(X_trainPCAScaler.shape[1],), name=\"input_1\")\n",
    "    #layer 1\n",
    "    layers = tf.keras.layers.Dense(200,activation=\"relu\")(inputs)\n",
    "    layers = tf.keras.layers.Dropout(0.5)(layers)\n",
    "    #layer 2\n",
    "    layers = tf.keras.layers.Dense(200,activation=\"relu\")(layers)\n",
    "    layers = tf.keras.layers.Dropout(0.5)(layers)\n",
    "    #layer 3\n",
    "    layers = tf.keras.layers.Dense(200,activation=\"relu\")(layers)\n",
    "    layers = tf.keras.layers.Dropout(0.5)(layers)\n",
    "    # layer 4\n",
    "    predictions = tf.keras.layers.Dense(21, activation=\"softmax\", name=\"output_1\")(layers)\n",
    "    # model generation\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=predictions)\n",
    "    # optimizer\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08,)\n",
    "    # loss function\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    # Compile model\n",
    "    model.compile(loss=loss_fn, optimizer=opt, metrics=f1_m)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Nakano_Net()\n",
    "# summarize layers\n",
    "print(model.summary())\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "train(model, X_trainPCAScaler, one_hot_labels_train, X_validationPCAScaler, one_hot_labels_validation, X_testPCAScaler, one_hot_labels_test, 128, 25, \"Nakano1\")\n",
    "Final_Results_Test(log_Dir, X_testPCAScaler, one_hot_labels_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot metrics\n",
    "plt.plot(history.history['f1_m'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('Epoch vs F1-Score')\n",
    "plt.show()\n",
    "\n",
    "#GRÁFICOS DE LAS TRES CURVAS TRAIN TEST Y VALIDACIÓN\n",
    "graphics(history, AccTest, LossTest, log_Dir, model_Name, lossTEST, lossTRAIN, lossVALID, accuracyTEST, accuracyTRAIN, accuracyVALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_trainPCAScaler, one_hot_labels_train, verbose=0)\n",
    "print(\"Baseline Error train: %.2f%%\" % (100-scores[1]*100))\n",
    "\n",
    "scores = model.evaluate(X_validationPCAScaler, one_hot_labels_validation, verbose=0)\n",
    "print(\"Baseline Error dev: %.2f%%\" % (100-scores[1]*100))\n",
    "\n",
    "scores = model.evaluate(X_testPCAScaler, one_hot_labels_test, verbose=0)\n",
    "print(\"Baseline Error test: %.2f%%\" % (100-scores[1]*100))\n",
    "\n",
    "predictions = model.predict(X_trainPCAScaler)\n",
    "\n",
    "metrics(Y_train, [argmax(x) for x in predictions])\n",
    "\n",
    "predictions = model.predict(X_validationPCAScaler)\n",
    "\n",
    "metrics(Y_dev, [argmax(x) for x in predictions])\n",
    "\n",
    "predictions = model.predict(X_testPCAScaler)\n",
    "\n",
    "metrics(Y_test, [argmax(x) for x in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalization test\n",
    "# load Gardenia jasminoides data\n",
    "filename = \"/home/bioml/Projects/PhD/InpactorDB/version_final/Generalization_test/Gardenia_jasminoides.fasta.kmers\"\n",
    "gen_data = pd.read_csv(filename)\n",
    "label_vectors_gen = gen_data['Label'].values\n",
    "feature_vectors_gen = gen_data.drop(['Label'], axis=1).values\n",
    "feature_vectors_scaler = scaler.transform(feature_vectors_gen)\n",
    "X_gen_pca_scaling = pca.transform(feature_vectors_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained model\n",
    "model = tf.keras.models.load_model('logs/Nakano1_2020-11-12_14-32-59/saved-model-159-0.9827.hdf5', custom_objects={'f1_m':f1_m})\n",
    "predictions = model.predict(X_gen_pca_scaling)\n",
    "\n",
    "metrics(label_vectors_gen, [argmax(x) for x in predictions])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Nakano_FNN2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
