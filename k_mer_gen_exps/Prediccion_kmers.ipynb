{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0U0issomqi0"
   },
   "source": [
    "# **1. IMPORTACIÓN DE LIBRERIAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56573,
     "status": "ok",
     "timestamp": 1615507362264,
     "user": {
      "displayName": "Luis Humberto Lopez Murillo",
      "photoUrl": "",
      "userId": "06322058237111928965"
     },
     "user_tz": 300
    },
    "id": "0xw_yPCHmfa3",
    "outputId": "3c87103b-e45d-47fd-bcf2-fc229c095cee"
   },
   "outputs": [],
   "source": [
    "from scipy import misc, ndimage, signal                                         # \"scipy\" is one of the core packages that make up the SciPy stack. It provides many user-friendly and efficient numerical routines, such as routines for numerical integration, interpolation, optimization, linear algebra, and statistics.\n",
    "                                                                                # \"misc\" Various utilities that don’t have another home: ascent(), central_diff_weights(), derivative(), face(), electrocardiogram()\n",
    "                                                                                # \"ndimage\" This package contains various functions for multidimensional image processing\n",
    "                                                                                # \"signal\" This package contains various functions for signal processing: convolution, B-splines, filtering, filter design, Matlab-style IIR filter design, Continuous-time linear systems, Discrete-time linear systems, LTI representations, Waveforms, Window functions¶, Wavelets, Peak finding, Spectral analysis.\n",
    "import numpy as np                                                              # \"numpy\" is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more\n",
    "import random                                                                   # \"random\" This module implements pseudo-random number generators for various distributions\n",
    "#import ntpath                                                                   # ----------------------------------¿Para qué sirve?\n",
    "import os                                                                       # \"os\" This module provides a portable way of using operating system dependent functionality. If you just want to read or write a file see open(), if you want to manipulate paths, see the os.path module, and if you want to read all the lines in all the files on the command line see the fileinput module. \n",
    "import pandas as pd                                                             # \"pandas\" is a Python package that provides fast, flexible, and expressive data structures designed to make working with structured (tabular, multidimensional, potentially heterogeneous) and time series data both easy and intuitive\n",
    "import matplotlib as mpl                                                        # \"matplotlib\" is a comprehensive library for creating static, animated, and interactive visualizations in Python\n",
    "import matplotlib.pyplot as plt                                                 # \"pyplot\" is a collection of functions that make matplotlib work like MATLAB\n",
    "import matplotlib.colors as colors                                              # \"colors\" A module for converting numbers or color arguments to RGB or RGBA\n",
    "import tensorflow as tf                                                         # \"tensorflow\" TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications\n",
    "                                                                                # \"keras\" Keras is a deep learning API (application programming interface) written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result as fast as possible is key to doing good research.\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import optimizers                                         # \"optimizers\" is a container of functions -----------¿De qué trata?\n",
    "from tensorflow.keras import regularizers                                       # \"regularizers\" is a container of functions -----------¿De qué trata?\n",
    "from tensorflow.keras import backend as K                                       # \"backend\" is a container of functions -----------¿De qué trata?\n",
    "from tensorflow.keras import datasets,layers,models,Input,Model\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, Adagrad, SGD, Adadelta, Adamax, Nadam\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, AveragePooling2D, Cropping2D\n",
    "from tensorflow.keras.layers import Dropout, Activation, Flatten, Concatenate, Dense, Reshape, Add, PReLU, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.activations import relu\n",
    "import time as tm                                                               # \"time\" This module provides various time-related functions\n",
    "from time import time                                                           # \"time.time\" Return the time in seconds since the epoch as a floating point number. On Windows and most Unix systems, the epoch is January 1, 1970, 00:00:00 (UTC).\n",
    "import datetime                                                                 # \"datetime\" supplies classes for manipulating dates and times\n",
    "from operator import itemgetter                                                 # \"operator\" exports a set of efficient functions that fall into categories that perform object comparisons, logical operations, mathematical operations and sequence operations.\n",
    "                                                                                # \"itemgetter\" is a container of functions -----------¿De qué trata?\n",
    "import glob                                                                     # \"glob\" is used to retrieve files/pathnames matching a specified pattern.\n",
    "import tensorflow.keras.utils                                                   # \"utils\" is a container of functions -----------¿De qué trata?\n",
    "from tensorflow.keras.utils import to_categorical                               # \"to_categorical\" Converts a class vector (integers) to binary class matrix.\n",
    "from numpy import argmax                                                        # \"argmax\" Returns the indices of the maximum values along an axis.\n",
    "import seaborn as sn                                                            # \"seaborn\" Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "from sklearn.model_selection  import train_test_split                           # \"sklearn\" features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n",
    "                                                                                # \"train_test_split\" splits arrays or matrices into random train and test subsets\n",
    "from sklearn.metrics import confusion_matrix                                    # \"confusion_matrix\" Compute confusion matrix to evaluate the accuracy of a classification.\n",
    "from sklearn.metrics import accuracy_score                                      # \"accuracy_score\" In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\n",
    "from sklearn.metrics import f1_score                                            # \"f1_score\" Compute the F1 score, also known as balanced F-score or F-measure\n",
    "from sklearn.metrics import recall_score                                        # \"recall_score\" The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The best value is 1 and the worst value is 0.\n",
    "from sklearn.metrics import precision_score                                     # \"precision_Score\" The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1 and the worst value is 0.\n",
    "from sklearn.metrics import classification_report                               # \"classification_report\" Build a text report showing the main classification metrics\n",
    "from tensorflow.keras import layers                                             # \"layers\" is a container of functions -----------¿De qué trata?\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import cv2\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zwyPjvfm2qA"
   },
   "source": [
    "# **2. CARGA DE DATOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIizSKdL2T43"
   },
   "source": [
    "## **2.1 PRUEBA DE CONTEO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "executionInfo": {
     "elapsed": 35620,
     "status": "ok",
     "timestamp": 1615507438699,
     "user": {
      "displayName": "Luis Humberto Lopez Murillo",
      "photoUrl": "",
      "userId": "06322058237111928965"
     },
     "user_tz": 300
    },
    "id": "nIQERW1OtA4b",
    "outputId": "d69ee1f2-a633-4dc0-d58d-9a6bddf2ca52"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'InpactorDB_Repbase_format.fa.kmers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cd66b0eb89b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'InpactorDB_non-redundant+negative.fasta.filtered_center.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'InpactorDB_Repbase_format.fa.kmers'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mTexto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mTexto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTexto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'InpactorDB_Repbase_format.fa.kmers'"
     ]
    }
   ],
   "source": [
    "X = np.load('InpactorDB_non-redundant+negative.fasta.filtered_center.npy')\n",
    "Y = open('InpactorDB_Repbase_format.fa.kmers','r')\n",
    "Texto=Y.read()\n",
    "Y.close()\n",
    "Texto=Texto.splitlines()\n",
    "Texto=Texto[1:]\n",
    "label=[]\n",
    "for i in range(len(Texto)):\n",
    "    A=[int(x) for x in Texto[i].split(',')]\n",
    "    label.append(A)\n",
    "\n",
    "label=np.array(label)\n",
    "label=label[:,1:5461]\n",
    "\n",
    "print(X.shape)\n",
    "print(label.shape)\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "X_train, X_test_dev, Y_train, Y_test_dev = train_test_split(X, label, test_size=validation_size, random_state=seed)\n",
    "\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_test_dev, Y_test_dev, test_size=0.5, random_state=seed)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(Y_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "X = None\n",
    "Y = None\n",
    "\n",
    "X_test_dev = None\n",
    "Y_test_dev = None\n",
    "\n",
    "\"\"\"\n",
    "one_hot_labels_train = tf.keras.utils.to_categorical(Y_train[:,0], num_classes=21)\n",
    "one_hot_labels_validation = tf.keras.utils.to_categorical(Y_dev[:,0], num_classes=21)\n",
    "one_hot_labels_test = tf.keras.utils.to_categorical(Y_test[:,0], num_classes=21)\n",
    "\n",
    "print(one_hot_labels_train.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_-VfoM42YbW"
   },
   "source": [
    "## **2.2 PRUEBA DE CLASIFICACIÓN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4467,
     "status": "ok",
     "timestamp": 1615493563342,
     "user": {
      "displayName": "Luis Humberto Lopez Murillo",
      "photoUrl": "",
      "userId": "06322058237111928965"
     },
     "user_tz": 300
    },
    "id": "6YYFU-lM2cYp",
    "outputId": "2be80f41-fe92-4caa-ea71-716e800112f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884122, 5, 1000)\n",
      "(884122, 1)\n",
      "(707297, 5, 1000)\n",
      "(707297, 1)\n",
      "(88412, 5, 1000)\n",
      "(88412, 1)\n",
      "(88413, 5, 1000)\n",
      "(88413, 1)\n",
      "(707297, 21)\n"
     ]
    }
   ],
   "source": [
    "X = np.load('InpactorDB_non-redundant+negative.fasta.filtered_shortest.npy')\n",
    "Y = np.load('InpactorDB_non-redundant+negative.fasta.filtered_shortest_labels.npy')\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "validation_size = 0.2\n",
    "seed = 7\n",
    "X_train, X_test_dev, Y_train, Y_test_dev = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_test_dev, Y_test_dev, test_size=0.5, random_state=seed)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_dev.shape)\n",
    "print(Y_dev.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "X = None\n",
    "Y = None\n",
    "\n",
    "X_test_dev = None\n",
    "Y_test_dev = None\n",
    "\n",
    "\n",
    "one_hot_labels_train = tf.keras.utils.to_categorical(Y_train, num_classes=21)\n",
    "one_hot_labels_dev = tf.keras.utils.to_categorical(Y_dev, num_classes=21)\n",
    "one_hot_labels_test = tf.keras.utils.to_categorical(Y_test, num_classes=21)\n",
    "\n",
    "print(one_hot_labels_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIO6asj-thMQ"
   },
   "source": [
    "# **3. DEFINICIÓN DE MODELOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJamHCor1boB"
   },
   "source": [
    "## **3.1 CONTEO DE KMERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5779,
     "status": "ok",
     "timestamp": 1615507462506,
     "user": {
      "displayName": "Luis Humberto Lopez Murillo",
      "photoUrl": "",
      "userId": "06322058237111928965"
     },
     "user_tz": 300
    },
    "id": "iFlD27EA1gGU"
   },
   "outputs": [],
   "source": [
    "def CONTEO_KMER(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2], 1), name=\"input_1\")\n",
    "    # layer 1\n",
    "    \n",
    "\n",
    "    k=0\n",
    "    n_1=4**1\n",
    "    W_1=np.zeros((5,1,1,n_1))\n",
    "    b_1=np.zeros((1,n_1)).reshape(n_1,)\n",
    "    for i in range(4):\n",
    "      W_1[i,0,0,i]=1\n",
    "    \n",
    "    k=0\n",
    "    n_2=4**2\n",
    "    W_2=np.zeros((5,2,1,n_2))\n",
    "    b_2=-np.ones((1,n_2)).reshape(n_2,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        W_2[i,0,0,k]=1\n",
    "        W_2[j,1,0,k]=1\n",
    "        k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_3=4**3\n",
    "    W_3=np.zeros((5,3,1,n_3))\n",
    "    b_3=-2*np.ones((1,n_3)).reshape(n_3,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          W_3[i,0,0,k]=1\n",
    "          W_3[j,1,0,k]=1\n",
    "          W_3[l,2,0,k]=1\n",
    "          k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_4=4**4\n",
    "    W_4=np.zeros((5,4,1,n_4))\n",
    "    b_4=-3*np.ones((1,n_4)).reshape(n_4,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          for m in range(4):\n",
    "            W_4[i,0,0,k]=1\n",
    "            W_4[j,1,0,k]=1\n",
    "            W_4[l,2,0,k]=1\n",
    "            W_4[m,3,0,k]=1\n",
    "            k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_5=4**5\n",
    "    W_5=np.zeros((5,5,1,n_5))\n",
    "    b_5=-4*np.ones((1,n_5)).reshape(n_5,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          for m in range(4):\n",
    "            for n in range(4):\n",
    "              W_5[i,0,0,k]=1\n",
    "              W_5[j,1,0,k]=1\n",
    "              W_5[l,2,0,k]=1\n",
    "              W_5[m,3,0,k]=1\n",
    "              W_5[n,4,0,k]=1\n",
    "              k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_6=4**6\n",
    "    W_6=np.zeros((5,6,1,n_6))\n",
    "    b_6=-5*np.ones((1,n_6)).reshape(n_6,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          for m in range(4):\n",
    "            for n in range(4):\n",
    "              for p in range(4):\n",
    "                W_6[i,0,0,k]=1\n",
    "                W_6[j,1,0,k]=1\n",
    "                W_6[l,2,0,k]=1\n",
    "                W_6[m,3,0,k]=1\n",
    "                W_6[n,4,0,k]=1\n",
    "                W_6[p,5,0,k]=1\n",
    "                k=k+1\n",
    "    return np.array([W_1, b_1, W_2, b_2, W_3, b_3, W_4, b_4, W_5, b_5, W_6, b_6])\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv2D(n_1, (5, 1), strides=(1,1), weights=[W_1,b_1],activation=fun_act, use_bias=True, name='Mental_01')(inputs)\n",
    "    layers_1=tf.keras.backend.sum(layers_1,axis=-2)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Conv2D(n_2, (5, 2), strides=(1,1), weights=[W_2,b_2],activation=fun_act, use_bias=True, name='Mental_02')(inputs)\n",
    "    layers_2=tf.keras.backend.sum(layers_2,axis=-2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Conv2D(n_3, (5, 3), strides=(1,1), weights=[W_3,b_3],activation=fun_act, use_bias=True, name='Mental_03')(inputs)\n",
    "    layers_3 = tf.keras.backend.sum(layers_3,axis=-2)\n",
    "\n",
    "    layers_4 = tf.keras.layers.Conv2D(n_4, (5, 4), strides=(1,1), weights=[W_4,b_4],activation=fun_act, use_bias=True, name='Mental_04')(inputs)\n",
    "    layers_4 = tf.keras.backend.sum(layers_4,axis=-2)\n",
    "\n",
    "    layers_5 = tf.keras.layers.Conv2D(n_5, (5, 5), strides=(1,1), weights=[W_5,b_5],activation=fun_act, use_bias=True, name='Mental_05')(inputs)\n",
    "    layers_5 = tf.keras.backend.sum(layers_5,axis=-2)\n",
    "\n",
    "    layers_6 = tf.keras.layers.Conv2D(n_6, (5, 6), strides=(1,1), weights=[W_6,b_6],activation=fun_act, use_bias=True, name='Mental_06')(inputs)\n",
    "    layers_6 = tf.keras.backend.sum(layers_6,axis=-2)\n",
    "\n",
    "    layers=tf.concat([layers_1,layers_2,layers_3, layers_4,layers_5, layers_6],2)\n",
    "    layers = tf.keras.layers.Flatten()(layers)\n",
    "    model = tf.keras.Model(inputs = inputs, outputs=layers)\n",
    "\n",
    "    opt = optimizador(learning_rate=lr)\n",
    "    # loss function\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    # Compile model\n",
    "    #model.compile(loss=loss_fn, optimizer=opt, metrics=['acc', 'AUC', 'mse','mae','mape'])\n",
    "    model.compile(loss=loss_fn, optimizer=opt)\n",
    "    return model\n",
    "model_name='CONTEO_KMER'\n",
    "model = CONTEO_KMER()\n",
    "print(model.summary())\n",
    "\n",
    "Datos_train=X_train[0:1,:,:]\n",
    "Labels_train=Y_train[0:1,:]\n",
    "A=model.predict(Datos_train)\n",
    "Diferencia=(A-Labels_train)\n",
    "print('dim A es',A.shape)\n",
    "print('Diferencia primer ejemplo = ',np.sum(Diferencia[0,:]))\n",
    "print('La cantidad de A en labels es de ', Y_train[0,0])\n",
    "print('La cantidad real de A es ',np.sum(X_train[0,0,:]))\n",
    "print('Diferencia total = ',np.sum(np.abs(Diferencia)))\n",
    "print('El vector de diferencia en sus primeros 50 elementos es ', Diferencia[0,0:50])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[[1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 1., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., 0.]]]])\n",
      " array([0., 0., 0., 0.])\n",
      " array([[[[1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "          0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "          0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "          0.]],\n",
      "\n",
      "        [[0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "          0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "          1.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          1.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0.]]]])\n",
      " array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1.])\n",
      " array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "          0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "          0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          1., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "          0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "          0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "          0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "          0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "          0., 0., 1., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "          1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "          1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "          0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "          0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "          0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0.]]]])\n",
      " array([-2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
      "       -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
      "       -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
      "       -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.,\n",
      "       -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2., -2.])\n",
      " array([[[[1., 1., 1., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., ..., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., ..., 0., 1., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]]]])\n",
      " array([-3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3., -3.,\n",
      "       -3., -3., -3., -3., -3., -3., -3., -3., -3.])\n",
      " array([[[[1., 1., 1., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., ..., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., ..., 0., 1., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]]]])\n",
      " array([-4., -4., -4., ..., -4., -4., -4.])\n",
      " array([[[[1., 1., 1., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., ..., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., ..., 0., 1., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.]]]])\n",
      " array([-5., -5., -5., ..., -5., -5., -5.])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioml/anaconda3/envs/tf22/lib/python3.7/site-packages/ipykernel_launcher.py:85: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0ca3475162aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONTEO_KMER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Weights_SL.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'save' is not defined"
     ]
    }
   ],
   "source": [
    "weights = CONTEO_KMER()\n",
    "print(weights)\n",
    "save('Weights_SL.npy', weights.astype(np.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.load('Weights_SL.npy', allow_pickle=True)\n",
    "W_1 = weights[0]\n",
    "b_1 = weights[1]\n",
    "W_2 = weights[2]\n",
    "b_2 = weights[3]\n",
    "W_3 = weights[4]\n",
    "b_3 = weights[5]\n",
    "W_4 = weights[6]\n",
    "b_4 = weights[7]\n",
    "W_5 = weights[8]\n",
    "b_5 = weights[9]\n",
    "W_6 = weights[10]\n",
    "b_6 = weights[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KrDGkM51gtY"
   },
   "source": [
    "## **3.2 CLASIFICADOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 693,
     "status": "ok",
     "timestamp": 1615494228329,
     "user": {
      "displayName": "Luis Humberto Lopez Murillo",
      "photoUrl": "",
      "userId": "06322058237111928965"
     },
     "user_tz": 300
    },
    "id": "ynSrSqvctlmV"
   },
   "outputs": [],
   "source": [
    "def Clasificacion_SL(optimizador=Adam,lr=0.001,momen=0,init_mode='glorot_uniform',fun_act='relu',dp=0.2,regularizer=l2,w_reg=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Inputs\n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2], 1), name=\"input_1\")\n",
    "    # layer 1\n",
    "\n",
    "    layers_1 = tf.keras.layers.Conv2D(4, (5, 1), strides=(1,1), weights=[W_1,b_1],activation=fun_act, use_bias=True, name='Mental_01')(inputs)\n",
    "    layers_1=tf.keras.backend.sum(layers_1,axis=-2)\n",
    "\n",
    "    layers_2 = tf.keras.layers.Conv2D(16, (5, 2), strides=(1,1), weights=[W_2,b_2],activation=fun_act, use_bias=True, name='Mental_02')(inputs)\n",
    "    layers_2=tf.keras.backend.sum(layers_2,axis=-2)\n",
    "\n",
    "    layers_3 = tf.keras.layers.Conv2D(64, (5, 3), strides=(1,1), weights=[W_3,b_3],activation=fun_act, use_bias=True, name='Mental_03')(inputs)\n",
    "    layers_3 = tf.keras.backend.sum(layers_3,axis=-2)\n",
    "\n",
    "    layers_4 = tf.keras.layers.Conv2D(256, (5, 4), strides=(1,1), weights=[W_4,b_4],activation=fun_act, use_bias=True, name='Mental_04')(inputs)\n",
    "    layers_4 = tf.keras.backend.sum(layers_4,axis=-2)\n",
    "\n",
    "    layers_5 = tf.keras.layers.Conv2D(1024, (5, 5), strides=(1,1), weights=[W_5,b_5],activation=fun_act, use_bias=True, name='Mental_05')(inputs)\n",
    "    layers_5 = tf.keras.backend.sum(layers_5,axis=-2)\n",
    "\n",
    "    layers_6 = tf.keras.layers.Conv2D(4096, (5, 6), strides=(1,1), weights=[W_6,b_6],activation=fun_act, use_bias=True, name='Mental_06')(inputs)\n",
    "    layers_6 = tf.keras.backend.sum(layers_6,axis=-2)\n",
    "\n",
    "    layers=tf.concat([layers_1,layers_2,layers_3, layers_4,layers_5, layers_6],2)\n",
    "    layers = tf.keras.layers.Flatten()(layers)\n",
    "    filtro = tf.keras.Model(inputs = inputs, outputs=layers)\n",
    "\n",
    "    kmers=filtro.output\n",
    "    for layer in filtro.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    layers = tf.keras.layers.Dense(200,activation=\"relu\",kernel_regularizer=regularizers.l1(0.0001),bias_regularizer=regularizers.l2(0.01))(kmers)\n",
    "    layers = tf.keras.layers.Dropout(0.5)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.99, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    #layer 2\n",
    "    layers = tf.keras.layers.Dense(200,activation=\"relu\",kernel_regularizer=regularizers.l1(0.0001),bias_regularizer=regularizers.l2(0.01))(layers)\n",
    "    layers = tf.keras.layers.Dropout(0.5)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.99, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    #layer 3\n",
    "    layers = tf.keras.layers.Dense(200,activation=\"relu\",kernel_regularizer=regularizers.l1(0.0001),bias_regularizer=regularizers.l2(0.01))(layers)\n",
    "    layers = tf.keras.layers.Dropout(0.5)(layers)\n",
    "    layers = tf.keras.layers.BatchNormalization(momentum=0.99, epsilon=0.001, center=True, scale=False, trainable=True, fused=None, renorm=False, renorm_clipping=None, renorm_momentum=0.4, adjustment=None)(layers)\n",
    "    # layer 4\n",
    "    predictions = tf.keras.layers.Dense(21, activation=\"softmax\", name=\"output_1\")(layers)\n",
    "    # model generation\n",
    "    model = tf.keras.Model(inputs = filtro.input, outputs=predictions)\n",
    "    # optimizer\n",
    "    opt = optimizador(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08,)\n",
    "    # loss function\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    # Compile model\n",
    "    #model.compile(loss=loss_fn, optimizer=opt, metrics=['acc', 'AUC', 'mse','mae','mape'])\n",
    "    model.compile(loss=loss_fn, optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6x2Ww7_uKkN"
   },
   "source": [
    "# **4. ENTRENAMIENTO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 11840,
     "status": "error",
     "timestamp": 1615494240762,
     "user": {
      "displayName": "Luis Humberto Lopez Murillo",
      "photoUrl": "",
      "userId": "06322058237111928965"
     },
     "user_tz": 300
    },
    "id": "tz4s9XhHY7Qu",
    "outputId": "5ba83373-000e-486d-c43d-a99f9b4829c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 5, 1000, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Mental_01 (Conv2D)              (None, 1, 1000, 4)   24          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_02 (Conv2D)              (None, 1, 999, 16)   176         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_03 (Conv2D)              (None, 1, 998, 64)   1024        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_04 (Conv2D)              (None, 1, 997, 256)  5376        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_05 (Conv2D)              (None, 1, 996, 1024) 26624       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Mental_06 (Conv2D)              (None, 1, 995, 4096) 126976      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowOpLa [(None, 1, 4)]       0           Mental_01[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_1 (TensorFlowOp [(None, 1, 16)]      0           Mental_02[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_2 (TensorFlowOp [(None, 1, 64)]      0           Mental_03[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_3 (TensorFlowOp [(None, 1, 256)]     0           Mental_04[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_4 (TensorFlowOp [(None, 1, 1024)]    0           Mental_05[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_5 (TensorFlowOp [(None, 1, 4096)]    0           Mental_06[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 1, 5460)]    0           tf_op_layer_Sum[0][0]            \n",
      "                                                                 tf_op_layer_Sum_1[0][0]          \n",
      "                                                                 tf_op_layer_Sum_2[0][0]          \n",
      "                                                                 tf_op_layer_Sum_3[0][0]          \n",
      "                                                                 tf_op_layer_Sum_4[0][0]          \n",
      "                                                                 tf_op_layer_Sum_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 5460)         0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 200)          1092200     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 200)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 200)          600         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200)          40200       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 200)          600         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200)          40200       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 200)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200)          600         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_1 (Dense)                (None, 21)           4221        batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,338,821\n",
      "Trainable params: 1,177,421\n",
      "Non-trainable params: 161,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "11051/11052 [============================>.] - ETA: 0s - loss: 1.9868 - accuracy: 0.4936"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[33,4096,1,995] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_1/Mental_06/Conv2D (defined at <ipython-input-5-4d833b43d3a5>:7) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_2609]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4d833b43d3a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./Modelos_kmer/{}.hdf5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_labels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot_labels_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot_labels_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mone_hot_labels_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tf22/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[33,4096,1,995] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model_1/Mental_06/Conv2D (defined at <ipython-input-5-4d833b43d3a5>:7) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_2609]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "model_name='Clasificacion_SL'\n",
    "model = Clasificacion_SL()\n",
    "\n",
    "print(model.summary())\n",
    "filepath='./Modelos_kmer/{}.hdf5'.format(model_name)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss',save_weights_only=True)\n",
    "history=model.fit(X_train, one_hot_labels_train, epochs=50, callbacks=[checkpoint], batch_size=64, validation_data=(X_dev,one_hot_labels_dev))\n",
    "train_loss=model.evaluate(X_train,one_hot_labels_train)\n",
    "val_loss=model.evaluate(X_dev,one_hot_labels_dev)\n",
    "test_loss=model.evaluate(X_test,one_hot_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11784,
     "status": "ok",
     "timestamp": 1615507253695,
     "user": {
      "displayName": "Luis Humberto Lopez Murillo",
      "photoUrl": "",
      "userId": "06322058237111928965"
     },
     "user_tz": 300
    },
    "id": "fbEfy18W7xi2",
    "outputId": "ad40033b-0433-4ade-b5fa-4f28470d4dc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "def weights():\n",
    "    k=0\n",
    "    n_1=4**1\n",
    "    W_1=np.zeros((5,1,1,n_1))\n",
    "    b_1=np.zeros((1,n_1)).reshape(n_1,)\n",
    "    for i in range(4):\n",
    "      W_1[i,0,0,i]=1\n",
    "    \n",
    "    k=0\n",
    "    n_2=4**2\n",
    "    W_2=np.zeros((5,2,1,n_2))\n",
    "    b_2=-np.ones((1,n_2)).reshape(n_2,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        W_2[i,0,0,k]=1\n",
    "        W_2[j,1,0,k]=1\n",
    "        k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_3=4**3\n",
    "    W_3=np.zeros((5,3,1,n_3))\n",
    "    b_3=-2*np.ones((1,n_3)).reshape(n_3,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          W_3[i,0,0,k]=1\n",
    "          W_3[j,1,0,k]=1\n",
    "          W_3[l,2,0,k]=1\n",
    "          k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_4=4**4\n",
    "    W_4=np.zeros((5,4,1,n_4))\n",
    "    b_4=-3*np.ones((1,n_4)).reshape(n_4,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          for m in range(4):\n",
    "            W_4[i,0,0,k]=1\n",
    "            W_4[j,1,0,k]=1\n",
    "            W_4[l,2,0,k]=1\n",
    "            W_4[m,3,0,k]=1\n",
    "            k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_5=4**5\n",
    "    W_5=np.zeros((5,5,1,n_5))\n",
    "    b_5=-4*np.ones((1,n_5)).reshape(n_5,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          for m in range(4):\n",
    "            for n in range(4):\n",
    "              W_5[i,0,0,k]=1\n",
    "              W_5[j,1,0,k]=1\n",
    "              W_5[l,2,0,k]=1\n",
    "              W_5[m,3,0,k]=1\n",
    "              W_5[n,4,0,k]=1\n",
    "              k=k+1\n",
    "\n",
    "    k=0\n",
    "    n_6=4**6\n",
    "    W_6=np.zeros((5,6,1,n_6))\n",
    "    b_6=-5*np.ones((1,n_6)).reshape(n_6,)\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        for l in range(4):\n",
    "          for m in range(4):\n",
    "            for n in range(4):\n",
    "              for p in range(4):\n",
    "                W_6[i,0,0,k]=1\n",
    "                W_6[j,1,0,k]=1\n",
    "                W_6[l,2,0,k]=1\n",
    "                W_6[m,3,0,k]=1\n",
    "                W_6[n,4,0,k]=1\n",
    "                W_6[p,5,0,k]=1\n",
    "                k=k+1\n",
    "    return [W_1,b_1,W_2,b_2,W_3,b_3,W_4,b_4,W_5,b_5,W_6,b_6]\n",
    "A=weights()\n",
    "np.save('Weights_SL'+'.npy', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vy9PdFwcvDsb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMTJnXon5eQcMsFFEP9wcWG",
   "collapsed_sections": [],
   "name": "Prediccion_kmers.ipynb",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
